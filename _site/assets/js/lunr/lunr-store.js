var store = [{
        "title": "DQN",
        "excerpt":"This post documents my implementation of the Deep Q Network (DQN) algorithm.     A Deep Q Network implementation in tensorflow with target network &amp; random experience replay. The code is tested with Gym’s discrete action space environment, CartPole-v0 on Colab.   Code on my Github   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     Notations:   Model network =    Model parameter =    Model network Q value =  (s, a)   Target network =    Target parameter =    Target network Q value =  (, )     Equations:   TD target = r (s, a)     (, )   TD error = (TD target)  (Model network Q value) = [r (s, a)     (, )]   (s, a)     Key implementation details:   Update target parameter  with model parameter . Copy  to  with either soft or hard parameter update.   Hard parameter update:   with tf.variable_scope('hard_replace'):   self.target_replace_hard = [t.assign(m) for t, m in zip(self.target_net_params, self.model_net_params)]      # hard params replacement if self.learn_step % self.tau_step == 0:     self.sess.run(self.target_replace_hard)   self.learn_step += 1   Soft parameter update: polyak    + (1  polyak)       with tf.variable_scope('soft_replace'):               self.target_replace_soft = [t.assign(self.polyak * m + (1 - self.polyak) * t)                               for t, m in zip(self.target_net_params, self.model_net_params)]      Stop TD target from contributing to gradient computation:   # exclude td_target in gradient computation td_target = tf.stop_gradient(td_target)     Tensorflow graph:        References:   Human-level control through deep reinforcement learning (Mnih et al., 2015)        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/DQN/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "DDQN",
        "excerpt":"This post documents my implementation of the Double Deep Q Network (DDQN) algorithm.     A Double Deep Q Network (DDQN) implementation in tensorflow with random experience replay. The code is tested with Gym’s discrete action space environment, CartPole-v0 on Colab.   Code on my Github   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     Notations:   Model network =    Model parameter =    Model network Q value =  (s, a)   Target network =    Target parameter =    Target network Q value =  (, )     Equations:   TD target = r (s, a)    (,   (s, a))   TD error = (TD target)  (Model network Q value) = [r (s, a)    (,   (s, a))]   (s, a)     Key implementation details:   Create a placeholder to feed Q values from model network:   self.model_s_next_Q_val = tf.placeholder(tf.float32, [None,self.num_actions], name='model_s_next_Q_val')   Select Q values from model network using  as features &amp; feed them to the training session:   # select actions from model network model_s_next_Q_val = self.sess.run(self.model_Q_val, feed_dict={self.s: s_next})  # training _, loss = self.sess.run([self.optimizer, self.loss],                         feed_dict = {self.s: s,                                      self.a: a,                                      self.r: r,                                      self.s_next: s_next,                                      self.done: done,                                      self.model_s_next_Q_val: model_s_next_Q_val})   Select minibatch actions with largest Q values from model network, create indices &amp; select corresponding minibatch actions from target network:   def td_target(self, s_next, r, done, model_s_next_Q_val, target_Q_val):     # select action with largest Q value from model network     model_max_a = tf.argmax(model_s_next_Q_val, axis=1, output_type=tf.dtypes.int32)      arr = tf.range(tf.shape(model_max_a)[0], dtype=tf.int32) # create row indices     indices = tf.stack([arr, model_max_a], axis=1) # create 2D indices             max_target_Q_val = tf.gather_nd(target_Q_val, indices) # select minibatch actions from target network     max_target_Q_val = tf.reshape(max_target_Q_val, (self.minibatch_size,1))     Tensorflow graph:        References:   Deep Reinforcement Learning with Double Q-learning (Hasselt, Guez &amp; Silver, 2016)        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/DDQN/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Dueling DDQN",
        "excerpt":"This post documents my implementation of the Dueling Double Deep Q Network (Dueling DDQN) algorithm.     A Dueling Double Deep Q Network (Dueling DDQN) implementation in tensorflow with random experience replay. The code is tested with Gym’s discrete action space environment, CartPole-v0 on Colab.   Code on my Github   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     Notations:   Network =    Parameter =    Network Q value =  (s, a)   Value function = V(s)   Advantage function = A(s, a)   Parameter from the Advantage function layer =    Parameter from the Value function layer =      Equations:   (eqn 9) from the original paper (Wang et al., 2015):   Q(s, a; , , ) = V(s; , )  [ A(s, a; , )    A(s, ; , ) ]     Key implementation details:   V represents the value function layer, A represents the Advantage function layer:  # contruct neural network def built_net(self, var_scope, w_init, b_init, features, num_hidden, num_output):                   with tf.variable_scope(var_scope):                 feature_layer = tf.contrib.layers.fully_connected(features, num_hidden,                                                         activation_fn = tf.nn.relu,                                                         weights_initializer = w_init,                                                         biases_initializer = b_init)       V = tf.contrib.layers.fully_connected(feature_layer, 1,                                             activation_fn = None,                                             weights_initializer = w_init,                                             biases_initializer = b_init)       A = tf.contrib.layers.fully_connected(feature_layer, num_output,                                             activation_fn = None,                                             weights_initializer = w_init,                                             biases_initializer = b_init)          Q_val = V + (A - tf.reduce_mean(A, reduction_indices=1, keepdims=True)) # refer to eqn 9 from the original paper               return Q_val        Tensorflow graph:        References:   Dueling Network Architectures for Deep Reinforcement Learning (Wang et al., 2015)        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/Duel_DDQN/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Dueling DDQN with PER",
        "excerpt":"This post documents my implementation of the Dueling Double Deep Q Network with Priority Experience Replay (Duel DDQN with PER) algorithm.     A Dueling Double Deep Q Network with Priority Experience Replay (Duel DDQN with PER) implementation in tensorflow. The code is tested with Gym’s discrete action space environment, CartPole-v0 on Colab.   Code on my Github   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     Notations:   Model network =    Model parameter =    Model network Q value =  (s, a)   Target network =    Target parameter =    Target network Q value =  (, )   A small constant to ensure that no sample has 0 probability to be selected = e   Hyper parameter  =       Decides how to sample, range from 0 to 1, where 0 corresponds to fully uniformly random sample selection &amp; 1 corresponding to selecting samples based on highest priority.   Hyper parameter  =       Starts close to 0, gradually annealed  to 1, slowly giving more importance to weights during training.   Minibatch size = k   Replay memory size = N     Equations:   TD target = r (s, a)    (,   (s, a))   TD error =  = (TD target)  (Model network Q value) = [r (s, a)    (,   (s, a))]   (s, a)    =  =   e   probability(i) = P(i) =    weights =  = (N  P(i))      Key implementation details:   Sum tree:   Assume an example of a sum tree with 7 nodes (with 4 leaves which corresponds to the replay memory size):   At initialization:        When item 1 is added:        When item 2 is added:        When item 3 is added:        When item 4 is added:        When item 5 is added:        Figure below shows the corresponding code &amp; array contents. The tree represents the entire sum tree while data represents the leaves.      In the implementation, only one sumTree object is needed to store the collected experiences, this sumTree object resides in the Replay_memory class. The sumTree object has number of leaves = replay memory size = capacity. The data array in sumTree object stores an Exp object, which is a sample of experience.   The following code decides how to sample:   def sample(self, k): # k = minibatch size     batch = []      # total_p() gives the total sum of priorities of the leaves in the sumTree     # which is the value stored in the root node     segment = self.tree.total_p() / k      for i in range(k):         a = segment * i # start of segment         b = segment * (i + 1) # end of segment         s = np.random.uniform(a, b) # rand value between a, b          (idx, p, data) = self.tree.get(s)         batch.append( (idx, p, data) )                  return batch       Refer to appendix B.2.1, under the section, “Proportional prioritization”, from the original (Schaul et al., 2016) paper for sampling details.     Tensorflow graph:        References:   Prioritized experience replay (Schaul et al., 2016)        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/Duel_DDQN_with_PER/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Numpy array manipulation",
        "excerpt":"This post provides a simple usage examples for common Numpy array manipulation.     This Jupyter notebook contains simple examples on how to manipulate Numpy arrays. The code blocks below shows the codes &amp; it’s corresponding display output.     Code on my Github   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     Setting up a Numpy array:   buffer=[0,1] print('buffer=', buffer) $buffer= [0, 1]  new=2 print('new=', new) $new= 2  buffer = np.array(buffer + [new]) # append a new item &amp; create a numpy array print('np.array(buffer + [new])=', buffer) $np.array(buffer + [new])= [0 1 2]   Slicing examples:   # numpy array slicing syntax # buffer[start:stop:step]  print('buffer[1:]=', buffer[1:]) # starting from index 1 $buffer[1:]= [1 2]  print('buffer[-1:]=', buffer[-1:]) # getting item in last index $buffer[-1:]= [2]  print('buffer[:1]=', buffer[:1]) # stop at index 1 (exclusive), keep only 1st item $buffer[:1]= [0]  print('buffer[:-1]=', buffer[:-1]) # stop at last index (exclusive), discard item in last index $buffer[:-1]= [0 1]  print('buffer[::-1]=', buffer[::-1]) # start from last index (reversal) $buffer[::-1]= [2 1 0]  print('buffer[1::-1]=', buffer[1::-1]) # reverse starting from index 1 $buffer[1::-1]= [1 0]  # Starting from index 1 will return [1 2], reversing will return [2,1] print('buffer[1:][::-1]=', buffer[1:][::-1]) $buffer[1:][::-1]= [2 1]   np.newaxis is an alias for None:   # np.newaxis = None  print('buffer[:, np.newaxis]=', buffer[:, np.newaxis]) $buffer[:, np.newaxis]= [[0][1][2]]  print('buffer[:, None]=', buffer[:, None]) $buffer[:, None]= [[0][1][2]]  print('buffer[np.newaxis, :]=', buffer[np.newaxis, :]) $buffer[np.newaxis, :]= [[0 1 2]]  print('buffer[None, :]=', buffer[None, :]) $buffer[None, :]= [[0 1 2]]   Stacking:   a = [1,2,3] b = [4,5,6] c = [7,8,9]  r = np.hstack((a,b,c)) # horizontal stacking print(\"r=\", r) $r= [1 2 3 4 5 6 7 8 9]  QUEUE = queue.Queue() QUEUE.put(a) QUEUE.put(b) QUEUE.put(c)  r = [QUEUE.get() for _ in range(QUEUE.qsize())] print(r) $[[1, 2, 3], [4, 5, 6], [7, 8, 9]]  r = np.vstack(r) # vertical stacking print(r) $[[1 2 3]   [4 5 6]   [7 8 9]]  print(r[:, ::-1]) # col reversal $[[3 2 1]   [6 5 4]   [9 8 7]]        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/np_array_manipulation/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Python's multiprocessing package",
        "excerpt":"This post demonstrates how to use the Python’s multiprocessing package to achieve parallel data generation.     The main program has a chief that spawns multiple worker processes. Each worker  spawns a single work process. The work process generates random integer data  [1,3].   Each worker has it’s own local queue. When data is generated, it is stored in it’s local queue. When the local queue’s size is greater than 5, the data is retrieved &amp; 0.1 is added to the data, this result is stored in the Chief’s global queue. When the Chief’s global queue’s size is greater than 3, the result is retrieved &amp; printed on screen.     Code on my Github   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     The Worker class:   class Worker(object):   def __init__(self, worker_id, g_queue):     self.g_queue = g_queue     self.worker_id = worker_id     self.queue = Queue() # local worker queue     self.work_process = Process(target=self.work, args=())     self.work_process.start()     info(worker_id, self.work_process, \"Worker\")    def work(self):      info(self.worker_id, self.work_process, \"work\")      while True:       data = np.random.randint(1,4)       self.queue.put(data)        # process data in queue       if self.queue.qsize() &gt; 5:         data = self.queue.get()         result = data + 0.1         self.g_queue.put(result) # send result to global queue        time.sleep(1) # work every x sec interval      return self.w_id     The Chief class:   class Chief(object):   def __init__(self, num_workers):     self.g_queue = Queue() # global queue         self.num_workers = num_workers    def dispatch_workers(self):        worker_processes = [Process(target=Worker(w_id, self.g_queue), args=()) for w_id in range(num_workers)]     return worker_processes    def result(self):     if self.g_queue.qsize() &gt; 3:       result = self.g_queue.get()       print(\"result\", result)   The main program:   if __name__ == '__main__':     print('main parent process id:', os.getppid())   print('main process id:', os.getpid())    num_workers = 2   chief = Chief(num_workers)   workers_processes = chief.dispatch_workers()    i = 0   while True:         time.sleep(2) # chk g_queue every x sec interval to get result     chief.result()     print(\"i=\", i)      if i&gt;9:       break     i+=1       A helper display function:   def info(worker_id, process, function_name):     print(\"worker_id=\", worker_id,           'module name:', __name__,           'function name:', function_name,           'parent process:', os.getppid(),           'current process id:', os.getpid(),           'spawn process id:', process.pid)        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/py_mpp/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "N-step targets",
        "excerpt":"This post documents my implementation of the N-step Q-values estimation algorithm.     N-step Q-values estimation.   Code on my Github   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.   The following two functions computes truncated Q-values estimates:   A) n_step_targets_missing      treats missing terms as 0.   B) n_step_targets_max      use maximum terms possible.     Equations:   1-step truncated estimate:    = E( +    V())   2-step truncated estimate:    = E( +     +    V())   3-step truncated estimate:    = E( +     +     +    V())   N-step truncated estimate:    = E( +     +     + … +    V())     Example:   Assuming we have the following variables setup:   N=2 # N steps gamma=2 t=5 v_s_ = 10 # value of next state  epr=np.arange(t).reshape(t,1) print(\"epr=\", epr)  baselines=np.arange(t).reshape(t,1) print(\"baselines=\", baselines)   Display output of episodic rewards(epr) &amp; baselines:   epr= [[0]  [1]  [2]  [3]  [4]]  baselines= [[0]  [1]  [2]  [3]  [4]]     A) This function computes the n-step targets, treats missing terms as zero:   # if number of steps unavailable, missing terms treated as 0. def n_step_targets_missing(epr, baselines, gamma, N):   N = N+1   targets = np.zeros_like(epr)       if N &gt; epr.size:     N = epr.size   for t in range(epr.size):        print(\"t=\", t)     for n in range(N):       print(\"n=\", n)       if t+n == epr.size:                     print('missing terms treated as 0, break') # last term for those with insufficient steps.         break # missing terms treated as 0       if n == N-1: # last term         targets[t] += (gamma**n) * baselines[t+n] # last term for those with sufficient steps         print('last term for those with sufficient steps, end inner n loop')       else:         targets[t] += (gamma**n) * epr[t+n] # non last terms   return targets   Run the function n_step_targets_missing:   print('n_step_targets_missing:') T = n_step_targets_missing(epr, baselines, gamma, N) print(T)   Display the output:   n_step_targets_missing: t= 0 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 1 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 2 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 3 n= 0 n= 1 n= 2 missing terms treated as 0, break t= 4 n= 0 n= 1 missing terms treated as 0, break [[10]  [17]  [24]  [11]  [ 4]]   For the output above, note that when t+n = 5 which is greater than the last index 4, missing terms are treated as 0.     B) This function computes the n-step targets, it will use maximum number of terms possible:   # if number of steps unavailable, use max steps available. # uses v_s_ as input def n_step_targets_max(epr, baselines, v_s_, gamma, N):   N = N+1   targets = np.zeros_like(epr)       if N &gt; epr.size:     N = epr.size   for t in range(epr.size):       print(\"t=\", t)     for n in range(N):       print(\"n=\", n)       if t+n == epr.size:                     targets[t] += (gamma**n) * v_s_ # last term for those with insufficient steps.         print('last term for those with INSUFFICIENT steps, break')         break       if n == N-1:         targets[t] += (gamma**n) * baselines[t+n] # last term for those with sufficient steps         print('last term for those with sufficient steps, end inner n loop')       else:         targets[t] += (gamma**n) * epr[t+n] # non last terms   return targets   Run the function n_step_targets_max:   print('n_step_targets_max:') T = n_step_targets_max(epr, baselines, v_s_, gamma, N) print(T)   Display the output:   n_step_targets_max: t= 0 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 1 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 2 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 3 n= 0 n= 1 n= 2 last term for those with INSUFFICIENT steps, break t= 4 n= 0 n= 1 last term for those with INSUFFICIENT steps, break [[10]  [17]  [24]  [51]  [24]]   For the output above, note that when t+n = 5 which is greater than the last index 4, maximum terms are used where possible. ( Last term for those with INSUFFICIENT steps is given by   (gamma**n) * v_s_ =  V()), where v_s_ = V()   When t = 2, normal 2 steps estimation:    = E( +     +    V())   When t = 3, 2 steps estimation with insufficient step, using v_s_ in the last term:    = E( +     +    V())   When t = 4, insufficient step for 2 steps estimation, resorting to 1 step estimation:    = E( +     V())        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/n_step_targets/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Distributed Tensorflow",
        "excerpt":"This post demonstrates a simple usage example of distributed Tensorflow with Python multiprocessing package.     Distributed Tensorflow with Python multiprocessing package.   A tf.FIFOQueue is used as a storage across processes.   Code on my Github   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     Cluster definition:   2 workers, 1 parameter server.  cluster = tf.train.ClusterSpec({     \"worker\": [\"localhost:2223\",                \"localhost:2224\"               ],     \"ps\": [\"localhost:2225\"] })     Parameter server function:   A tf.Variable (var) &amp; a tf.FIFOQueue (q) is declared with the parameter server. They are both sharable across processes.   For tf.FIFOQueue to be sharable, it has to be declared with the same device (in this case, the ps device) in both the parameter_server function and the worker function. A shared_name has to be given as well.   The tf.Variable (var) is also declared under the ps device. The value of var is displayed in the first for loop.   At the end of the function, the values stored in q will be displayed in the last for loop.   def parameter_server():     with tf.device(\"/job:ps/task:0\"):         var = tf.Variable(0.0, name='var')                 q = tf.FIFOQueue(10, tf.float32, shared_name=\"shared_queue\")      server = tf.train.Server(cluster,                              job_name=\"ps\",                              task_index=0)     sess = tf.Session(target=server.target)      print(\"Parameter server: waiting for cluster connection...\")     sess.run(tf.report_uninitialized_variables())     print(\"Parameter server: cluster ready!\")      print(\"Parameter server: initializing variables...\")     sess.run(tf.global_variables_initializer())     print(\"Parameter server: variables initialized\")      for i in range(10):         print(\"Parameter server: var has value %.1f\" % sess.run(var))         sleep(1.0)         if sess.run(var) == 10.0:           break      sleep(3.0)     print(\"ps q.size(): \", sess.run(q.size()))        for j in range(sess.run(q.size())):         print(\"ps: r\", sess.run(q.dequeue()))      #print(\"Parameter server: blocking...\")     #server.join() # currently blocks forever         print(\"Parameter server: ended...\")   Worker function:   A tf.FIFOQueue (q) is declared with the ps device. A same shared_name is also used.   The tf.Variable (var) is declared under the worker device. It does not have to be declared under the ps device.   The for loop increments the value of var and the values are stored in q.   def worker(worker_n):     with tf.device(\"/job:ps/task:0\"):         q = tf.FIFOQueue(10, tf.float32, shared_name=\"shared_queue\")          with tf.device(tf.train.replica_device_setter(                         worker_device='/job:worker/task:' + str(worker_n),                         cluster=cluster)):         var = tf.Variable(0.0, name='var')      server = tf.train.Server(cluster,                              job_name=\"worker\",                              task_index=worker_n)     sess = tf.Session(target=server.target)      print(\"Worker %d: waiting for cluster connection...\" % worker_n)     sess.run(tf.report_uninitialized_variables())     print(\"Worker %d: cluster ready!\" % worker_n)      while sess.run(tf.report_uninitialized_variables()):         print(\"Worker %d: waiting for variable initialization...\" % worker_n)         sleep(1.0)     print(\"Worker %d: variables initialized\" % worker_n)      for i in range(5):         print(\"Worker %d: incrementing var\" % worker_n, sess.run(var))         sess.run(var.assign_add(1.0))         qe = q.enqueue(sess.run(var))         sess.run(qe)         sleep(1.0)      print(\"Worker %d: ended...\" % worker_n)   Main program:   Create the processes, run them and finally terminate them in a for loop.   ps_proc = Process(target=parameter_server, daemon=True) w1_proc = Process(target=worker, args=(0, ), daemon=True) w2_proc = Process(target=worker, args=(1, ), daemon=True)  ps_proc.start() w1_proc.start() w2_proc.start()  ps_proc.join() # only ps need to call join()  for proc in [w1_proc, w2_proc, ps_proc]:     proc.terminate() # only way to kill server is to kill it's process  print('All done.')               Output:   WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer.  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer. Parameter server: waiting for cluster connection... Worker 0: waiting for cluster connection... Worker 1: waiting for cluster connection... Worker 1: cluster ready! Worker 1: waiting for variable initialization... Parameter server: cluster ready! Parameter server: initializing variables... Parameter server: variables initialized Parameter server: var has value 0.0 Worker 0: cluster ready! /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:65: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size &gt; 0` to check that an array is not empty. Worker 0: variables initialized Worker 0: incrementing var 0.0 /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:65: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size &gt; 0` to check that an array is not empty. Worker 1: variables initialized Worker 1: incrementing var 1.0 Parameter server: var has value 2.0 Worker 0: incrementing var 2.0 Worker 1: incrementing var 3.0 Parameter server: var has value 4.0 Worker 0: incrementing var 4.0 Worker 1: incrementing var 5.0 Parameter server: var has value 6.0 Worker 0: incrementing var 6.0 Worker 1: incrementing var 7.0 Parameter server: var has value 8.0 Worker 0: incrementing var 8.0 Worker 1: incrementing var 9.0 Worker 0: ended... Worker 1: ended... ps q.size():  10 ps: r 1.0 ps: r 2.0 ps: r 3.0 ps: r 4.0 ps: r 5.0 ps: r 6.0 ps: r 7.0 ps: r 8.0 ps: r 9.0 ps: r 10.0 Parameter server: ended... All done.        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/dist_tf/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Accumulate gradients with Tensorflow",
        "excerpt":"This post demonstrates how to accumulate gradients with Tensorflow.     Code on my Github   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     import tensorflow as tf  def accumu_grad(self, OPT, loss, scope):     # retrieve trainable variables in scope of graph     #tvs = tf.trainable_variables(scope=scope + '/actor')     tvs = tf.trainable_variables(scope=scope)      # ceate a list of variables with the same shape as the trainable     accumu = [tf.Variable(tf.zeros_like(tv.initialized_value()), trainable=False) for tv in tvs]      zero_op = [tv.assign(tf.zeros_like(tv)) for tv in accumu] # initialized with 0s      gvs = OPT.compute_gradients(loss, tvs) # obtain list of gradients &amp; variables     #gvs = [(tf.where( tf.is_nan(grad), tf.zeros_like(grad), grad ), var) for grad, var in gvs]      # adds to each element from the list you initialized earlier with zeros its gradient     # accumu and gvs are in same shape, index 0 is grads, index 1 is vars     accumu_op = [accumu[i].assign_add(gv[0]) for i, gv in enumerate(gvs)]      apply_op = OPT.apply_gradients([(accumu[i], gv[1]) for i, gv in enumerate(gvs)]) # apply grads      return zero_op, accumu_op, apply_op, accumu                        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/tf_accumulate_grad/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "A3C multi-threaded discrete version with N step targets",
        "excerpt":"This post documents my implementation of the A3C (Asynchronous Advantage Actor Critic) algorithm (discrete). (multi-threaded discrete version)     A3C (Asynchronous Advantage Actor Critic) implementation with Tensorflow. This is a multi-threaded discrete version. The code is tested with Gym’s discrete action space environment, CartPole-v0 on Colab.     Code on my Github: (missing terms are treated as 0)   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     Code on my Github: (use maximum terms possible)   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     Notations:   Actor network =    Actor network parameter =    Critic network =    Critic network parameter =    Advantage function = A   Number of trajectories = m     Equations:   Actor component: log    Critic component = Advantage function = A =  -    Q values with N-step truncated estimate :    = E( +   +   + … +  V())   Check this post for more information on N-step truncated estimate.   Policy gradient estimator   =    =     log   -    =     log  A     Key implementation details:   The ACNet class defines the models (Tensorflow graphs) and contains both the actor and the critic networks. The Worker class contains the work function that does the main bulk of the computation. A copy of ACNet is declared globally &amp; it’s parameters are shared by the threaded workers. Each worker also have it’s own local copy of ACNet. Workers are instantiated &amp; threaded in the main program.   ACNet class:   Loss function for the actor network for the discrete environment:   with tf.name_scope('actor_loss'):     log_prob = tf.reduce_sum(tf.log(self.action_prob + 1e-5) * tf.one_hot(self.a, num_actions, dtype=tf.float32), axis=1, keep_dims=True)     actor_component = log_prob * tf.stop_gradient(self.baselined_returns)     # entropy for exploration     entropy = -tf.reduce_sum(self.action_prob * tf.log(self.action_prob + 1e-5), axis=1, keep_dims=True)  # encourage exploration     self.actor_loss = tf.reduce_mean( -(ENTROPY_BETA * entropy + actor_component) )                                          Loss function for the critic network for the discrete environment:   TD_err = tf.subtract(self.critic_target, self.V, name='TD_err')       .       .       . with tf.name_scope('critic_loss'):     self.critic_loss = tf.reduce_mean(tf.square(TD_err))   The following function in the ACNet class creates the actor and critic’s neural networks:   def _create_net(self, scope):     w_init = tf.glorot_uniform_initializer()     with tf.variable_scope('actor'):         hidden = tf.layers.dense(self.s, actor_hidden, tf.nn.relu6, kernel_initializer=w_init, name='hidden')         action_prob = tf.layers.dense(hidden, num_actions, tf.nn.softmax, kernel_initializer=w_init, name='action_prob')             with tf.variable_scope('critic'):         hidden = tf.layers.dense(self.s, critic_hidden, tf.nn.relu6, kernel_initializer=w_init, name='hidden')         V = tf.layers.dense(hidden, 1, kernel_initializer=w_init, name='V')              actor_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')     critic_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')            return action_prob, V, actor_params, critic_params   Worker class:   Discounted rewards are used as critic’s targets:   critic_target = self.discount_rewards(buffer_r, GAMMA, V_s)   N-step returns are used in the computation of the Advantage function (baselined_returns):   # Advantage function baselined_returns = n_step_targets - baseline   2 versions of N-step targets could be used:           missing terms are treated as 0.            use maximum terms possible.       Check this post for more information on N-step targets.   The following code segment accumulates gradients &amp; apply them to the local critic network:   self.AC.accumu_grad_critic(feed_dict) # accumulating gradients for local critic   self.AC.apply_accumu_grad_critic(feed_dict)   The following code segment computes the advantage function(baselined_returns):   baseline = SESS.run(self.AC.V, {self.AC.s: buffer_s}) # Value function epr = np.vstack(buffer_r).astype(np.float32) n_step_targets = self.compute_n_step_targets_missing(epr, baseline, GAMMA, N_step) # Q values # Advantage function baselined_returns = n_step_targets - baseline   The following code segment accumulates gradients for the local actor network:   self.AC.accumu_grad_actor(feed_dict) # accumulating gradients for local actor     The following code segment push the parameters from the local networks to the global networks and then pulls the updated global parameters to the local networks:   # update self.AC.push_global_actor(feed_dict)                 self.AC.push_global_critic(feed_dict)     .     .     . self.AC.pull_global()   The following code segment initialize storage for accumulated local gradients.   self.AC.init_grad_storage_actor() # initialize storage for accumulated gradients. self.AC.init_grad_storage_critic()               Check this post for more information on how to accumulate gradients in Tensorflow.   Main program:   The following code segment creates the workers:   workers = [] for i in range(num_workers): # Create worker     i_name = 'W_%i' % i # worker name     workers.append(Worker(i_name, GLOBAL_AC))   The following code segment threads the workers:   worker_threads = [] for worker in workers:     job = lambda: worker.work()     t = threading.Thread(target=job)     t.start()     worker_threads.append(t) COORD.join(worker_threads)     Tensorflow graph:                       References:   Asynchronous Methods for Deep Reinforcement Learning (Mnih, Badia, Mirza, Graves, Harley, Lillicrap, et al., 2016)        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/A3C_disc_thread_nStep/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "A3C multi-threaded continuous version with N step targets",
        "excerpt":"This post documents my implementation of the A3C (Asynchronous Advantage Actor Critic) algorithm. (multi-threaded continuous version)     An A3C (Asynchronous Advantage Actor Critic) implementation with Tensorflow. This is a multi-threaded continuous version. The code is tested with Gym’s continuous action space environment, Pendulum-v0 on Colab.     Code on my Github: (use maximum terms possible)   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     The majority of the code is very similar to the discrete version with the exceptions highlighted in the implementation details section:     Key implementation details:   Action selection:   with tf.name_scope('select_action'):     #mean = mean * action_bound[1]                        mean = mean * ( action_bound[1] - action_bound[0] ) / 2     sigma += 1e-4     normal_dist = tf.distributions.Normal(mean, sigma)                          self.choose_a = tf.clip_by_value(tf.squeeze(normal_dist.sample(1), axis=[0, 1]), action_bound[0], action_bound[1])                     Loss function of the actor network:   with tf.name_scope('actor_loss'):     log_prob = normal_dist.log_prob(self.a)     #actor_component = log_prob * tf.stop_gradient(TD_err)     actor_component = log_prob * tf.stop_gradient(self.baselined_returns)     entropy = -tf.reduce_mean(normal_dist.entropy()) # Compute the differential entropy of the multivariate normal.                        self.actor_loss = -tf.reduce_mean( ENTROPY_BETA * entropy + actor_component)   The following code segment creates a LSTM layer:   def _lstm(self, Inputs, cell_size):         # [time_step, feature] =&gt; [time_step, batch, feature]         s = tf.expand_dims(Inputs, axis=1, name='time_major')           lstm_cell = tf.nn.rnn_cell.LSTMCell(cell_size)         self.init_state = lstm_cell.zero_state(batch_size=1, dtype=tf.float32)         outputs, self.final_state = tf.nn.dynamic_rnn(cell=lstm_cell, inputs=s, initial_state=self.init_state, time_major=True)         # joined state representation                   lstm_out = tf.reshape(outputs, [-1, cell_size], name='flatten_rnn_outputs')           return lstm_out   The following function in the ACNet class creates the actor and critic’s neural networks(note that the critic’s network contains a LSTM layer):   def _create_net(self, scope):     w_init = tf.glorot_uniform_initializer()     #w_init = tf.random_normal_initializer(0., .1)     with tf.variable_scope('actor'):                                 hidden = tf.layers.dense(self.s, actor_hidden, tf.nn.relu6, kernel_initializer=w_init, name='hidden')                     #lstm_out = self._lstm(hidden, cell_size)         # tanh range = [-1,1]         mean = tf.layers.dense(hidden, num_actions, tf.nn.tanh, kernel_initializer=w_init, name='mean')         # softplus range = {0,inf}         sigma = tf.layers.dense(hidden, num_actions, tf.nn.softplus, kernel_initializer=w_init, name='sigma')     with tf.variable_scope('critic'):         hidden = tf.layers.dense(self.s, critic_hidden, tf.nn.relu6, kernel_initializer=w_init, name='hidden')         lstm_out = self._lstm(hidden, cell_size)         V = tf.layers.dense(lstm_out, 1, kernel_initializer=w_init, name='V')       actor_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')     critic_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')     return mean, sigma, V, actor_params, critic_params     Tensorflow graph:                       ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/A3C_cont_thread_nStep/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "A3C distributed tensorflow",
        "excerpt":"This post documents my implementation of the A3C (Asynchronous Advantage Actor Critic) algorithm (Distributed discrete version).     A3C (Asynchronous Advantage Actor Critic) implementation with distributed Tensorflow &amp; Python multiprocessing package. This is a discrete version with N-step targets (use maximum terms possible). The code is tested with Gym’s discrete action space environment, CartPole-v0 on Colab.     Code on my Github   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     The majority of the code is very similar to the discrete version with the exceptions highlighted in the implementation details section:     Key implementation details:   Updating the global episode counter &amp; adding the episodic return to a tf.FIFOqueue at the end of the work() function.   SESS.run(GLOBAL_EP.assign_add(1.0)) qe = GLOBAL_RUNNING_R.enqueue(ep_r) SESS.run(qe)   The distributed Tensorflow part is very similar to a simple example described in this post.   Pin the global variables under the parameter server in both the parameter_server() &amp; worker(worker_n) function:   with tf.device(\"/job:ps/task:0\"):     GLOBAL_AC = ACNet(net_scope, sess, globalAC=None) # only need its params     GLOBAL_EP = tf.Variable(0.0, name='GLOBAL_EP') # num of global episodes        # a queue of ep_r     GLOBAL_RUNNING_R = tf.FIFOQueue(max_global_episodes, tf.float32, shared_name=\"GLOBAL_RUNNING_R\")           In the parameter_server() function, check the size of the tf.FIFOqueue every 1 sec. If it’s full, dequeue the items in a list. the list will be used for display.   while True:     time.sleep(1.0)     #print(\"ps 1 GLOBAL_EP: \", sess.run(GLOBAL_EP))     #print(\"ps 1 GLOBAL_RUNNING_R.size(): \", sess.run(GLOBAL_RUNNING_R.size()))       if sess.run(GLOBAL_RUNNING_R.size()) &gt;= max_global_episodes: # GLOBAL_EP starts from 0, hence +1 to max_global_episodes                   time.sleep(5.0)         #print(\"ps 2 GLOBAL_RUNNING_R.size(): \", sess.run(GLOBAL_RUNNING_R.size()))           GLOBAL_RUNNING_R_list = []         for j in range(sess.run(GLOBAL_RUNNING_R.size())):             ep_r = sess.run(GLOBAL_RUNNING_R.dequeue())             GLOBAL_RUNNING_R_list.append(ep_r) # for display         break        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/A3C_dist_tf/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "DPPO distributed tensorflow",
        "excerpt":"This post documents my implementation of the Distributed Proximal Policy Optimization (Distributed PPO or DPPO) algorithm. (Distributed continuous version)     Distributed Proximal Policy Optimization (Distributed PPO or DPPO) continuous version implementation with distributed Tensorflow and Python’s multiprocessing package. This implementation uses normalized running rewards with GAE. The code is tested with Gym’s continuous action space environment, Pendulum-v0 on Colab.     Code on my Github:   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     Notations:   current policy =    old policy =    epsilon =    Advantage function = A     Equations:   Truncated version of generalized advantage estimation (GAE) =    =    where  =    when  = 1,    =    Probability ratio =    =    Clipped Surrogate Objective function =    =      Key implementation details:   The following class is adapted from OpenAI’s baseline: This class is used for the normalization of rewards in this program before GAE computation.   class RunningStats(object):     def __init__(self, epsilon=1e-4, shape=()):         self.mean = np.zeros(shape, 'float64')         self.var = np.ones(shape, 'float64')         self.std = np.ones(shape, 'float64')         self.count = epsilon      def update(self, x):         batch_mean = np.mean(x, axis=0)         batch_var = np.var(x, axis=0)         batch_count = x.shape[0]         self.update_from_moments(batch_mean, batch_var, batch_count)      def update_from_moments(self, batch_mean, batch_var, batch_count):         delta = batch_mean - self.mean         new_mean = self.mean + delta * batch_count / (self.count + batch_count)         m_a = self.var * self.count         m_b = batch_var * batch_count         M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)         new_var = M2 / (self.count + batch_count)          self.mean = new_mean         self.var = new_var         self.std = np.maximum(np.sqrt(self.var), 1e-6)         self.count = batch_count + self.count   This function in the PPO class is adapted from OpenAI’s Baseline, returns TD lamda return &amp; advantage       def add_vtarg_and_adv(self, R, done, V, v_s_, gamma, lam):         # Compute target value using TD(lambda) estimator, and advantage with GAE(lambda)         # last element is only used for last vtarg, but we already zeroed it if last new = 1         done = np.append(done, 0)         V_plus = np.append(V, v_s_)         T = len(R)         adv = gaelam = np.empty(T, 'float32')         lastgaelam = 0         for t in reversed(range(T)):             nonterminal = 1-done[t+1]                     delta = R[t] + gamma * V_plus[t+1] * nonterminal - V_plus[t]             gaelam[t] = lastgaelam = delta + gamma * lam * nonterminal * lastgaelam            #print(\"adv=\", adv.shape)         #print(\"V=\", V.shape)         #print(\"V_plus=\", V_plus.shape)         tdlamret = np.vstack(adv) + V         #print(\"tdlamret=\", tdlamret.shape)         return tdlamret, adv # tdlamret is critic_target or Qs         The following code segment from the PPO class defines the Clipped Surrogate Objective function:   with tf.variable_scope('surrogate'):                     ratio = self.pi.prob(self.act) / self.oldpi.prob(self.act)                     surr = ratio * self.adv                     self.aloss = -tf.reduce_mean(tf.minimum(surr, tf.clip_by_value(ratio, 1.-epsilon, 1.+epsilon)*self.adv))   The following code segment from the work() function in the worker class normalized the running rewards for each worker:   self.running_stats_r.update(np.array(buffer_r))                     buffer_r = np.clip( (np.array(buffer_r) - self.running_stats_r.mean) / self.running_stats_r.std, -stats_CLIP, stats_CLIP )   The following code segment from the work() function in the worker class computes  the TD lamda return &amp; advantage:   tdlamret, adv = self.ppo.add_vtarg_and_adv(np.vstack(buffer_r), np.vstack(buffer_done), np.vstack(buffer_V), v_s_, GAMMA, lamda)    The following update function in the PPO class does the training &amp; the updating of global &amp; local parameters (Note the at the beginning of training,   probability ratio = 1):   def update(self, s, a, r, adv):         self.sess.run(self.update_oldpi_op)      for _ in range(A_EPOCH): # train actor         self.sess.run(self.atrain_op, {self.state: s, self.act: a, self.adv: adv})         # update actor         self.sess.run([self.push_actor_pi_params,                        self.pull_actor_pi_params],                       {self.state: s, self.act: a, self.adv: adv})     for _ in range(C_EPOCH): # train critic         # update critic         self.sess.run(self.ctrain_op, {self.state: s, self.discounted_r: r})         self.sess.run([self.push_critic_params,                        self.pull_critic_params],                       {self.state: s, self.discounted_r: r})        The distributed Tensorflow &amp; multiprocessing code sections are very similar to the ones describe in the following posts:   A3C distributed tensorflow   Distributed Tensorflow     References:   Proximal Policy Optimization Algorithms (Schulman, Wolski, Dhariwal, Radford, Klimov, 2017)   Emergence of Locomotion Behaviours in Rich Environments (Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, et al., 2017)        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/DPPO_dist_tf/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "RND (Random Network Distillation) with Proximal Policy Optimization (PPO) Tensorflow",
        "excerpt":"This post documents my implementation of the Random Network Distillation (RND) with Proximal Policy Optimization (PPO) algorithm. (continuous version)     Random Network Distillation (RND) with Proximal Policy Optimization (PPO) implentation in Tensorflow. This is a continuous version which solves the mountain car continuous problem (MountainCarContinuous-v0). The RND helps learning with curiosity driven exploration.   The agent starts to converge correctly at around 30 episodes &amp; reached the flag 291 times out of 300 episodes (97% hit rate). It takes 385.09387278556824 seconds to complete 300 episodes on Google’s Colab.   Edit: A new version which corrects a numerical error(causes nan action) takes 780.2065596580505 seconds for 300 episodes. Both versions have similar results. The URL for the new version is updated. Added random seeds for numpy &amp; Tensorflow global seed &amp; ops seed achieve better consistency &amp; faster convergence.   Checkout the resulting charts from the program output.   Code on my Github:           Python file,            Jupyter notebook (The Jupyter notebook, which also contain the resulting charts at the end, can be run directly on Google’s Colab.)       If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     Notations &amp; equations   fixed feature from target network =    predicted feature from predictor network =    intrinsic reward =  = ||  -  ||    For notations &amp; equations regarding PPO, refer to this post.     Key implementation details:   Preprocessing, state featurization:   Prior to training, the states are featurized with the RBF kernel.   (states are also featurized during every training batch.)   Refer to scikit-learn.org documentation: 5.7.2. Radial Basis Function Kernel for more information on RBF kernel.   if state_ftr == True: \"\"\" The following code for state featurization is adapted &amp; modified from dennybritz's repository located at: https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb \"\"\"     # Feature Preprocessing: Normalize to zero mean and unit variance     # We use a few samples from the observation space to do this     states = np.array([env.observation_space.sample() for x in range(sample_size)]) # pre-trained, states preprocessing     scaler = sklearn.preprocessing.StandardScaler()     scaler.fit(states) # Compute the mean and std to be used for later scaling.      # convert states to a featurizes representation.     # We use RBF kernels with different variances to cover different parts of the space     featurizer = sklearn.pipeline.FeatureUnion([ # Concatenates results of multiple transformer objects.             (\"rbf1\", RBFSampler(gamma=5.0, n_components=n_comp)),             (\"rbf2\", RBFSampler(gamma=2.0, n_components=n_comp)),             (\"rbf3\", RBFSampler(gamma=1.0, n_components=n_comp)),             (\"rbf4\", RBFSampler(gamma=0.5, n_components=n_comp))             ])     featurizer.fit(         scaler.transform(states)) # Perform standardization by centering and scaling  # state featurization of state(s) only, # not used on s_ for RND's target &amp; predictor networks def featurize_state(state):     scaled = scaler.transform([state]) # Perform standardization by centering and scaling     featurized = featurizer.transform(scaled) # Transform X separately by each transformer, concatenate results.     return featurized[0]  def featurize_batch_state(batch_states):     fs_list = []     for s in batch_states:         fs = featurize_state(s)         fs_list.append(fs)     return fs_list   Preprocessing, next state normalization for RND:   Variance is computed for the next states buffer_s_ using the RunningStats class. During every training batch, the next states are normalize and clipped.   def state_next_normalize(sample_size, running_stats_s_):    buffer_s_ = []   s = env.reset()   for i in range(sample_size):     a = env.action_space.sample()     s_, r, done, _ = env.step(a)     buffer_s_.append(s_)    running_stats_s_.update(np.array(buffer_s_))   if state_next_normal == True:   state_next_normalize(sample_size, running_stats_s_)     Tensorboard graphs:   Big picture:   There are two main modules, the PPO and the RND.   Current state, state is passed into PPO.   Next state, state_ is passed into RND.        PPO module:   PPO module contains the actor network &amp; the critic network.        PPO’s actor:   At every iteration, an action is sampled from policy network pi.      PPO’s critic:   The critic contains two value function networks. One for extrinsic rewards &amp; one  for intrinsic rewards. Two sets of TD lambda returns &amp; advantages are also  computed.   For extrinsic rewards: tdlamret adv   For intrinsic rewards: tdlamret_i adv_i   The TD lambda returns are used as the PPO’s critics targets in their respective networks while the advantages are summed &amp; used as the advantage in the actor’s loss computation.        RND module:   RND module contains the target network &amp; the predictor network.        RND target network:   The target network is a fixed network, meaning that it’s never trained. It’s weights are randomized once during initialization. The target network is used to encode next states state_. It’s output are encoded next states.        RND predictor network:   The predictor_loss is the intrinsic reward. It is the difference between the predictor network’s output with the target network’s output. The predictor network is trying to guess the target network’s encoded output.        Key to note:   All networks used in this program are linear.   The actor module is basically similar to this DPPO code documented in this post.   The difference is in the critic module. This implementation has two value functions in the critic module rather than one.   The predictor_loss is the intrinsic reward.        Problems encountered:   The actor’s network occasionally returns ‘'’nan’’’ for action. This happens randomly, most likely caused by exploding gradients. Not initializing or randomly initializing actor’s weights results in nan when outputting action.        Program output:   hit_counter 291 0.97   Number of steps per episode:      Reward per episode:      Moving average reward per episode:      — 385.09387278556824 seconds —     References:   Exploration by Random Network Distillation (Yuri Burda, Harrison Edwards, Amos Storkey, Oleg Klimov, 2018)        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/RND/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": ".bash_profile for Mac",
        "excerpt":"This post demonstrates how to create customized functions to bundle commands in a .bash_profile file on Mac.     Edit .bash_profile for Mac.      Start Terminal   Enter “cd ~/” to go to home folder   Edit .bash_profile with “open -e .bash_profile” to open in TextEdit.   Enter “. .bash_profile” to reload .bash_profile.     Examples   To bundle common git operations, add the following to .bash_profile file:   function lazy_git() {     git checkout test_ver     git add .     git commit -a -m \"$1\"     git checkout master     git merge test_ver     git push     git checkout test_ver }    To bundle common jekyll operations, add the following to .bash_profile file:   The command serve runs localhost.   function lazy_jekyll_serve() {     cd /Users/tester/gitHubRepo/ChuaCheowHuan.github.io     pwd     bundle exec jekyll serve }   The command build build the site. This command is neccessary for generating sitemap.xml &amp; robot.txt.   function lazy_jekyll_build() {     cd /Users/tester/gitHubRepo/ChuaCheowHuan.github.io     pwd     bundle exec jekyll build }        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/bash_script/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Tensorflow graphs in Tensorboard",
        "excerpt":"This post demonstrate how setup &amp; access Tensorflow graphs.     In order to access Tensorflow graphs, you need to use Tensorboard which comes will Tensorflow installed.   The following snippet shows how to setup a FileWriter with a Tensorflow graph.   tf.reset_default_graph()  # Your Tensorflow graph goes here. # ...  sess = tf.Session() sess.run(tf.global_variables_initializer())  # Declare tf.summary.FileWriter where log is your output directory for # Tensorboard &amp; add the graph to the writer. writer = tf.summary.FileWriter('log', sess.graph)  # Run your training loop # sess.run(...)  writer.close()   Run this command in terminal to start tensorboard:  $ tensorboard --logdir log  $ tensorboard --logdir ~/ray_results  Where log is the log folder.   Navigate to http://127.0.0.1:6006 in your browser to access Tensorflow. Your graph is in the graph tab.        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/tf_graph/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Custom MARL (multi-agent reinforcement learning) CDA (continuous double auction) environment",
        "excerpt":"A custom MARL (multi-agent reinforcement learning) environment where multiple agents trade against one another in a CDA (continuous double auction).     Code on my Github     The environment doesn’t use any external data. Data is generated by self play of the agents themselves through their interaction with the limit order book.   At each time step, the environment emits the top k rows of the aggregated order book as observations to the agents.     Example:  An example of using RLlib to pit 1 PPO (Proximal Policy Optimization) agent against 3 random agents using this CDA environment is available in:  CDA_env_disc_RLlib.py   To run:  $ cd gym-continuousDoubleAuction/gym_continuousDoubleAuction  $ python CDA_env_disc_RLlib.py   Sample training output results:  . . . Result for PPO_continuousDoubleAuction-v0_0:   custom_metrics: {}   date: 2019-09-30_21-16-20   done: true   episode_len_mean: 1001.0   episode_reward_max: 0.0   episode_reward_mean: 0.0   episode_reward_min: 0.0   episodes_this_iter: 4   episodes_total: 38   experiment_id: 56cbdad4389343eca5cfd49eadeb3554   hostname: Duality0.local   info:     grad_time_ms: 15007.219     learner:       policy_0:         cur_kl_coeff: 0.0003906250058207661         cur_lr: 4.999999873689376e-05         entropy: 10.819798469543457         entropy_coeff: 0.0         kl: 8.689265087014064e-06         model: {}         policy_loss: 153.9163055419922         total_loss: 843138688.0         vf_explained_var: 0.0         vf_loss: 843138496.0     num_steps_sampled: 40000     num_steps_trained: 40000     opt_peak_throughput: 266.538     opt_samples: 4000.0     sample_peak_throughput: 80.462     sample_time_ms: 49713.208     update_time_ms: 176.14   iterations_since_restore: 10   node_ip: 192.168.1.12   num_healthy_workers: 2   off_policy_estimator: {}   pid: 10220   policy_reward_mean:     policy_0: 12414.421052631578     policy_1: -301.39473684210526     policy_2: -952.1578947368421     policy_3: -11160.868421052632   sampler_perf:     mean_env_wait_ms: 18.1753569144153     mean_inference_ms: 4.126144958830859     mean_processing_ms: 1.5262831265657335   time_since_restore: 649.1416146755219   time_this_iter_s: 61.54709506034851   time_total_s: 649.1416146755219   timestamp: 1569849380   timesteps_since_restore: 40000   timesteps_this_iter: 4000   timesteps_total: 40000   training_iteration: 10   trial_id: ea67f638  2019-09-30 21:16:20,507\tWARNING util.py:145 -- The `process_trial` operation took 0.4397752285003662 seconds to complete, which may be a performance bottleneck. 2019-09-30 21:16:21,407\tWARNING util.py:145 -- The `experiment_checkpoint` operation took 0.899777889251709 seconds to complete, which may be a performance bottleneck. == Status == Using FIFO scheduling algorithm. Resources requested: 0/4 CPUs, 0/0 GPUs Memory usage on this node: 3.3/4.3 GB Result logdir: /Users/hadron0/ray_results/PPO Number of trials: 1 ({'TERMINATED': 1}) TERMINATED trials:  - PPO_continuousDoubleAuction-v0_0:\tTERMINATED, [3 CPUs, 0 GPUs], [pid=10220], 649 s, 10 iter, 40000 ts, 0 rew  == Status == Using FIFO scheduling algorithm. Resources requested: 0/4 CPUs, 0/0 GPUs Memory usage on this node: 3.3/4.3 GB Result logdir: /Users/hadron0/ray_results/PPO Number of trials: 1 ({'TERMINATED': 1}) TERMINATED trials:  - PPO_continuousDoubleAuction-v0_0:\tTERMINATED, [3 CPUs, 0 GPUs], [pid=10220], 649 s, 10 iter, 40000 ts, 0 rew   Running the following tensorboard command &amp; navigate to localhost:6006 in your browser to access the tensorboard graphs:  $ tensorboard --logdir ~/ray_results   The figure below from Tensorboard shows the agents’ performance:       PPO agent is using policy 0 while policies 1 to 3 are used by the random agents.   Dependencies:  Please see requirements.txt in this repository.   Installation:  The environment is installable via pip.  $ cd gym-continuousDoubleAuction  $ pip install -e .   TODO:  1) custom RLlib workflow to include custom RND + PPO policies. 2) parametric or hybrid action space 3) more robust tests 3) better documentation   Acknowledgements:  The orderbook matching engine is adapted from https://github.com/dyn4mik3/OrderBook   Contributing:  Please see CONTRIBUTING.md.   Disclaimer:  This repository is only meant for research purposes &amp; is never meant to be used in any form of trading. Past performance is no guarantee of future results. If you suffer losses from using this repository, you are the sole person responsible for the losses. The author will NOT be held responsible in any way.        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/MARL_CDA_env/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Random policy in RLlib",
        "excerpt":"Creating &amp; seeding a random policy class in RLlib.     Code on my Github     Function:   def make_RandomPolicy(_seed):      # a hand-coded policy that acts at random in the env (doesn't learn)     class RandomPolicy(Policy):         \"\"\"Hand-coded policy that returns random actions.\"\"\"         def __init__(self, observation_space, action_space, config):             self.observation_space = observation_space             self.action_space = action_space             self.action_space.seed(_seed)          def compute_actions(self,                             obs_batch,                             state_batches,                             prev_action_batch=None,                             prev_reward_batch=None,                             info_batch=None,                             episodes=None,                             **kwargs):             \"\"\"Compute actions on a batch of observations.\"\"\"             return [self.action_space.sample() for _ in obs_batch], [], {}          def learn_on_batch(self, samples):             \"\"\"No learning.\"\"\"             #return {}             pass          def get_weights(self):             pass          def set_weights(self, weights):             pass      return RandomPolicy     Usage example:   # Setup PPO with an ensemble of `num_policies` different policies     policies = {\"policy_{}\".format(i): gen_policy(i) for i in range(args.num_policies)} # contains many \"policy_graphs\" in a policies dictionary      # override policy with random policy     policies[\"policy_{}\".format(args.num_policies-3)] = (make_RandomPolicy(1), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary     policies[\"policy_{}\".format(args.num_policies-2)] = (make_RandomPolicy(2), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary     policies[\"policy_{}\".format(args.num_policies-1)] = (make_RandomPolicy(3), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary  ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/RLlib_rand_policy/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Dockerized Postgres connection with Django web app in Travis CI",
        "excerpt":"Introducing a delay to allow proper connection between dockerized Postgres &amp; Django web app in Travis CI.     Code on my Github     If you see the following error in the Travis’s job log while attempting to test dockerized Django apps with Travis, it means that the postgres docker container has started but not yet ready to accept connections.   psycopg2.OperationalError: could not connect to server: Connection refused 539\tIs the server running on host \"db\" (172.18.0.2) and accepting 540\tTCP/IP connections on port 5432?  . . .  django.db.utils.OperationalError: could not connect to server: Connection refused 587\tIs the server running on host \"db\" (172.18.0.2) and accepting 588\tTCP/IP connections on port 5432?  The command \"docker-compose run web python manage.py test\" exited with 1.   A solution for this issue is to introduce a delay until connection is ready before executing the test.   The delay has to be implemented in the docker-compose.yml file before migration &amp; running of Django’s server shown below:   command: bash -c 'while !&lt;/dev/tcp/db/5432; do sleep 1; done; python3 manage.py migrate'   command: bash -c 'while !&lt;/dev/tcp/db/5432; do sleep 1; done; python3 manage.py runserver 0.0.0.0:8000'     Config files:   These are the relevant config files used in a Django project with the delay introduced in the docker-compose.yml file. The actual command to run the test is in the .travis.yml file.   The database configuration in settings.py  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.postgresql',         'NAME': 'postgres',         'USER': 'postgres',         'HOST': 'db',         'PORT': 5432,         #'PORT': 5433,     } }   The Dockerfile:  FROM python:3 WORKDIR /usr/src/app ADD requirements.txt /usr/src/app RUN pip install -r requirements.txt ADD . /usr/src/app   The docker-compose.yml file:  version: '3'  services:     db:         image: postgres     migration:         build: . #        command: python3 manage.py migrate         command: bash -c 'while !&lt;/dev/tcp/db/5432; do sleep 1; done; python3 manage.py migrate'         volumes:             - .:/usr/src/app         depends_on:             - db     web:         build: . #        command: python3 manage.py runserver 0.0.0.0:8000         command: bash -c 'while !&lt;/dev/tcp/db/5432; do sleep 1; done; python3 manage.py runserver 0.0.0.0:8000'         volumes:             - .:/usr/src/app         ports:             - \"8000:8000\"         depends_on:             - db             - migration   The .travis.yml file:  language: python python:     - 3.6 services:     - docker #    - postgres install:     - pip install -r requirements.txt #before_script: #    - psql -c 'create database testdb;' -U postgres #    - psql -c 'create database travisci;' -U postgres script: #    - docker-compose build #    - docker-compose run web python manage.py migrate     - docker-compose run web python manage.py test #    - python manage.py test     After introducing the delay, this is the successful test output in Travis’s job log.   . . . ....... 528---------------------------------------------------------------------- 529Ran 10 tests in 0.126s 530 531OK 532Destroying test database for alias 'default'... 533The command \"docker-compose run web python manage.py test\" exited with 0.     References:   See this post in stackoverflow.        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/docker_travis/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Django + Postgres + Docker + Travis CI + Heroku CD",
        "excerpt":"Basic workflow of testing a dockerized Django &amp; Postgres web app with Travis (continuous integration) &amp; deployment to Heroku (continuous deployment).     Prerequisite:   1) This post assumes that the reader has accounts with Github, Travis &amp; Heroku &amp; already has the accounts configured. For example, linking Travis with Github, adding a Postgres server in Heroku &amp; setting OS environment variables in Travis &amp; Heroku websites.   2) Basic working knowledge of Django &amp; Docker.     Code on my Github     Which copy of Postgres to use during the different stages in the workflow?   During development, we won’t be using a local copy of Postgres database server. The Docker’s copy of Postgres is used.   When testing in Travis, we don’t have to ask Travis for a copy of Postgres, we won’t be using Travis’s copy, we’ll be using the Docker’s copy.   During deployment, we have to use Heroku’s copy.     makemigrations   Always run docker-compose web run python manage.py makemigrations before deployment to Heroku or in our case, before pushing to Github.   The actual python manage.py migrate for the Postgres server addon from Heroku will be run in the Procfile file.     Listing files &amp; directories in a tree:   cd web_app_DPDTH  $ tree -a -I \"CS50_web_dev|staticfiles|static|templates|LICENSE|README.md|__init__.py|settings_DPTH_.py|urls.py|wsgi.py|db.sqlite3|airline4_tests_.py|apps.py|migrations|views.py|models.py|flights.csv|manage.py|wait-for-it.sh|admin.py|.git|.travis_DPTH_.yml|__pycache__\"  . ├── .travis.yml ├── .travis_DPDTH_.yml ├── .travis_old.yml ├── Dockerfile ├── Procfile ├── airline │   └── settings.py ├── docker-compose.yml ├── docker_push.sh ├── flights │   └── tests.py ├── heroku-container-release.sh └── requirements.txt    As shown in the tree above, the 9 files that matter in the workflow:   1) tests.py   2) settings.py   3) requirements.txt   4) Dockerfile   5) docker-compose.yml   6) .travis.yml   7) docker_push.sh   8) heroku-container-release.sh   9) Procfile   We will look at the contents of each of the 9 files in the sections below.     tests.py   This is the test file that Travis will use for testing the app. You write whatever test you want for Travis to run with.   from django.db.models import Max from django.test import Client, TestCase  from .models import Airport, Flight, Passenger  # Create your tests here. class FlightsTestCase(TestCase):      def setUp(self):          # Create airports.         a1 = Airport.objects.create(code=\"AAA\", city=\"City A\")         a2 = Airport.objects.create(code=\"BBB\", city=\"City B\")          # Create flights.         Flight.objects.create(origin=a1, destination=a2, duration=100)         Flight.objects.create(origin=a1, destination=a1, duration=200)         Flight.objects.create(origin=a2, destination=a1, duration=300)      # 1     def test_departures_count(self):         a = Airport.objects.get(code=\"AAA\")         self.assertEqual(a.departures.count(), 2)      # 2     def test_arrivals_count(self):         a = Airport.objects.get(code=\"AAA\")         self.assertEqual(a.arrivals.count(), 2)      # 3     def test_valid_flight(self):         a1 = Airport.objects.get(code=\"AAA\")         a2 = Airport.objects.get(code=\"BBB\")         f = Flight.objects.get(origin=a1, destination=a2)         self.assertTrue(f.is_valid_flight())     settings.py   Under the database section, the DATABASES['default'] sets the default database so the default database is the one connected by the OS environment variable DATABASE_URL, however if this is unavailable, we’ll use the one defined with 'HOST': 'db'.   This setup allows us to use 'HOST': 'db' which is the Docker’s copy of postgres during development phase &amp; also during tesing phase with Travis while using the Heroku’s copy during deployment which is provided by connecting to the DATABASE_URL.   The DATABASE_URL, as an OS environment variable which is generated by Heroku after a Database is added to the web app in the Heroku website.   Add and/or edit the following to the settings.py file:   import django_heroku import dj_database_url   MIDDLEWARE = [     'django.middleware.security.SecurityMiddleware',      'whitenoise.middleware.WhiteNoiseMiddleware',  # new      'django.contrib.sessions.middleware.SessionMiddleware',     'django.middleware.common.CommonMiddleware',     'django.middleware.csrf.CsrfViewMiddleware',     'django.contrib.auth.middleware.AuthenticationMiddleware',     'django.contrib.messages.middleware.MessageMiddleware',     'django.middleware.clickjacking.XFrameOptionsMiddleware', ]   DATABASES = {     'default': {         'ENGINE': 'django.db.backends.postgresql',         'NAME': 'postgres',         'USER': 'postgres',         'PASSWORD': 'postgres',         'HOST': 'db', # Docker's copy of postgres         'PORT': 5432,         #'PORT': 5433,     } }  DATABASE_URL = os.environ.get('DATABASE_URL') db_from_env = dj_database_url.config(default=DATABASE_URL, conn_max_age=500, ssl_require=True) DATABASES['default'].update(db_from_env)   STATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')   django_heroku.settings(locals())      requirements.txt   This file lets Docker &amp; Travis know what packages are needed for the app. This is needed in Dockerfile &amp; .travis.yml files.   django&gt;=2.0.11 psycopg2 psycopg2-binary dj-database-url==0.5.0 gunicorn whitenoise django-heroku pytz sqlparse     Dockerfile   This contains the instructions for building a Docker image.   The CMD gunicorn airline.wsgi:application --bind 0.0.0.0:$PORT tells Docker to use gunicorn as the web server. See here for details.   FROM python:3  WORKDIR /usr/src/app  ADD requirements.txt /usr/src/app  RUN pip install -r requirements.txt  ADD . /usr/src/app  # collect static files RUN python manage.py collectstatic --noinput  CMD gunicorn airline.wsgi:application --bind 0.0.0.0:$PORT     docker-compose.yml   This contains the instructions on how to run a Docker containers which is an instance of a Docker image.   Notice the sleep delay introduced in the 2 command: sections. See here for details.   version: '3'  services:     db:         image: postgres     migration:         build: .         command: bash -c 'while !&lt;/dev/tcp/db/5432; do sleep 1; done; python3 manage.py migrate'         volumes:             - .:/usr/src/app         depends_on:             - db     web:         build: . #        container_name: webapp-dpdth         image: webapp-dpdth         command: bash -c 'while !&lt;/dev/tcp/db/5432; do sleep 1; done; python3 manage.py runserver 0.0.0.0:8000'         volumes:             - .:/usr/src/app         ports:             - \"8000:8000\"         depends_on:             - db             - migration      .travis.yml   This file contains instructions for Travis. Notice that we’re not using the Postgres from Travis because we’re using Postgres from Docker directly. The postgresql is therefore commented out under the services: section.   Under script:, we ask Travis to run the test using Docker with the docker-compose command.   Under deploy:, we execute a docker_push.sh script, more details in the sections below.   The skip_cleanup: true tells Travis not to remove any files that it deems unnecessary after deployment. Travis does not have permission to do that on Heroku anyway.   language: python python:     - 3.6 services:     - docker #    - postgresql install:     - pip install -r requirements.txt script:     - docker-compose run web python manage.py test deploy:     provider: script     script: bash docker_push.sh     skip_cleanup: true     on:         branch: master     Workflow for after testing with Travis to deployment to Heroku   The main workflow after testing to deployment is as such:   tag image -&gt; push image to registry -&gt; release image   We’ll see how to do that in the following script files:   1) docker_push.sh   2) heroku-container-release.sh     docker_push.sh   This file does several things listed as follows:   1) Login to the Heroku’s image registry.   2) tag the source image webapp-dpdth:latest to the target image registry.heroku.com/webapp-dpdth/web. Replace webapp-dpdth with your app name on Heroku.   3) Push the target image to Heroku’s registry if the branch tested on Travis is a master branch &amp; that it’s not a PR.   4) Change ownership &amp; permission of files to allow Travis to execute the heroku-container-release.sh script.   #!/bin/bash  sudo docker login --username $HEROKU_DOCKER_USERNAME --password $HEROKU_AUTH_TOKEN registry.heroku.com sudo docker tag webapp-dpdth:latest registry.heroku.com/webapp-dpdth/web if [ $TRAVIS_BRANCH == \"master\" ] &amp;&amp; [ $TRAVIS_PULL_REQUEST == \"false\" ]; then sudo docker push registry.heroku.com/webapp-dpdth/web; fi  chmod +x heroku-container-release.sh sudo chown $USER:docker ~/.docker sudo chown $USER:docker ~/.docker/config.json sudo chmod g+rw ~/.docker/config.json  ./heroku-container-release.sh      heroku-container-release.sh   This file is for releasing a Docker image via Heroku’s API. Replace webapp-dpdth with your app name on Heroku.   #!/bin/bash imageId=$(docker inspect registry.heroku.com/webapp-dpdth/web --format={{.Id}}) payload='{\"updates\":[{\"type\":\"web\",\"docker_image\":\"'\"$imageId\"'\"}]}' curl -n -X PATCH https://api.heroku.com/apps/webapp-dpdth/formation \\ -d \"$payload\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/vnd.heroku+json; version=3.docker-releases\" \\ -H \"Authorization: Bearer $HEROKU_AUTH_TOKEN\"   See here for details.     Procfile   This file is for Heroku. The command in the release: section will run after a Docker image is released. It will run the migrate command with --noinput option. Without running migrate, the database on Heroku may not function correctly.   It also tells Heroku to deploy the web app using Gunicorn as the production server.   Note that airline is the Django project name. It’s not the web app name in the Django project &amp; is also not the web app name in Heroku.   release: python manage.py migrate --noinput web: gunicorn airline.wsgi     The deployed web app   With the above files in place, push to Github &amp; Travis will start testing. After all tests passed, deployment starts. If there isn’t any failures, the web app will be running on:   https://webapp-dpdth.herokuapp.com   This link brings you to the admin page. It is using the Heroku’s copy of Postgres.   This link brings you to my built log in Travis.com which shows how a successful test/deploy built looks like.     Web security:   Please note that web security has not been throughly consider in this basic workflow describe above. Do NOT simply use the above workflow for production.   For example the SECRET_KEY in the settings.py isn’t dealt with at all and web security is really beyond the scope of this post.        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/DPDTH/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Django + Postgres + Travis CI + Heroku CD",
        "excerpt":"Basic workflow of testing a Django &amp; Postgres web app with Travis (continuous integration) &amp; deployment to Heroku (continuous deployment).     Prerequisite:   This post assumes that the reader has accounts with Github, Travis &amp; Heroku &amp; already has the accounts configured. For example, linking Travis with Github, setting up Postgres server in Heroku &amp; setting OS environment variables in Travis &amp; Heroku websites.     Code on my Github     See here for a Dockerized version.     Which copy of Postgres to use during the different stages in the workflow?   During development, the local Postgres database server is used. When testing in Travis, we’ll used Travis’s copy of Postgres &amp; when deploying, we’ll have to use Heroku’s copy.     makemigrations   Always run python manage.py makemigrations before deployment to Heroku or in our case, before pushing to Github.   The actual python manage.py migrate for the Postgres server addon from Heroku will be run by the deploy section in the .travis.yml file.     Listing files &amp; directories in a tree:   cd web_app_DPTH  $ tree -a -I \"CS50_web_dev|staticfiles|static|templates|LICENSE|README.md|__init__.py|settings_DPTH_.py|urls.py|wsgi.py|db.sqlite3|airline4_tests_.py|apps.py|migrations|views.py|models.py|flights.csv|manage.py|wait-for-it.sh|admin.py|.git|.travis_DPTH_.yml|Dockerfile|docker-compose.yml\"    . ├── .travis.yml ├── Procfile ├── airline │   └── settings.py ├── flights │   └── tests.py └── requirements.txt   As shown in the tree above, the 5 files that matter in the workflow:   1) tests.py   2) settings.py   3) requirements.txt   4) .travis.yml   5) Procfile   We will look at the contents of each of the 5 files in the sections below.     tests.py   This is the test file that Travis will use for testing the app. You write whatever test you want for Travis to run with.   from django.db.models import Max from django.test import Client, TestCase  from .models import Airport, Flight, Passenger  # Create your tests here. class FlightsTestCase(TestCase):      def setUp(self):          # Create airports.         a1 = Airport.objects.create(code=\"AAA\", city=\"City A\")         a2 = Airport.objects.create(code=\"BBB\", city=\"City B\")          # Create flights.         Flight.objects.create(origin=a1, destination=a2, duration=100)         Flight.objects.create(origin=a1, destination=a1, duration=200)         Flight.objects.create(origin=a2, destination=a1, duration=300)      # 1     def test_departures_count(self):         a = Airport.objects.get(code=\"AAA\")         self.assertEqual(a.departures.count(), 2)      # 2     def test_arrivals_count(self):         a = Airport.objects.get(code=\"AAA\")         self.assertEqual(a.arrivals.count(), 2)      # 3     def test_valid_flight(self):         a1 = Airport.objects.get(code=\"AAA\")         a2 = Airport.objects.get(code=\"BBB\")         f = Flight.objects.get(origin=a1, destination=a2)         self.assertTrue(f.is_valid_flight())     settings.py   Under the database section, the database credentials, as OS environment variables, has to be made available to Travis &amp; Heroku. They can be set in their respective websites.   Add and/or edit the following to the settings.py file:   import django_heroku import dj_database_url   MIDDLEWARE = [     'django.middleware.security.SecurityMiddleware',      'whitenoise.middleware.WhiteNoiseMiddleware',  # new      'django.contrib.sessions.middleware.SessionMiddleware',     'django.middleware.common.CommonMiddleware',     'django.middleware.csrf.CsrfViewMiddleware',     'django.contrib.auth.middleware.AuthenticationMiddleware',     'django.contrib.messages.middleware.MessageMiddleware',     'django.middleware.clickjacking.XFrameOptionsMiddleware', ]   DATABASES = {     'default': {         'ENGINE': 'django.db.backends.postgresql',         'NAME': os.environ['DATABASE_NAME'],         'USER': os.environ['DATABASE_USER'],         'PASSWORD': os.environ['DATABASE_PASSWORD'],         'HOST': os.environ['DATABASE_HOST'],         'PORT': os.environ['DATABASE_PORT'],     } }   STATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')   django_heroku.settings(locals())      requirements.txt   This file lets Travis know what packages are needed for the app.   django&gt;=2.0.11 psycopg2 psycopg2-binary dj-database-url==0.5.0 gunicorn whitenoise django-heroku pytz sqlparse     .travis.yml   This file contains instructions for Travis &amp; is needed when Travis starts running. $HEROKU_API_KEY can be generated from the Heroku website &amp; stored as an OS environment variable in the Travis website. The test is done with the Travis’s copy of Postgres.   language: python python:     - 3.6 services:     - postgresql install:     - pip install -r requirements.txt script:     - python manage.py test deploy:     provider: heroku     api_key: $HEROKU_API_KEY     app: webapp-dpth     run: python manage.py migrate     on: master     Procfile   This file is for Heroku. It tells Heroku to deploy the web app using Gunicorn as the production server.   Note that airline is the Django project name.   web: gunicorn airline.wsgi     The deployed web app   With the above files in place, push to Github &amp; Travis will start testing. After all tests passed, deployment starts. If there isn’t any failures, the web app will be running on:   https://webapp-dpth.herokuapp.com   This link brings you to the admin page. It is using the Heroku’s copy of Postgres.   This link brings you to my built log in Travis.com which shows how a successful test/deploy built looks like.     Web security:   Please note that web security has not been throughly consider in this basic workflow describe above. Do NOT simply use the above workflow for production.   For example the SECRET_KEY in the settings.py isn’t dealt with at all and web security is really beyond the scope of this post.        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/DPTH/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Reinforcement learning custom environment in Sagemaker with Ray (RLlib)",
        "excerpt":"Demo setup for simple (reinforcement learning) custom environment in Sagemaker. This example uses Proximal Policy Optimization with Ray (RLlib).     Code on my Github     The training script:   import json import os  import gym import ray from ray.tune import run_experiments import ray.rllib.agents.a3c as a3c import ray.rllib.agents.ppo as ppo from ray.tune.registry import register_env from mod_op_env import ArrivalSim  from sagemaker_rl.ray_launcher import SageMakerRayLauncher  \"\"\" def create_environment(env_config):     import gym #     from gym.spaces import Space     from gym.envs.registration import register     # This import must happen inside the method so that worker processes import this code     register(         id='ArrivalSim-v0',         entry_point='env:ArrivalSim',         kwargs= {'price' : 40}     )     return gym.make('ArrivalSim-v0') \"\"\" def create_environment(env_config):     price = 30.0     # This import must happen inside the method so that worker processes import this code     from mod_op_env import ArrivalSim     return ArrivalSim(price)   class MyLauncher(SageMakerRayLauncher):     def __init__(self):                 super(MyLauncher, self).__init__()         self.num_gpus = int(os.environ.get(\"SM_NUM_GPUS\", 0))         self.hosts_info = json.loads(os.environ.get(\"SM_RESOURCE_CONFIG\"))[\"hosts\"]         self.num_total_gpus = self.num_gpus * len(self.hosts_info)      def register_env_creator(self):         register_env(\"ArrivalSim-v0\", create_environment)      def get_experiment_config(self):         return {           \"training\": {             \"env\": \"ArrivalSim-v0\",             \"run\": \"PPO\",             \"stop\": {               \"training_iteration\": 3,             },              \"local_dir\": \"/opt/ml/model/\",             \"checkpoint_freq\" : 3,              \"config\": {                                               #\"num_workers\": max(self.num_total_gpus-1, 1),               \"num_workers\": max(self.num_cpus-1, 1),               #\"use_gpu_for_workers\": False,               \"train_batch_size\": 128, #5,               \"sample_batch_size\": 32, #1,               \"gpu_fraction\": 0.3,               \"optimizer\": {                 \"grads_per_step\": 10               },             },             #\"trial_resources\": {\"cpu\": 1, \"gpu\": 0, \"extra_gpu\": max(self.num_total_gpus-1, 1), \"extra_cpu\": 0},             #\"trial_resources\": {\"cpu\": 1, \"gpu\": 0, \"extra_gpu\": max(self.num_total_gpus-1, 0),             #                    \"extra_cpu\": max(self.num_cpus-1, 1)},             \"trial_resources\": {\"cpu\": 1,                                 \"extra_cpu\": max(self.num_cpus-1, 1)},                         }         }  if __name__ == \"__main__\":     os.environ[\"LC_ALL\"] = \"C.UTF-8\"     os.environ[\"LANG\"] = \"C.UTF-8\"     os.environ[\"RAY_USE_XRAY\"] = \"1\"     print(ppo.DEFAULT_CONFIG)     MyLauncher().train_main()     The Jupyter notebook:   !/bin/bash ./setup.sh  from time import gmtime, strftime import sagemaker role = sagemaker.get_execution_role()  sage_session = sagemaker.session.Session() s3_bucket = sage_session.default_bucket()   s3_output_path = 's3://{}/'.format(s3_bucket) print(\"S3 bucket path: {}\".format(s3_output_path))  job_name_prefix = 'ArrivalSim'  from sagemaker.rl import RLEstimator, RLToolkit, RLFramework  estimator = RLEstimator(entry_point=\"mod_op_train.py\", # Our launcher code                         source_dir='src', # Directory where the supporting files are at. All of this will be                                           # copied into the container.                         dependencies=[\"common/sagemaker_rl\"], # some other utils files.                         toolkit=RLToolkit.RAY, # We want to run using the Ray toolkit against the ray container image.                         framework=RLFramework.TENSORFLOW, # The code is in tensorflow backend.                         toolkit_version='0.5.3', # Toolkit version. This will also choose an apporpriate tf version.                                                                        #toolkit_version='0.6.5', # Toolkit version. This will also choose an apporpriate tf version.                                                 role=role, # The IAM role that we created at the begining.                         #train_instance_type=\"ml.m4.xlarge\", # Since we want to run fast, lets run on GPUs.                         train_instance_type=\"local\", # Since we want to run fast, lets run on GPUs.                         train_instance_count=1, # Single instance will also work, but running distributed makes things                                                 # fast, particularly in the case of multiple rollout training.                         output_path=s3_output_path, # The path where we can expect our trained model.                         base_job_name=job_name_prefix, # This is the name we setup above to be to track our job.                         hyperparameters = {      # Some hyperparameters for Ray toolkit to operate.                           \"s3_bucket\": s3_bucket,                           \"rl.training.stop.training_iteration\": 2, # Number of iterations.                           \"rl.training.checkpoint_freq\": 2,                         },                         #metric_definitions=metric_definitions, # This will bring all the logs out into the notebook.                     )  estimator.fit()     The container in Sagemaker Jupyter notebook instance:        See related issue/motivation.        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/sagemaker_RL_custom_env/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Custom Sagemaker reinforcement learning container",
        "excerpt":"Building &amp; testing custom Sagemaker RL container.   Instead of using the official SageMaker supported version of Ray RLlib (version 0.5.3 &amp; 0.6.5), I want to use version 0.7.3. In order to do so, I have to build &amp; test my custom Sagemaker RL container.     The Dockerfile:   Add the Dockerfile below to sagemaker-rl-container/ray/docker/0.7.3/:   ARG processor #FROM 520713654638.dkr.ecr.us-west-2.amazonaws.com/sagemaker-tensorflow-scriptmode:1.14.0-$processor-py3 FROM 520713654638.dkr.ecr.us-west-2.amazonaws.com/sagemaker-tensorflow-scriptmode:1.12.0-$processor-py3  RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\         build-essential \\         jq \\         libav-tools \\         libjpeg-dev \\         libxrender1 \\         python3.6-dev \\         python3-opengl \\         wget \\         xvfb &amp;&amp; \\     apt-get clean &amp;&amp; \\     rm -rf /var/lib/apt/lists/*  RUN pip install --no-cache-dir \\     Cython==0.29.7 \\     gym==0.14.0 \\     lz4==2.1.10 \\     opencv-python-headless==4.1.0.25 \\     PyOpenGL==3.1.0 \\     pyyaml==5.1.1 \\     redis&gt;=3.2.2 \\     ray==0.7.3 \\     ray[rllib]==0.7.3 \\     scipy==1.3.0 \\     requests  # https://click.palletsprojects.com/en/7.x/python3/ ENV LC_ALL=C.UTF-8 ENV LANG=C.UTF-8  # Copy workaround script for incorrect hostname COPY lib/changehostname.c /  COPY lib/start.sh /usr/local/bin/start.sh RUN chmod +x /usr/local/bin/start.sh  # Starts framework ENTRYPOINT [\"bash\", \"-m\", \"start.sh\"]     Remove unneeded test files:   Backup the test folder as test_bkup in sagemaker-rl-container/.   Remove the following files not used in testing in sagemaker-rl-container/test/integration/local/:   test_coach.py test_vw_cb_explore.py test_vw_cbify.py test_vw_serving.py     Add/replace codes in test files to get role:   In the sagemaker-rl-container/test/conftest.pyfile, add/replace the following:   from sagemaker import get_execution_role   #parser.addoption('--role', default='SageMakerContainerBuildIntegrationTests') parser.addoption('--role', default=get_execution_role()),    In the following files:   sagemaker-rl-container/test/integration/local/test_gym.py sagemaker-rl-container/test/integration/local/test_ray.py   Add/replace the following:   from sagemaker import get_execution_role   #role='SageMakerRole', role = get_execution_role(),     Build the image:   In SageMaker, start a Jupyter notebook instance &amp; open a terminal.   Login into SageMaker ECR account:   $ (aws ecr get-login --no-include-email --region &lt;region&gt; --registry-ids &lt;AWS_ACC_ID&gt;) $ (aws ecr get-login --no-include-email --region us-west-2 --registry-ids 520713654638)   Copy &amp; paste the output from the above command into the terminal &amp; press Enter.     Pull the base Tensorflow image from the aws ecr:   $ docker pull 520713654638.dkr.ecr.us-west-2.amazonaws.com/sagemaker-tensorflow-scriptmode:1.12.0-cpu-py3   Build the Ray image using the Dockerfile.tf from above:   $ docker build -t custom-smk-rl-ctn:tf-1.12.0-ray-0.7.3-cpu-py3 -f ray/docker/0.7.3/Dockerfile.tf --build-arg processor=cpu .     Local testing:   Install dependencies for testing:   $ cd sagemaker-rl-container $ pip install .   Run the command below for local testing:   clear &amp;&amp; \\ docker images &amp;&amp; \\ pytest test/integration/local --framework tensorflow \\                               --toolkit ray \\                               --toolkit-version 0.7.3 \\                               --docker-base-name custom-smk-rl-ctn \\                               --tag tf-1.12.0-ray-0.7.3-cpu-py3 \\                               --processor cpu | tee local_test_output.txt   The output from the test will be saved in local_test_output.txt.     Pushing to registry on AWS ECR:   $ (aws ecr get-login --no-include-email --region &lt;region&gt; --registry-ids &lt;AWS_ACC_ID&gt;) $ (aws ecr get-login --no-include-email --region us-west-2  --registry-ids 123456789012) # Copy &amp; paste output to terminal &amp; press enter.  $ aws ecr create-repository --repository-name &lt;repo_name&gt; $ aws ecr create-repository --repository-name custom-smk-rl-ctn  $ docker tag &lt;image_ID&gt; &lt;AWS_ACC_ID&gt;.dkr.ecr.us-west-2.amazonaws.com/&lt;repo_name&gt;:&lt;tag&gt; $ docker tag ba542f0b9706 &lt;123456789012&gt;.dkr.ecr.us-west-2.amazonaws.com/custom-smk-rl-ctn:tf-1.12.0-cpu-py3 $ docker tag ba542f0b9706 &lt;123456789012&gt;.dkr.ecr.us-west-2.amazonaws.com/custom-smk-rl-ctn:tf-1.12.0-ray-0.7.3-cpu-py3  $ docker push &lt;AWS_ACC_ID&gt;.dkr.ecr.us-west-2.amazonaws.com/&lt;repo_name&gt;:&lt;tag&gt; $ docker push &lt;123456789012&gt;.dkr.ecr.us-west-2.amazonaws.com/custom-smk-rl-ctn:tf-1.12.0-cpu-py3 $ docker push &lt;123456789012&gt;.dkr.ecr.us-west-2.amazonaws.com/custom-smk-rl-ctn:tf-1.12.0-ray-0.7.3-cpu-py3   $ aws ecr describe-repositories  $ aws ecr list-images --repository-name &lt;repo_name&gt; $ aws ecr list-images --repository-name custom-smk-rl-ctn     Testing with AWS SageMaker ML instance:   Run the command below for testing with SageMaker:   clear &amp;&amp; \\ docker images &amp;&amp; \\ pytest test/integration/sagemaker --aws-id 123456789012 \\                                   --instance-type ml.m4.xlarge \\                                   --framework tensorflow \\                                   --toolkit ray \\                                   --toolkit-version 0.7.3 \\                                   --docker-base-name custom-smk-rl-ctn \\                                   --tag tf-1.12.0-ray-0.7.3-cpu-py3 | tee SageMaker_test_output.txt   The output from the test will be saved in SageMaker_test_output.txt.     Pushing to registry on Docker hub:   $ docker login  $ docker tag &lt;image_ID&gt; &lt;DockerHubUserName&gt;/&lt;repo_name&gt;:&lt;tag&gt; $ docker tag ba542f0b9706 &lt;DockerHubUserName&gt;/custom-smk-rl-ctn:tf-1.12.0-cpu-py3 $ docker tag ba542f0b9706 &lt;DockerHubUserName&gt;/custom-smk-rl-ctn:tf-1.12.0-ray-0.7.3-cpu-py3  $ docker push &lt;DockerHubUserName&gt;/&lt;repo_name&gt;:&lt;tag&gt; $ docker push &lt;DockerHubUserName&gt;/custom-smk-rl-ctn:tf-1.12.0-cpu-py3 $ docker push &lt;DockerHubUserName&gt;/custom-smk-rl-ctn:tf-1.12.0-ray-0.7.3-cpu-py3     Training with custom SageMaker RL container:     Useful Docker commands:   $ docker ps -a  $ docker images  $ docker rm &lt;container&gt;  $ docker rmi &lt;image&gt;     Useful AWS commands:   $ aws ecr delete-repository --force --repository-name &lt;repo_name&gt;     References:   https://github.com/aws/sagemaker-rl-container        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/custom_sagemaker_RL_container/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"}]
