var store = [{
        "title": "DQN",
        "excerpt":"A Deep Q Network implementation in tensorflow with target network &amp; random experience replay.   Environment from OpenAI’s gym: CartPole-v0   Code      Notations   Model network =    Model parameter =    Model network Q value =  (s, a)     Target network =    Target parameter =    Target network Q value =  (, )      Equations   TD target = r (s, a)     (, )     TD  error = (TD target)  (Model network Q value)    = [r (s, a)     (, )]   (s, a)      Implementation details   Update target parameter  with model parameter  :     Copy  to  with either soft or hard parameter update.       Hard parameter update:                    Soft parameter update:         polyak    + (1  polyak)                          Stop TD target from contributing to gradient computation:            References   Human-level control through deep reinforcement learning (Mnih et al., 2015)      ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/DQN/",
        "teaser":"http://localhost:4000/assets/images/foggy.jpg"},{
        "title": "DDQN",
        "excerpt":"A Double Deep Q Network (DDQN) implementation in tensorflow with random experience replay.   Environment from OpenAI’s gym: CartPole-v0   Code      Notations   Model network =    Model parameter =    Model network Q value =  (s, a)     Target network =    Target parameter =    Target network Q value =  (, )      Equations   TD target = r (s, a)    (,   (s, a))     TD  error = (TD target)  (Model network Q value)    = [r (s, a)    (,   (s, a))]   (s, a)      Implementation details   Create a placeholder to feed Q values from model network:         Select Q values from model network using  as features &amp; feed them to the training session:         Select minibatch actions with largest Q values from model network, create indices &amp; select corresponding minibatch actions from target network:            References   Deep Reinforcement Learning with Double Q-learning (Hasselt, Guez &amp; Silver, 2016)      ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/DDQN/",
        "teaser":"http://localhost:4000/assets/images/foggy.jpg"},{
        "title": "Dueling DDQN",
        "excerpt":"A Dueling Double Deep Q Network (Dueling DDQN) implementation in tensorflow with random experience replay.   Environment from OpenAI’s gym: CartPole-v0   Code      Notations   Network =    Parameter =    Network Q value =  (s, a)     Value function = V(s)   Advantage function = A(s, a)     Parameter from the Advantage function layer =    Parameter from the Value function layer =       Equations   (eqn 9) from the original paper (Wang et al., 2015):   Q(s, a; , , ) = V(s; , )     [ A(s, a; , )    A(s, ; , ) ]      Implementation details   V represents the value function layer, A represents the Advantage function layer:            References   Dueling Network Architectures for Deep Reinforcement Learning (Wang et al., 2015)      ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/Duel_DDQN/",
        "teaser":"http://localhost:4000/assets/images/foggy.jpg"},{
        "title": "Dueling DDQN with PER",
        "excerpt":"A Dueling Double Deep Q Network with Priority Experience Replay (Duel DDQN with PER) implementation in tensorflow.   Environment from OpenAI’s gym: CartPole-v0   Code      Notations   Model network =    Model parameter =    Model network Q value =  (s, a)     Target network =    Target parameter =    Target network Q value =  (, )     A small constant to ensure that no sample has 0 probability to be selected = e   Hyper parameter  =      Decides how to sample, range from 0 to 1, where 0 corresponds to fully uniformly random sample selection &amp; 1 corresponding to selecting samples based on highest priority.    Hyper parameter  =      Starts close to 0, gradually annealed  to 1, slowly giving more importance to weights during training.    Minibatch size = k   Replay memory size = N      Equations   TD target = r (s, a)    (,   (s, a))     TD  error =     = (TD target)  (Model network Q value)    = [r (s, a)    (,   (s, a))]   (s, a)        =     =   e     probability(i) = P(i)    =      weights =  = (N  P(i))       Implementation details   Sum tree:      Assume an example of a sum tree with 7 nodes (with 4 leaves which corresponds to the replay memory size):            At initialization:                                 When item 1 is added:                                 When item 2 is added:                                 When item 3 is added:                                 When item 4 is added:                                 When item 5 is added:                            Figure below shows the corresponding code &amp; array contents. The tree represents the entire sum tree while data represents the leaves.                   In the implementation, only one sumTree object is needed to store the collected experiences, this sumTree object resides in the Replay_memory class. The sumTree object has number of leaves = replay memory size = capacity. The data array in sumTree object stores an Exp object, which is a sample of experience.      The following code decides how to sample:         Refer to appendix B.2.1, under the section, “Proportional prioritization”, from the original (Schaul et al., 2016) paper for sampling details.      References   Prioritized experience replay (Schaul et al., 2016)      ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/Duel_DDQN_with_PER/",
        "teaser":"http://localhost:4000/assets/images/foggy.jpg"}]
