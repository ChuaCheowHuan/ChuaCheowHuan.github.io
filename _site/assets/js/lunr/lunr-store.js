var store = [{
        "title": "DQN",
        "excerpt":"This post documents my implementation of the Deep Q Network (DQN) algorithm.     A Deep Q Network implementation in tensorflow with target network &amp; random experience replay. The code is tested with Gym’s discrete action space environment, CartPole-v0 on Colab.   Code on my Github   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     Notations:   Model network =    Model parameter =    Model network Q value =  (s, a)   Target network =    Target parameter =    Target network Q value =  (, )     Equations:   TD target = r (s, a)     (, )   TD error = (TD target)  (Model network Q value) = [r (s, a)     (, )]   (s, a)     Key implementation details:   Update target parameter  with model parameter . Copy  to  with either soft or hard parameter update.   Hard parameter update:   with tf.variable_scope('hard_replace'):   self.target_replace_hard = [t.assign(m) for t, m in zip(self.target_net_params, self.model_net_params)]      # hard params replacement if self.learn_step % self.tau_step == 0:     self.sess.run(self.target_replace_hard)   self.learn_step += 1   Soft parameter update: polyak    + (1  polyak)       with tf.variable_scope('soft_replace'):               self.target_replace_soft = [t.assign(self.polyak * m + (1 - self.polyak) * t)                               for t, m in zip(self.target_net_params, self.model_net_params)]      Stop TD target from contributing to gradient computation:   # exclude td_target in gradient computation td_target = tf.stop_gradient(td_target)     Tensorflow graph:        References:   Human-level control through deep reinforcement learning (Mnih et al., 2015)        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/DQN/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "DDQN",
        "excerpt":"This post documents my implementation of the Double Deep Q Network (DDQN) algorithm.     A Double Deep Q Network (DDQN) implementation in tensorflow with random experience replay. The code is tested with Gym’s discrete action space environment, CartPole-v0 on Colab.   Code on my Github   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     Notations:   Model network =    Model parameter =    Model network Q value =  (s, a)   Target network =    Target parameter =    Target network Q value =  (, )     Equations:   TD target = r (s, a)    (,   (s, a))   TD error = (TD target)  (Model network Q value) = [r (s, a)    (,   (s, a))]   (s, a)     Key implementation details:   Create a placeholder to feed Q values from model network:   self.model_s_next_Q_val = tf.placeholder(tf.float32, [None,self.num_actions], name='model_s_next_Q_val')   Select Q values from model network using  as features &amp; feed them to the training session:   # select actions from model network model_s_next_Q_val = self.sess.run(self.model_Q_val, feed_dict={self.s: s_next})  # training _, loss = self.sess.run([self.optimizer, self.loss],                         feed_dict = {self.s: s,                                      self.a: a,                                      self.r: r,                                      self.s_next: s_next,                                      self.done: done,                                      self.model_s_next_Q_val: model_s_next_Q_val})   Select minibatch actions with largest Q values from model network, create indices &amp; select corresponding minibatch actions from target network:   def td_target(self, s_next, r, done, model_s_next_Q_val, target_Q_val):     # select action with largest Q value from model network     model_max_a = tf.argmax(model_s_next_Q_val, axis=1, output_type=tf.dtypes.int32)      arr = tf.range(tf.shape(model_max_a)[0], dtype=tf.int32) # create row indices     indices = tf.stack([arr, model_max_a], axis=1) # create 2D indices             max_target_Q_val = tf.gather_nd(target_Q_val, indices) # select minibatch actions from target network     max_target_Q_val = tf.reshape(max_target_Q_val, (self.minibatch_size,1))     Tensorflow graph:        References:   Deep Reinforcement Learning with Double Q-learning (Hasselt, Guez &amp; Silver, 2016)        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/DDQN/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Dueling DDQN",
        "excerpt":"This post documents my implementation of the Dueling Double Deep Q Network (Dueling DDQN) algorithm.     A Dueling Double Deep Q Network (Dueling DDQN) implementation in tensorflow with random experience replay. The code is tested with Gym’s discrete action space environment, CartPole-v0 on Colab.   Code on my Github   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     Notations:   Network =    Parameter =    Network Q value =  (s, a)   Value function = V(s)   Advantage function = A(s, a)   Parameter from the Advantage function layer =    Parameter from the Value function layer =      Equations:   (eqn 9) from the original paper (Wang et al., 2015):   Q(s, a; , , ) = V(s; , )  [ A(s, a; , )    A(s, ; , ) ]     Key implementation details:   V represents the value function layer, A represents the Advantage function layer:  # contruct neural network def built_net(self, var_scope, w_init, b_init, features, num_hidden, num_output):                   with tf.variable_scope(var_scope):                 feature_layer = tf.contrib.layers.fully_connected(features, num_hidden,                                                         activation_fn = tf.nn.relu,                                                         weights_initializer = w_init,                                                         biases_initializer = b_init)       V = tf.contrib.layers.fully_connected(feature_layer, 1,                                             activation_fn = None,                                             weights_initializer = w_init,                                             biases_initializer = b_init)       A = tf.contrib.layers.fully_connected(feature_layer, num_output,                                             activation_fn = None,                                             weights_initializer = w_init,                                             biases_initializer = b_init)          Q_val = V + (A - tf.reduce_mean(A, reduction_indices=1, keepdims=True)) # refer to eqn 9 from the original paper               return Q_val        Tensorflow graph:        References:   Dueling Network Architectures for Deep Reinforcement Learning (Wang et al., 2015)        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/Duel_DDQN/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Dueling DDQN with PER",
        "excerpt":"This post documents my implementation of the Dueling Double Deep Q Network with Priority Experience Replay (Duel DDQN with PER) algorithm.     A Dueling Double Deep Q Network with Priority Experience Replay (Duel DDQN with PER) implementation in tensorflow. The code is tested with Gym’s discrete action space environment, CartPole-v0 on Colab.   Code on my Github   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     Notations:   Model network =    Model parameter =    Model network Q value =  (s, a)   Target network =    Target parameter =    Target network Q value =  (, )   A small constant to ensure that no sample has 0 probability to be selected = e   Hyper parameter  =       Decides how to sample, range from 0 to 1, where 0 corresponds to fully uniformly random sample selection &amp; 1 corresponding to selecting samples based on highest priority.   Hyper parameter  =       Starts close to 0, gradually annealed  to 1, slowly giving more importance to weights during training.   Minibatch size = k   Replay memory size = N     Equations:   TD target = r (s, a)    (,   (s, a))   TD error =  = (TD target)  (Model network Q value) = [r (s, a)    (,   (s, a))]   (s, a)    =  =   e   probability(i) = P(i) =    weights =  = (N  P(i))      Key implementation details:   Sum tree:   Assume an example of a sum tree with 7 nodes (with 4 leaves which corresponds to the replay memory size):   At initialization:        When item 1 is added:        When item 2 is added:        When item 3 is added:        When item 4 is added:        When item 5 is added:        Figure below shows the corresponding code &amp; array contents. The tree represents the entire sum tree while data represents the leaves.      In the implementation, only one sumTree object is needed to store the collected experiences, this sumTree object resides in the Replay_memory class. The sumTree object has number of leaves = replay memory size = capacity. The data array in sumTree object stores an Exp object, which is a sample of experience.   The following code decides how to sample:   def sample(self, k): # k = minibatch size     batch = []      # total_p() gives the total sum of priorities of the leaves in the sumTree     # which is the value stored in the root node     segment = self.tree.total_p() / k      for i in range(k):         a = segment * i # start of segment         b = segment * (i + 1) # end of segment         s = np.random.uniform(a, b) # rand value between a, b          (idx, p, data) = self.tree.get(s)         batch.append( (idx, p, data) )                  return batch       Refer to appendix B.2.1, under the section, “Proportional prioritization”, from the original (Schaul et al., 2016) paper for sampling details.     Tensorflow graph:        References:   Prioritized experience replay (Schaul et al., 2016)        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/Duel_DDQN_with_PER/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Numpy array manipulation",
        "excerpt":"This post provides a simple usage examples for common Numpy array manipulation.     This Jupyter notebook contains simple examples on how to manipulate Numpy arrays. The code blocks below shows the codes &amp; it’s corresponding display output.     Code on my Github   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     Setting up a Numpy array:   buffer=[0,1] print('buffer=', buffer) $buffer= [0, 1]  new=2 print('new=', new) $new= 2  buffer = np.array(buffer + [new]) # append a new item &amp; create a numpy array print('np.array(buffer + [new])=', buffer) $np.array(buffer + [new])= [0 1 2]   Slicing examples:   # numpy array slicing syntax # buffer[start:stop:step]  print('buffer[1:]=', buffer[1:]) # starting from index 1 $buffer[1:]= [1 2]  print('buffer[-1:]=', buffer[-1:]) # getting item in last index $buffer[-1:]= [2]  print('buffer[:1]=', buffer[:1]) # stop at index 1 (exclusive), keep only 1st item $buffer[:1]= [0]  print('buffer[:-1]=', buffer[:-1]) # stop at last index (exclusive), discard item in last index $buffer[:-1]= [0 1]  print('buffer[::-1]=', buffer[::-1]) # start from last index (reversal) $buffer[::-1]= [2 1 0]  print('buffer[1::-1]=', buffer[1::-1]) # reverse starting from index 1 $buffer[1::-1]= [1 0]  # Starting from index 1 will return [1 2], reversing will return [2,1] print('buffer[1:][::-1]=', buffer[1:][::-1]) $buffer[1:][::-1]= [2 1]   np.newaxis is an alias for None:   # np.newaxis = None  print('buffer[:, np.newaxis]=', buffer[:, np.newaxis]) $buffer[:, np.newaxis]= [[0][1][2]]  print('buffer[:, None]=', buffer[:, None]) $buffer[:, None]= [[0][1][2]]  print('buffer[np.newaxis, :]=', buffer[np.newaxis, :]) $buffer[np.newaxis, :]= [[0 1 2]]  print('buffer[None, :]=', buffer[None, :]) $buffer[None, :]= [[0 1 2]]   Stacking:   a = [1,2,3] b = [4,5,6] c = [7,8,9]  r = np.hstack((a,b,c)) # horizontal stacking print(\"r=\", r) $r= [1 2 3 4 5 6 7 8 9]  QUEUE = queue.Queue() QUEUE.put(a) QUEUE.put(b) QUEUE.put(c)  r = [QUEUE.get() for _ in range(QUEUE.qsize())] print(r) $[[1, 2, 3], [4, 5, 6], [7, 8, 9]]  r = np.vstack(r) # vertical stacking print(r) $[[1 2 3]   [4 5 6]   [7 8 9]]  print(r[:, ::-1]) # col reversal $[[3 2 1]   [6 5 4]   [9 8 7]]        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/np_array_manipulation/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Python's multiprocessing package",
        "excerpt":"This post demonstrates how to use the Python’s multiprocessing package to achieve parallel data generation.     The main program has a chief that spawns multiple worker processes. Each worker  spawns a single work process. The work process generates random integer data  [1,3].   Each worker has it’s own local queue. When data is generated, it is stored in it’s local queue. When the local queue’s size is greater than 5, the data is retrieved &amp; 0.1 is added to the data, this result is stored in the Chief’s global queue. When the Chief’s global queue’s size is greater than 3, the result is retrieved &amp; printed on screen.     Code on my Github   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     The Worker class:   class Worker(object):   def __init__(self, worker_id, g_queue):     self.g_queue = g_queue     self.worker_id = worker_id     self.queue = Queue() # local worker queue     self.work_process = Process(target=self.work, args=())     self.work_process.start()     info(worker_id, self.work_process, \"Worker\")    def work(self):      info(self.worker_id, self.work_process, \"work\")      while True:       data = np.random.randint(1,4)       self.queue.put(data)        # process data in queue       if self.queue.qsize() &gt; 5:         data = self.queue.get()         result = data + 0.1         self.g_queue.put(result) # send result to global queue        time.sleep(1) # work every x sec interval      return self.w_id     The Chief class:   class Chief(object):   def __init__(self, num_workers):     self.g_queue = Queue() # global queue         self.num_workers = num_workers    def dispatch_workers(self):        worker_processes = [Process(target=Worker(w_id, self.g_queue), args=()) for w_id in range(num_workers)]     return worker_processes    def result(self):     if self.g_queue.qsize() &gt; 3:       result = self.g_queue.get()       print(\"result\", result)   The main program:   if __name__ == '__main__':     print('main parent process id:', os.getppid())   print('main process id:', os.getpid())    num_workers = 2   chief = Chief(num_workers)   workers_processes = chief.dispatch_workers()    i = 0   while True:         time.sleep(2) # chk g_queue every x sec interval to get result     chief.result()     print(\"i=\", i)      if i&gt;9:       break     i+=1       A helper display function:   def info(worker_id, process, function_name):     print(\"worker_id=\", worker_id,           'module name:', __name__,           'function name:', function_name,           'parent process:', os.getppid(),           'current process id:', os.getpid(),           'spawn process id:', process.pid)        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/py_mpp/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "N-step targets",
        "excerpt":"This post documents my implementation of the N-step Q-values estimation algorithm.     N-step Q-values estimation.   Code on my Github   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.   The following two functions computes truncated Q-values estimates:   A) n_step_targets_missing      treats missing terms as 0.   B) n_step_targets_max      use maximum terms possible.     Equations:   1-step truncated estimate:    = E( +    V())   2-step truncated estimate:    = E( +     +    V())   3-step truncated estimate:    = E( +     +     +    V())   N-step truncated estimate:    = E( +     +     + … +    V())     Example:   Assuming we have the following variables setup:   N=2 # N steps gamma=2 t=5 v_s_ = 10 # value of next state  epr=np.arange(t).reshape(t,1) print(\"epr=\", epr)  baselines=np.arange(t).reshape(t,1) print(\"baselines=\", baselines)   Display output of episodic rewards(epr) &amp; baselines:   epr= [[0]  [1]  [2]  [3]  [4]]  baselines= [[0]  [1]  [2]  [3]  [4]]     A) This function computes the n-step targets, treats missing terms as zero:   # if number of steps unavailable, missing terms treated as 0. def n_step_targets_missing(epr, baselines, gamma, N):   N = N+1   targets = np.zeros_like(epr)       if N &gt; epr.size:     N = epr.size   for t in range(epr.size):        print(\"t=\", t)     for n in range(N):       print(\"n=\", n)       if t+n == epr.size:                     print('missing terms treated as 0, break') # last term for those with insufficient steps.         break # missing terms treated as 0       if n == N-1: # last term         targets[t] += (gamma**n) * baselines[t+n] # last term for those with sufficient steps         print('last term for those with sufficient steps, end inner n loop')       else:         targets[t] += (gamma**n) * epr[t+n] # non last terms   return targets   Run the function n_step_targets_missing:   print('n_step_targets_missing:') T = n_step_targets_missing(epr, baselines, gamma, N) print(T)   Display the output:   n_step_targets_missing: t= 0 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 1 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 2 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 3 n= 0 n= 1 n= 2 missing terms treated as 0, break t= 4 n= 0 n= 1 missing terms treated as 0, break [[10]  [17]  [24]  [11]  [ 4]]   For the output above, note that when t+n = 5 which is greater than the last index 4, missing terms are treated as 0.     B) This function computes the n-step targets, it will use maximum number of terms possible:   # if number of steps unavailable, use max steps available. # uses v_s_ as input def n_step_targets_max(epr, baselines, v_s_, gamma, N):   N = N+1   targets = np.zeros_like(epr)       if N &gt; epr.size:     N = epr.size   for t in range(epr.size):       print(\"t=\", t)     for n in range(N):       print(\"n=\", n)       if t+n == epr.size:                     targets[t] += (gamma**n) * v_s_ # last term for those with insufficient steps.         print('last term for those with INSUFFICIENT steps, break')         break       if n == N-1:         targets[t] += (gamma**n) * baselines[t+n] # last term for those with sufficient steps         print('last term for those with sufficient steps, end inner n loop')       else:         targets[t] += (gamma**n) * epr[t+n] # non last terms   return targets   Run the function n_step_targets_max:   print('n_step_targets_max:') T = n_step_targets_max(epr, baselines, v_s_, gamma, N) print(T)   Display the output:   n_step_targets_max: t= 0 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 1 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 2 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 3 n= 0 n= 1 n= 2 last term for those with INSUFFICIENT steps, break t= 4 n= 0 n= 1 last term for those with INSUFFICIENT steps, break [[10]  [17]  [24]  [51]  [24]]   For the output above, note that when t+n = 5 which is greater than the last index 4, maximum terms are used where possible. ( Last term for those with INSUFFICIENT steps is given by   (gamma**n) * v_s_ =  V()), where v_s_ = V()   When t = 2, normal 2 steps estimation:    = E( +     +    V())   When t = 3, 2 steps estimation with insufficient step, using v_s_ in the last term:    = E( +     +    V())   When t = 4, insufficient step for 2 steps estimation, resorting to 1 step estimation:    = E( +     V())        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/n_step_targets/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Distributed Tensorflow",
        "excerpt":"This post demonstrates a simple usage example of distributed Tensorflow with Python multiprocessing package.     Distributed Tensorflow with Python multiprocessing package.   A tf.FIFOQueue is used as a storage across processes.   Code on my Github   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     Cluster definition:   2 workers, 1 parameter server.  cluster = tf.train.ClusterSpec({     \"worker\": [\"localhost:2223\",                \"localhost:2224\"               ],     \"ps\": [\"localhost:2225\"] })     Parameter server function:   A tf.Variable (var) &amp; a tf.FIFOQueue (q) is declared with the parameter server. They are both sharable across processes.   For tf.FIFOQueue to be sharable, it has to be declared with the same device (in this case, the ps device) in both the parameter_server function and the worker function. A shared_name has to be given as well.   The tf.Variable (var) is also declared under the ps device. The value of var is displayed in the first for loop.   At the end of the function, the values stored in q will be displayed in the last for loop.   def parameter_server():     with tf.device(\"/job:ps/task:0\"):         var = tf.Variable(0.0, name='var')                 q = tf.FIFOQueue(10, tf.float32, shared_name=\"shared_queue\")      server = tf.train.Server(cluster,                              job_name=\"ps\",                              task_index=0)     sess = tf.Session(target=server.target)      print(\"Parameter server: waiting for cluster connection...\")     sess.run(tf.report_uninitialized_variables())     print(\"Parameter server: cluster ready!\")      print(\"Parameter server: initializing variables...\")     sess.run(tf.global_variables_initializer())     print(\"Parameter server: variables initialized\")      for i in range(10):         print(\"Parameter server: var has value %.1f\" % sess.run(var))         sleep(1.0)         if sess.run(var) == 10.0:           break      sleep(3.0)     print(\"ps q.size(): \", sess.run(q.size()))        for j in range(sess.run(q.size())):         print(\"ps: r\", sess.run(q.dequeue()))      #print(\"Parameter server: blocking...\")     #server.join() # currently blocks forever         print(\"Parameter server: ended...\")   Worker function:   A tf.FIFOQueue (q) is declared with the ps device. A same shared_name is also used.   The tf.Variable (var) is declared under the worker device. It does not have to be declared under the ps device.   The for loop increments the value of var and the values are stored in q.   def worker(worker_n):     with tf.device(\"/job:ps/task:0\"):         q = tf.FIFOQueue(10, tf.float32, shared_name=\"shared_queue\")          with tf.device(tf.train.replica_device_setter(                         worker_device='/job:worker/task:' + str(worker_n),                         cluster=cluster)):         var = tf.Variable(0.0, name='var')      server = tf.train.Server(cluster,                              job_name=\"worker\",                              task_index=worker_n)     sess = tf.Session(target=server.target)      print(\"Worker %d: waiting for cluster connection...\" % worker_n)     sess.run(tf.report_uninitialized_variables())     print(\"Worker %d: cluster ready!\" % worker_n)      while sess.run(tf.report_uninitialized_variables()):         print(\"Worker %d: waiting for variable initialization...\" % worker_n)         sleep(1.0)     print(\"Worker %d: variables initialized\" % worker_n)      for i in range(5):         print(\"Worker %d: incrementing var\" % worker_n, sess.run(var))         sess.run(var.assign_add(1.0))         qe = q.enqueue(sess.run(var))         sess.run(qe)         sleep(1.0)      print(\"Worker %d: ended...\" % worker_n)   Main program:   Create the processes, run them and finally terminate them in a for loop.   ps_proc = Process(target=parameter_server, daemon=True) w1_proc = Process(target=worker, args=(0, ), daemon=True) w2_proc = Process(target=worker, args=(1, ), daemon=True)  ps_proc.start() w1_proc.start() w2_proc.start()  ps_proc.join() # only ps need to call join()  for proc in [w1_proc, w2_proc, ps_proc]:     proc.terminate() # only way to kill server is to kill it's process  print('All done.')               Output:   WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer.  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer. Parameter server: waiting for cluster connection... Worker 0: waiting for cluster connection... Worker 1: waiting for cluster connection... Worker 1: cluster ready! Worker 1: waiting for variable initialization... Parameter server: cluster ready! Parameter server: initializing variables... Parameter server: variables initialized Parameter server: var has value 0.0 Worker 0: cluster ready! /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:65: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size &gt; 0` to check that an array is not empty. Worker 0: variables initialized Worker 0: incrementing var 0.0 /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:65: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size &gt; 0` to check that an array is not empty. Worker 1: variables initialized Worker 1: incrementing var 1.0 Parameter server: var has value 2.0 Worker 0: incrementing var 2.0 Worker 1: incrementing var 3.0 Parameter server: var has value 4.0 Worker 0: incrementing var 4.0 Worker 1: incrementing var 5.0 Parameter server: var has value 6.0 Worker 0: incrementing var 6.0 Worker 1: incrementing var 7.0 Parameter server: var has value 8.0 Worker 0: incrementing var 8.0 Worker 1: incrementing var 9.0 Worker 0: ended... Worker 1: ended... ps q.size():  10 ps: r 1.0 ps: r 2.0 ps: r 3.0 ps: r 4.0 ps: r 5.0 ps: r 6.0 ps: r 7.0 ps: r 8.0 ps: r 9.0 ps: r 10.0 Parameter server: ended... All done.        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/dist_tf/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Accumulate gradients with Tensorflow",
        "excerpt":"This post demonstrates how to accumulate gradients with Tensorflow.     Code on my Github   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     import tensorflow as tf  def accumu_grad(self, OPT, loss, scope):     # retrieve trainable variables in scope of graph     #tvs = tf.trainable_variables(scope=scope + '/actor')     tvs = tf.trainable_variables(scope=scope)      # ceate a list of variables with the same shape as the trainable     accumu = [tf.Variable(tf.zeros_like(tv.initialized_value()), trainable=False) for tv in tvs]      zero_op = [tv.assign(tf.zeros_like(tv)) for tv in accumu] # initialized with 0s      gvs = OPT.compute_gradients(loss, tvs) # obtain list of gradients &amp; variables     #gvs = [(tf.where( tf.is_nan(grad), tf.zeros_like(grad), grad ), var) for grad, var in gvs]      # adds to each element from the list you initialized earlier with zeros its gradient     # accumu and gvs are in same shape, index 0 is grads, index 1 is vars     accumu_op = [accumu[i].assign_add(gv[0]) for i, gv in enumerate(gvs)]      apply_op = OPT.apply_gradients([(accumu[i], gv[1]) for i, gv in enumerate(gvs)]) # apply grads      return zero_op, accumu_op, apply_op, accumu                        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/tf_accumulate_grad/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "A3C multi-threaded discrete version with N step targets",
        "excerpt":"This post documents my implementation of the A3C (Asynchronous Advantage Actor Critic) algorithm (discrete). (multi-threaded discrete version)     A3C (Asynchronous Advantage Actor Critic) implementation with Tensorflow. This is a multi-threaded discrete version. The code is tested with Gym’s discrete action space environment, CartPole-v0 on Colab.     Code on my Github: (missing terms are treated as 0)   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     Code on my Github: (use maximum terms possible)   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     Notations:   Actor network =    Actor network parameter =    Critic network =    Critic network parameter =    Advantage function = A   Number of trajectories = m     Equations:   Actor component: log    Critic component = Advantage function = A =  -    Q values with N-step truncated estimate :    = E( +   +   + … +  V())   Check this post for more information on N-step truncated estimate.   Policy gradient estimator   =    =     log   -    =     log  A     Key implementation details:   The ACNet class defines the models (Tensorflow graphs) and contains both the actor and the critic networks. The Worker class contains the work function that does the main bulk of the computation. A copy of ACNet is declared globally &amp; it’s parameters are shared by the threaded workers. Each worker also have it’s own local copy of ACNet. Workers are instantiated &amp; threaded in the main program.   ACNet class:   Loss function for the actor network for the discrete environment:   with tf.name_scope('actor_loss'):     log_prob = tf.reduce_sum(tf.log(self.action_prob + 1e-5) * tf.one_hot(self.a, num_actions, dtype=tf.float32), axis=1, keep_dims=True)     actor_component = log_prob * tf.stop_gradient(self.baselined_returns)     # entropy for exploration     entropy = -tf.reduce_sum(self.action_prob * tf.log(self.action_prob + 1e-5), axis=1, keep_dims=True)  # encourage exploration     self.actor_loss = tf.reduce_mean( -(ENTROPY_BETA * entropy + actor_component) )                                          Loss function for the critic network for the discrete environment:   TD_err = tf.subtract(self.critic_target, self.V, name='TD_err')       .       .       . with tf.name_scope('critic_loss'):     self.critic_loss = tf.reduce_mean(tf.square(TD_err))   The following function in the ACNet class creates the actor and critic’s neural networks:   def _create_net(self, scope):     w_init = tf.glorot_uniform_initializer()     with tf.variable_scope('actor'):         hidden = tf.layers.dense(self.s, actor_hidden, tf.nn.relu6, kernel_initializer=w_init, name='hidden')         action_prob = tf.layers.dense(hidden, num_actions, tf.nn.softmax, kernel_initializer=w_init, name='action_prob')             with tf.variable_scope('critic'):         hidden = tf.layers.dense(self.s, critic_hidden, tf.nn.relu6, kernel_initializer=w_init, name='hidden')         V = tf.layers.dense(hidden, 1, kernel_initializer=w_init, name='V')              actor_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')     critic_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')            return action_prob, V, actor_params, critic_params   Worker class:   Discounted rewards are used as critic’s targets:   critic_target = self.discount_rewards(buffer_r, GAMMA, V_s)   N-step returns are used in the computation of the Advantage function (baselined_returns):   # Advantage function baselined_returns = n_step_targets - baseline   2 versions of N-step targets could be used:           missing terms are treated as 0.            use maximum terms possible.       Check this post for more information on N-step targets.   The following code segment accumulates gradients &amp; apply them to the local critic network:   self.AC.accumu_grad_critic(feed_dict) # accumulating gradients for local critic   self.AC.apply_accumu_grad_critic(feed_dict)   The following code segment computes the advantage function(baselined_returns):   baseline = SESS.run(self.AC.V, {self.AC.s: buffer_s}) # Value function epr = np.vstack(buffer_r).astype(np.float32) n_step_targets = self.compute_n_step_targets_missing(epr, baseline, GAMMA, N_step) # Q values # Advantage function baselined_returns = n_step_targets - baseline   The following code segment accumulates gradients for the local actor network:   self.AC.accumu_grad_actor(feed_dict) # accumulating gradients for local actor     The following code segment push the parameters from the local networks to the global networks and then pulls the updated global parameters to the local networks:   # update self.AC.push_global_actor(feed_dict)                 self.AC.push_global_critic(feed_dict)     .     .     . self.AC.pull_global()   The following code segment initialize storage for accumulated local gradients.   self.AC.init_grad_storage_actor() # initialize storage for accumulated gradients. self.AC.init_grad_storage_critic()               Check this post for more information on how to accumulate gradients in Tensorflow.   Main program:   The following code segment creates the workers:   workers = [] for i in range(num_workers): # Create worker     i_name = 'W_%i' % i # worker name     workers.append(Worker(i_name, GLOBAL_AC))   The following code segment threads the workers:   worker_threads = [] for worker in workers:     job = lambda: worker.work()     t = threading.Thread(target=job)     t.start()     worker_threads.append(t) COORD.join(worker_threads)     Tensorflow graph:                       References:   Asynchronous Methods for Deep Reinforcement Learning (Mnih, Badia, Mirza, Graves, Harley, Lillicrap, et al., 2016)        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/A3C_disc_thread_nStep/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "A3C multi-threaded continuous version with N step targets",
        "excerpt":"This post documents my implementation of the A3C (Asynchronous Advantage Actor Critic) algorithm. (multi-threaded continuous version)     An A3C (Asynchronous Advantage Actor Critic) implementation with Tensorflow. This is a multi-threaded continuous version. The code is tested with Gym’s continuous action space environment, Pendulum-v0 on Colab.     Code on my Github: (use maximum terms possible)   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     The majority of the code is very similar to the discrete version with the exceptions highlighted in the implementation details section:     Key implementation details:   Action selection:   with tf.name_scope('select_action'):     #mean = mean * action_bound[1]                        mean = mean * ( action_bound[1] - action_bound[0] ) / 2     sigma += 1e-4     normal_dist = tf.distributions.Normal(mean, sigma)                          self.choose_a = tf.clip_by_value(tf.squeeze(normal_dist.sample(1), axis=[0, 1]), action_bound[0], action_bound[1])                     Loss function of the actor network:   with tf.name_scope('actor_loss'):     log_prob = normal_dist.log_prob(self.a)     #actor_component = log_prob * tf.stop_gradient(TD_err)     actor_component = log_prob * tf.stop_gradient(self.baselined_returns)     entropy = -tf.reduce_mean(normal_dist.entropy()) # Compute the differential entropy of the multivariate normal.                        self.actor_loss = -tf.reduce_mean( ENTROPY_BETA * entropy + actor_component)   The following code segment creates a LSTM layer:   def _lstm(self, Inputs, cell_size):         # [time_step, feature] =&gt; [time_step, batch, feature]         s = tf.expand_dims(Inputs, axis=1, name='time_major')           lstm_cell = tf.nn.rnn_cell.LSTMCell(cell_size)         self.init_state = lstm_cell.zero_state(batch_size=1, dtype=tf.float32)         outputs, self.final_state = tf.nn.dynamic_rnn(cell=lstm_cell, inputs=s, initial_state=self.init_state, time_major=True)         # joined state representation                   lstm_out = tf.reshape(outputs, [-1, cell_size], name='flatten_rnn_outputs')           return lstm_out   The following function in the ACNet class creates the actor and critic’s neural networks(note that the critic’s network contains a LSTM layer):   def _create_net(self, scope):     w_init = tf.glorot_uniform_initializer()     #w_init = tf.random_normal_initializer(0., .1)     with tf.variable_scope('actor'):                                 hidden = tf.layers.dense(self.s, actor_hidden, tf.nn.relu6, kernel_initializer=w_init, name='hidden')                     #lstm_out = self._lstm(hidden, cell_size)         # tanh range = [-1,1]         mean = tf.layers.dense(hidden, num_actions, tf.nn.tanh, kernel_initializer=w_init, name='mean')         # softplus range = {0,inf}         sigma = tf.layers.dense(hidden, num_actions, tf.nn.softplus, kernel_initializer=w_init, name='sigma')     with tf.variable_scope('critic'):         hidden = tf.layers.dense(self.s, critic_hidden, tf.nn.relu6, kernel_initializer=w_init, name='hidden')         lstm_out = self._lstm(hidden, cell_size)         V = tf.layers.dense(lstm_out, 1, kernel_initializer=w_init, name='V')       actor_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')     critic_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')     return mean, sigma, V, actor_params, critic_params     Tensorflow graph:                       ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/A3C_cont_thread_nStep/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "A3C distributed tensorflow",
        "excerpt":"This post documents my implementation of the A3C (Asynchronous Advantage Actor Critic) algorithm (Distributed discrete version).     A3C (Asynchronous Advantage Actor Critic) implementation with distributed Tensorflow &amp; Python multiprocessing package. This is a discrete version with N-step targets (use maximum terms possible). The code is tested with Gym’s discrete action space environment, CartPole-v0 on Colab.     Code on my Github   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     The majority of the code is very similar to the discrete version with the exceptions highlighted in the implementation details section:     Key implementation details:   Updating the global episode counter &amp; adding the episodic return to a tf.FIFOqueue at the end of the work() function.   SESS.run(GLOBAL_EP.assign_add(1.0)) qe = GLOBAL_RUNNING_R.enqueue(ep_r) SESS.run(qe)   The distributed Tensorflow part is very similar to a simple example described in this post.   Pin the global variables under the parameter server in both the parameter_server() &amp; worker(worker_n) function:   with tf.device(\"/job:ps/task:0\"):     GLOBAL_AC = ACNet(net_scope, sess, globalAC=None) # only need its params     GLOBAL_EP = tf.Variable(0.0, name='GLOBAL_EP') # num of global episodes        # a queue of ep_r     GLOBAL_RUNNING_R = tf.FIFOQueue(max_global_episodes, tf.float32, shared_name=\"GLOBAL_RUNNING_R\")           In the parameter_server() function, check the size of the tf.FIFOqueue every 1 sec. If it’s full, dequeue the items in a list. the list will be used for display.   while True:     time.sleep(1.0)     #print(\"ps 1 GLOBAL_EP: \", sess.run(GLOBAL_EP))     #print(\"ps 1 GLOBAL_RUNNING_R.size(): \", sess.run(GLOBAL_RUNNING_R.size()))       if sess.run(GLOBAL_RUNNING_R.size()) &gt;= max_global_episodes: # GLOBAL_EP starts from 0, hence +1 to max_global_episodes                   time.sleep(5.0)         #print(\"ps 2 GLOBAL_RUNNING_R.size(): \", sess.run(GLOBAL_RUNNING_R.size()))           GLOBAL_RUNNING_R_list = []         for j in range(sess.run(GLOBAL_RUNNING_R.size())):             ep_r = sess.run(GLOBAL_RUNNING_R.dequeue())             GLOBAL_RUNNING_R_list.append(ep_r) # for display         break        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/A3C_dist_tf/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "DPPO distributed tensorflow",
        "excerpt":"This post documents my implementation of the Distributed Proximal Policy Optimization (Distributed PPO or DPPO) algorithm. (Distributed continuous version)     Distributed Proximal Policy Optimization (Distributed PPO or DPPO) continuous version implementation with distributed Tensorflow and Python’s multiprocessing package. This implementation uses normalized running rewards with GAE. The code is tested with Gym’s continuous action space environment, Pendulum-v0 on Colab.     Code on my Github:   If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     Notations:   current policy =    old policy =    epsilon =    Advantage function = A     Equations:   Truncated version of generalized advantage estimation (GAE) =    =    where  =    when  = 1,    =    Probability ratio =    =    Clipped Surrogate Objective function =    =      Key implementation details:   The following class is adapted from OpenAI’s baseline: This class is used for the normalization of rewards in this program before GAE computation.   class RunningStats(object):     def __init__(self, epsilon=1e-4, shape=()):         self.mean = np.zeros(shape, 'float64')         self.var = np.ones(shape, 'float64')         self.std = np.ones(shape, 'float64')         self.count = epsilon      def update(self, x):         batch_mean = np.mean(x, axis=0)         batch_var = np.var(x, axis=0)         batch_count = x.shape[0]         self.update_from_moments(batch_mean, batch_var, batch_count)      def update_from_moments(self, batch_mean, batch_var, batch_count):         delta = batch_mean - self.mean         new_mean = self.mean + delta * batch_count / (self.count + batch_count)         m_a = self.var * self.count         m_b = batch_var * batch_count         M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)         new_var = M2 / (self.count + batch_count)          self.mean = new_mean         self.var = new_var         self.std = np.maximum(np.sqrt(self.var), 1e-6)         self.count = batch_count + self.count   This function in the PPO class is adapted from OpenAI’s Baseline, returns TD lamda return &amp; advantage       def add_vtarg_and_adv(self, R, done, V, v_s_, gamma, lam):         # Compute target value using TD(lambda) estimator, and advantage with GAE(lambda)         # last element is only used for last vtarg, but we already zeroed it if last new = 1         done = np.append(done, 0)         V_plus = np.append(V, v_s_)         T = len(R)         adv = gaelam = np.empty(T, 'float32')         lastgaelam = 0         for t in reversed(range(T)):             nonterminal = 1-done[t+1]                     delta = R[t] + gamma * V_plus[t+1] * nonterminal - V_plus[t]             gaelam[t] = lastgaelam = delta + gamma * lam * nonterminal * lastgaelam            #print(\"adv=\", adv.shape)         #print(\"V=\", V.shape)         #print(\"V_plus=\", V_plus.shape)         tdlamret = np.vstack(adv) + V         #print(\"tdlamret=\", tdlamret.shape)         return tdlamret, adv # tdlamret is critic_target or Qs         The following code segment from the PPO class defines the Clipped Surrogate Objective function:   with tf.variable_scope('surrogate'):                     ratio = self.pi.prob(self.act) / self.oldpi.prob(self.act)                     surr = ratio * self.adv                     self.aloss = -tf.reduce_mean(tf.minimum(surr, tf.clip_by_value(ratio, 1.-epsilon, 1.+epsilon)*self.adv))   The following code segment from the work() function in the worker class normalized the running rewards for each worker:   self.running_stats_r.update(np.array(buffer_r))                     buffer_r = np.clip( (np.array(buffer_r) - self.running_stats_r.mean) / self.running_stats_r.std, -stats_CLIP, stats_CLIP )   The following code segment from the work() function in the worker class computes  the TD lamda return &amp; advantage:   tdlamret, adv = self.ppo.add_vtarg_and_adv(np.vstack(buffer_r), np.vstack(buffer_done), np.vstack(buffer_V), v_s_, GAMMA, lamda)    The following update function in the PPO class does the training &amp; the updating of global &amp; local parameters (Note the at the beginning of training,   probability ratio = 1):   def update(self, s, a, r, adv):         self.sess.run(self.update_oldpi_op)      for _ in range(A_EPOCH): # train actor         self.sess.run(self.atrain_op, {self.state: s, self.act: a, self.adv: adv})         # update actor         self.sess.run([self.push_actor_pi_params,                        self.pull_actor_pi_params],                       {self.state: s, self.act: a, self.adv: adv})     for _ in range(C_EPOCH): # train critic         # update critic         self.sess.run(self.ctrain_op, {self.state: s, self.discounted_r: r})         self.sess.run([self.push_critic_params,                        self.pull_critic_params],                       {self.state: s, self.discounted_r: r})        The distributed Tensorflow &amp; multiprocessing code sections are very similar to the ones describe in the following posts:   A3C distributed tensorflow   Distributed Tensorflow     References:   Proximal Policy Optimization Algorithms (Schulman, Wolski, Dhariwal, Radford, Klimov, 2017)   Emergence of Locomotion Behaviours in Rich Environments (Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, et al., 2017)        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/DPPO_dist_tf/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "RND (Random Network Distillation) with Proximal Policy Optimization (PPO) Tensorflow",
        "excerpt":"This post documents my implementation of the Random Network Distillation (RND) with Proximal Policy Optimization (PPO) algorithm. (continuous version)     Random Network Distillation (RND) with Proximal Policy Optimization (PPO) implentation in Tensorflow. This is a continuous version which solves the mountain car continuous problem (MountainCarContinuous-v0). The RND helps learning with curiosity driven exploration.   The agent starts to converge correctly at around 30 episodes &amp; reached the flag 291 times out of 300 episodes (97% hit rate). It takes 385.09387278556824 seconds to complete 300 episodes on Google’s Colab.   Edit: A new version which corrects a numerical error(causes nan action) takes 780.2065596580505 seconds for 300 episodes. Both versions have similar results. The URL for the new version is updated. Added random seeds for numpy &amp; Tensorflow global seed &amp; ops seed achieve better consistency &amp; faster convergence.   Checkout the resulting charts from the program output.   Code on my Github:           Python file,            Jupyter notebook (The Jupyter notebook, which also contain the resulting charts at the end, can be run directly on Google’s Colab.)       If Github is not loading the Jupyter notebook, a known Github issue, click here to view the notebook on Jupyter’s nbviewer.     Notations &amp; equations   fixed feature from target network =    predicted feature from predictor network =    intrinsic reward =  = ||  -  ||    For notations &amp; equations regarding PPO, refer to this post.     Key implementation details:   Preprocessing, state featurization:   Prior to training, the states are featurized with the RBF kernel.   (states are also featurized during every training batch.)   Refer to scikit-learn.org documentation: 5.7.2. Radial Basis Function Kernel for more information on RBF kernel.   if state_ftr == True: \"\"\" The following code for state featurization is adapted &amp; modified from dennybritz's repository located at: https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb \"\"\"     # Feature Preprocessing: Normalize to zero mean and unit variance     # We use a few samples from the observation space to do this     states = np.array([env.observation_space.sample() for x in range(sample_size)]) # pre-trained, states preprocessing     scaler = sklearn.preprocessing.StandardScaler()     scaler.fit(states) # Compute the mean and std to be used for later scaling.      # convert states to a featurizes representation.     # We use RBF kernels with different variances to cover different parts of the space     featurizer = sklearn.pipeline.FeatureUnion([ # Concatenates results of multiple transformer objects.             (\"rbf1\", RBFSampler(gamma=5.0, n_components=n_comp)),             (\"rbf2\", RBFSampler(gamma=2.0, n_components=n_comp)),             (\"rbf3\", RBFSampler(gamma=1.0, n_components=n_comp)),             (\"rbf4\", RBFSampler(gamma=0.5, n_components=n_comp))             ])     featurizer.fit(         scaler.transform(states)) # Perform standardization by centering and scaling  # state featurization of state(s) only, # not used on s_ for RND's target &amp; predictor networks def featurize_state(state):     scaled = scaler.transform([state]) # Perform standardization by centering and scaling     featurized = featurizer.transform(scaled) # Transform X separately by each transformer, concatenate results.     return featurized[0]  def featurize_batch_state(batch_states):     fs_list = []     for s in batch_states:         fs = featurize_state(s)         fs_list.append(fs)     return fs_list   Preprocessing, next state normalization for RND:   Variance is computed for the next states buffer_s_ using the RunningStats class. During every training batch, the next states are normalize and clipped.   def state_next_normalize(sample_size, running_stats_s_):    buffer_s_ = []   s = env.reset()   for i in range(sample_size):     a = env.action_space.sample()     s_, r, done, _ = env.step(a)     buffer_s_.append(s_)    running_stats_s_.update(np.array(buffer_s_))   if state_next_normal == True:   state_next_normalize(sample_size, running_stats_s_)     Tensorboard graphs:   Big picture:   There are two main modules, the PPO and the RND.   Current state, state is passed into PPO.   Next state, state_ is passed into RND.        PPO module:   PPO module contains the actor network &amp; the critic network.        PPO’s actor:   At every iteration, an action is sampled from policy network pi.      PPO’s critic:   The critic contains two value function networks. One for extrinsic rewards &amp; one  for intrinsic rewards. Two sets of TD lambda returns &amp; advantages are also  computed.   For extrinsic rewards: tdlamret adv   For intrinsic rewards: tdlamret_i adv_i   The TD lambda returns are used as the PPO’s critics targets in their respective networks while the advantages are summed &amp; used as the advantage in the actor’s loss computation.        RND module:   RND module contains the target network &amp; the predictor network.        RND target network:   The target network is a fixed network, meaning that it’s never trained. It’s weights are randomized once during initialization. The target network is used to encode next states state_. It’s output are encoded next states.        RND predictor network:   The predictor_loss is the intrinsic reward. It is the difference between the predictor network’s output with the target network’s output. The predictor network is trying to guess the target network’s encoded output.        Key to note:   All networks used in this program are linear.   The actor module is basically similar to this DPPO code documented in this post.   The difference is in the critic module. This implementation has two value functions in the critic module rather than one.   The predictor_loss is the intrinsic reward.        Problems encountered:   The actor’s network occasionally returns ‘'’nan’’’ for action. This happens randomly, most likely caused by exploding gradients. Not initializing or randomly initializing actor’s weights results in nan when outputting action.        Program output:   hit_counter 291 0.97   Number of steps per episode:      Reward per episode:      Moving average reward per episode:      — 385.09387278556824 seconds —     References:   Exploration by Random Network Distillation (Yuri Burda, Harrison Edwards, Amos Storkey, Oleg Klimov, 2018)        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/RND/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": ".bash_profile for Mac",
        "excerpt":"This post demonstrates how to create customized functions to bundle commands in a .bash_profile file on Mac.     Edit .bash_profile for Mac.      Start Terminal   Enter “cd ~/” to go to home folder   Edit .bash_profile with “open -e .bash_profile” to open in TextEdit.   Enter “. .bash_profile” to reload .bash_profile.     Examples   To bundle common git operations, add the following to .bash_profile file:   function lazy_git() {     git checkout test_ver     git add .     git commit -a -m \"$1\"     git checkout master     git merge test_ver     git push     git checkout test_ver }    To bundle common jekyll operations, add the following to .bash_profile file:   The command serve runs localhost.   function lazy_jekyll_serve() {     cd /Users/tester/gitHubRepo/ChuaCheowHuan.github.io     pwd     bundle exec jekyll serve }   The command build build the site. This command is neccessary for generating sitemap.xml &amp; robot.txt.   function lazy_jekyll_build() {     cd /Users/tester/gitHubRepo/ChuaCheowHuan.github.io     pwd     bundle exec jekyll build }        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/bash_script/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Tensorflow graphs in Tensorboard",
        "excerpt":"This post demonstrate how setup &amp; access Tensorflow graphs.     In order to access Tensorflow graphs, you need to use Tensorboard which comes will Tensorflow installed.   The following snippet shows how to setup a FileWriter with a Tensorflow graph.   tf.reset_default_graph()  # Your Tensorflow graph goes here. # ...  sess = tf.Session() sess.run(tf.global_variables_initializer())  # Declare tf.summary.FileWriter where log is your output directory for # Tensorboard &amp; add the graph to the writer. writer = tf.summary.FileWriter('log', sess.graph)  # Run your training loop # sess.run(...)  writer.close()   Run this command in terminal to start tensorboard:  $ tensorboard --logdir log  $ tensorboard --logdir ~/ray_results  Where log is the log folder.   Navigate to http://127.0.0.1:6006 in your browser to access Tensorflow. Your graph is in the graph tab.        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/tf_graph/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Custom MARL (multi-agent reinforcement learning) CDA (continuous double auction) environment",
        "excerpt":"A custom MARL (multi-agent reinforcement learning) environment where multiple agents trade against one another in a CDA (continuous double auction).     This is WIP.     Contents:  1) Update   2) Purpose of this repository   3) Example   4) Dependencies   5) Installation   6) TODO   7) Acknowledgements   8) Contributing   9) Disclaimer   10) Generated LOB   11) Observation space   12) Action space   13) Making sense of the render output     Update:  20200327:   1) Include training script with n agents &amp; k trained agents.   2) New reward function which requires the agents to maximize profit while minimizing number of trades made in an episode (trading session).   3) Improve storage for callback functions &amp; plot functionality to allow plotting of more than 100 episodes.   20200322   20200304   20191030     Purpose of this repository:  The purpose of this repository is to create a custom MARL (multi-agent reinforcement learning) environment where multiple agents trade against one another in a CDA (continuous double auction).   The environment doesn’t use any external data. Data is generated by self-play of the agents themselves through their interaction with the limit order book.   At each time step, the environment emits the top k rows of the aggregated order book as observations to the agents. Each agent then samples an action from the action space &amp; all actions are randomly shuffled before execution in each time step.   Each time step is a snapshot of the limit order book &amp; a key assumption is that all traders(agents) suffer the same lag (wait for all traders to have their orders executed before seeing the next LOB snapshot).   Note:   Each agent is a trader, both terms will be used interchangeably in this environment.     Example:  The example is available in this Jupyter notebook implemented with RLlib: CDA_env_RLlib.ipynb. This notebook is tested in Colab.   This example uses two trained agents &amp; N random agents. All agents compete with one another in this zero-sum environment, irregardless of whether they’re trained or random.   competitive self-play   The policy weights of the winning trained agent(trader) is used to replace the policy weights of the other trained agents after each training iteration. Winning here is defined as having the highest reward per training iteration.   The reward function requires the agents to maximize profit while minimizing number of trades made in an episode (trading session). As the number of trades accumulates in the later stages of a session, profits will be scaled down by the number of trades &amp; losses will be magnified.   The trained agents are P0 &amp; P1, both using separate PPO policy weights. The rest are random agents.   The results with 10 agents are shown in the figures below:           If you’re running locally, you can run the following command &amp; navigate to localhost:6006 in your browser to access the tensorboard graphs:  $ tensorboard --logdir ~/ray_results     Other ways to run this environment:   By using the python CDA_env_rand.py script which is basically running a CDA simulator with dummy (non-learning) random agents.     Dependencies:   1) tensorFlow 2) ray[rllib] 3) pandas 4) sortedcontainers 5) sklearn   For a full list of dependencies &amp; versions, see requirements.txt in this repository.     Installation:  The environment is installable via pip.  $ cd gym-continuousDoubleAuction  $ pip install -e .     TODO:  1) Custom RLlib workflow to include custom RND + PPO policies. (for training script).   2) Parametric or hybrid action space (or experiment with different types of   action space).   3) More robust tests (add LOB test into test script).   4) Better documentation.   5) Display data for all steps (for visualization after simulation).   6) Move action consequences after each step by each agent into the respective info dictionary.   7) Instead of traders(agents) having the same lag, introduce zero lag (Each LOB snapshot in each t-step is visible to all traders) or random lag.   8) Allow traders to have different starting capital.   9) Expose the limit orders (that are currently in the LOB or aggregated LOB) which belongs to a particular trader as observation to that trader.   10) Allows a distribution of previous winning policies to be selected for trained agents.   11) Move TODO to issues.     Acknowledgements:  The orderbook matching engine is adapted from https://github.com/dyn4mik3/OrderBook     Contributing:  Please see CONTRIBUTING.md.     Disclaimer:  This repository is only meant for research purposes &amp; is never meant to be used in any form of trading. Past performance is no guarantee of future results. If you suffer losses from using this repository, you are the sole person responsible for the losses. The author will NOT be held responsible in any way.     Generated LOB:             Making sense of the render output:   The step separator:  ************************************************** t_step = 306 **************************************************    Actions:   Actions output from the model:   1) Each column represents the action from each trader(agent). 2) Row 1 represents the side: none, bid, ask (0 to 2). 3) Row 2 represents the type: market, limit, modify, cancel. 4) Row 3 represents the mean for size selection. 5) Row 4 represents the sigma for size selection. 6) Row 5 represents the price: based on LOB market depth from 0 to 11.  Model actions:  --  --  --  --  1   1   1   2  1   0   0   1 39  29   6  17 19  89  13   0  7   4   9  10 --  --  --  --   1) Column 1 represents the ID of each trader(agent). 2) Column 2 the side: none, bid, ask (0 to 2). 3) Column 3 type: market, limit, modify, cancel. 4) Column 4 represents the order size. 5) Column 5 represents the order price.  Formatted actions acceptable by LOB:  -  ---  ------  -----  -- 0  bid  limit   38982  15 1  bid  market   5779   0 2  bid  market    999   0 3  ask  limit   17001  47 -  ---  ------  -----  -- Shuffled action queue sequence for LOB executions:  -  ---  ------  -----  -- 3  ask  limit   17001  47 2  bid  market    999   0 1  bid  market   5779   0 0  bid  limit   38982  15 -  ---  ------  -----  --     Rewards, dones, &amp; infos:  rewards:  {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0}  dones:  {'__all__': True}  infos:  {0: {}, 1: {}, 2: {}, 3: {}}     Aggregated LOB:   1) The columns represents the 10 levels (1 to 10, left to right) of the market depth in the LOB. 2) Row 1 represents the bid size. 3) Row 2 represents the bid price. 4) Row 3 represents the ask size. 5) Row 4 represents the ask price.  agg LOB @ t-1  ------  -----  ------  ------  ------  ------  ------  ------  ------  ------   7746  19011  126634  116130   43073  124055   74977  188096  139117  143968     23     22      21      20      19      15      14      12      11      10 -62448  -7224  -65989  -96940  -77985  -93987  -55942   -4173  -16998  -81011    -36    -37     -38     -39     -40     -41     -42     -43     -47     -48 ------  -----  ------  ------  ------  ------  ------  ------  ------  ------   agg LOB @ t  ------  -----  ------  ------  ------  ------  ------  ------  ------  ------   7746  19011  126634  116130   43073  163037   74977  188096  139117  143968     23     22      21      20      19      15      14      12      11      10 -56669  -7224  -65989  -96940  -77985  -93987  -55942   -4173  -33999  -81011    -36    -37     -38     -39     -40     -41     -42     -43     -47     -48 ------  -----  ------  ------  ------  ------  ------  ------  ------  ------     LOB bids:   The current limit bid orders in the LOB.  LOB:  ***Bids***      size price  trade_id  timestamp  order_id 0    7746    23         0        345       265 1   19011    22         1        344       231 2   14553    21         2        107        99 3   63025    21         1        333       209 4   49056    21         3        349       268 5   89029    20         2         53        53 6   24060    20         0        201        46 7    3041    20         1        297       229 8   43073    19         1         35        35 9   42989    15         1        340       234 10  81066    15         3        336       259 11  38982    15         0        359       275 12  63003    14         0        253       201 13  11974    14         1        285       168 14  18089    12         3        351       105 15  91998    12         0        343       264 16  78009    12         1        352        40 17  45039    11         3        123       101 18  94078    11         0        204       172 19  97967    10         3        223       185 20  46001    10         1        313       243 21  45871     9         2         52        52 22  94993     9         3        209       176     LOB asks:   The current limit ask orders in the LOB.  ***Asks***      size price  trade_id  timestamp  order_id 0   40654    36         3        322       250 1   16015    36         0        323       251 2    7224    37         1        272       214 3   39980    38         3        299       190 4   26009    38         1        261       206 5   58977    39         0        231       188 6   37963    39         3        284       164 7   15995    40         0        305       235 8   61990    40         3        328       254 9   93987    41         0        353       143 10  55942    42         1        290       189 11   4173    43         0        112       104 12  16998    47         1        341       239     Tape (Time &amp; sales):  ***tape***     size price  timestamp  counter_party_ID  init_party_ID init_party_side 0   5779    36        358                 3              1             bid 1   5894    36        356                 3              0             bid 2  13347    36        355                 3              1             bid 3   2272    36        354                 3              0             bid 4    894    23        350                 0              1             ask 5  12874    23        347                 0              0             ask 6   7501    23        346                 0              1             ask 7   9405    22        342                 1              3             ask     Trades:   Trades that took place when executing the action of a trader(agent) at t-step.   act_seq_num represents the sequence of the action. In this case, it’s the 2nd action executed at t-step.  TRADES (act_seq_num): 2    seq_Trade_ID  timestamp price    size  time  counter_ID counter_side  counter_order_ID counter_new_book_size  init_ID init_side init_order_ID init_new_LOB_size 0             0        358    36  5779.0   358           3          ask               250                 40654        1       bid          None              None     New order in LOB:   The new limit orders inserted into LOB (includes unfilled leftover quantity from previous order).  order_in_book (act_seq_num): 0 type    side      quantity    price    trade_id    timestamp    order_id ------  ------  ----------  -------  ----------  -----------  ---------- limit   ask          17001       47           3          357         273 order_in_book (act_seq_num): 3 type    side      quantity    price    trade_id    timestamp    order_id ------  ------  ----------  -------  ----------  -----------  ---------- limit   bid          38982       15           0          359         275     Mark to market profit @ t-step:  mark_to_mkt profit@t: ID: 0; profit: 1491150.999999999999999999998 ID: 1; profit: 3583508.999999999999999999995 ID: 2; profit: -7421583.999999999999999999999 ID: 3; profit: -676658.0000000000000000000013     Accounts info:  Accounts:    ID          cash    cash_on_hold    position_val      prev_nav           nav    net_position     VWAP             profit    total_profit    num_trades ----  ------------  --------------  --------------  ------------  ------------  --------------  -------  -----------------  --------------  ------------    0  -4.51044e+07     3.11089e+07     1.64866e+07   2.49115e+06   2.49115e+06         -375119  39.9751        1.49115e+06     1.49115e+06            74    1  -3.8919e+07      3.27787e+07     1.07237e+07   4.58351e+06   4.58351e+06          -98798  72.2711        3.58351e+06     3.58351e+06            78    2  -1.92421e+07     3.55094e+06     9.2696e+06   -6.42158e+06  -6.42158e+06          257489  64.8229       -7.42158e+06    -7.42158e+06            23    3  -4.46985e+07     4.0254e+07      7.79141e+06   3.34692e+06   3.34692e+06          216428  39.1265  -676658               2.34692e+06            79     1) total_sys_profit (total profit of all agents at each step) should be equal to 0 (zero-sum game).   2) total_sys_nav (total net asset value of all agents at each step) is the total sum of beginning NAV of all traders(agents).   Note: Small random rounding errors are present.  total_sys_profit = -9E-21; total_sys_nav = 3999999.999999999999999999991     Sample output results for final training iteration:   1) The episode_reward is zero (zero sum game) for each episode.  episode_reward_max: 0.0 episode_reward_mean: 0.0 episode_reward_min: 0.0   2) The mean reward of each policy is shown under policy_reward_mean.  . . . Result for PPO_continuousDoubleAuction-v0_0:   custom_metrics: {}   date: 2019-09-30_21-16-20   done: true   episode_len_mean: 1001.0   episode_reward_max: 0.0   episode_reward_mean: 0.0   episode_reward_min: 0.0   episodes_this_iter: 4   episodes_total: 38   experiment_id: 56cbdad4389343eca5cfd49eadeb3554   hostname: Duality0.local   info:     grad_time_ms: 15007.219     learner:       policy_0:         cur_kl_coeff: 0.0003906250058207661         cur_lr: 4.999999873689376e-05         entropy: 10.819798469543457         entropy_coeff: 0.0         kl: 8.689265087014064e-06         model: {}         policy_loss: 153.9163055419922         total_loss: 843138688.0         vf_explained_var: 0.0         vf_loss: 843138496.0     num_steps_sampled: 40000     num_steps_trained: 40000     opt_peak_throughput: 266.538     opt_samples: 4000.0     sample_peak_throughput: 80.462     sample_time_ms: 49713.208     update_time_ms: 176.14   iterations_since_restore: 10   node_ip: 192.168.1.12   num_healthy_workers: 2   off_policy_estimator: {}   pid: 10220   policy_reward_mean:     policy_0: 12414.421052631578     policy_1: -301.39473684210526     policy_2: -952.1578947368421     policy_3: -11160.868421052632   sampler_perf:     mean_env_wait_ms: 18.1753569144153     mean_inference_ms: 4.126144958830859     mean_processing_ms: 1.5262831265657335   time_since_restore: 649.1416146755219   time_this_iter_s: 61.54709506034851   time_total_s: 649.1416146755219   timestamp: 1569849380   timesteps_since_restore: 40000   timesteps_this_iter: 4000   timesteps_total: 40000   training_iteration: 10   trial_id: ea67f638  2019-09-30 21:16:20,507\tWARNING util.py:145 -- The `process_trial` operation took 0.4397752285003662 seconds to complete, which may be a performance bottleneck. 2019-09-30 21:16:21,407\tWARNING util.py:145 -- The `experiment_checkpoint` operation took 0.899777889251709 seconds to complete, which may be a performance bottleneck. == Status == Using FIFO scheduling algorithm. Resources requested: 0/4 CPUs, 0/0 GPUs Memory usage on this node: 3.3/4.3 GB Result logdir: /Users/hadron0/ray_results/PPO Number of trials: 1 ({'TERMINATED': 1}) TERMINATED trials:  - PPO_continuousDoubleAuction-v0_0:\tTERMINATED, [3 CPUs, 0 GPUs], [pid=10220], 649 s, 10 iter, 40000 ts, 0 rew  == Status == Using FIFO scheduling algorithm. Resources requested: 0/4 CPUs, 0/0 GPUs Memory usage on this node: 3.3/4.3 GB Result logdir: /Users/hadron0/ray_results/PPO Number of trials: 1 ({'TERMINATED': 1}) TERMINATED trials:  - PPO_continuousDoubleAuction-v0_0:\tTERMINATED, [3 CPUs, 0 GPUs], [pid=10220], 649 s, 10 iter, 40000 ts, 0 rew        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/MARL_CDA_env/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Random policy in RLlib",
        "excerpt":"Creating &amp; seeding a random policy class in RLlib.     EDIT: Code URL updated below.   Code on my Github     Function:   def make_RandomPolicy(_seed):      # a hand-coded policy that acts at random in the env (doesn't learn)     class RandomPolicy(Policy):         \"\"\"Hand-coded policy that returns random actions.\"\"\"         def __init__(self, observation_space, action_space, config):             self.observation_space = observation_space             self.action_space = action_space             self.action_space.seed(_seed)          def compute_actions(self,                             obs_batch,                             state_batches,                             prev_action_batch=None,                             prev_reward_batch=None,                             info_batch=None,                             episodes=None,                             **kwargs):             \"\"\"Compute actions on a batch of observations.\"\"\"             return [self.action_space.sample() for _ in obs_batch], [], {}          def learn_on_batch(self, samples):             \"\"\"No learning.\"\"\"             #return {}             pass          def get_weights(self):             pass          def set_weights(self, weights):             pass      return RandomPolicy     Usage example:   # Setup PPO with an ensemble of `num_policies` different policies     policies = {\"policy_{}\".format(i): gen_policy(i) for i in range(args.num_policies)} # contains many \"policy_graphs\" in a policies dictionary      # override policy with random policy     policies[\"policy_{}\".format(args.num_policies-3)] = (make_RandomPolicy(1), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary     policies[\"policy_{}\".format(args.num_policies-2)] = (make_RandomPolicy(2), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary     policies[\"policy_{}\".format(args.num_policies-1)] = (make_RandomPolicy(3), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary  ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/RLlib_rand_policy/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Dockerized Postgres connection with Django web app in Travis CI",
        "excerpt":"Introducing a delay to allow proper connection between dockerized Postgres &amp; Django web app in Travis CI.     EDIT: Code URL updated.   Code on my Github     If you see the following error in the Travis’s job log while attempting to test dockerized Django apps with Travis, it means that the postgres docker container has started but not yet ready to accept connections.   psycopg2.OperationalError: could not connect to server: Connection refused 539\tIs the server running on host \"db\" (172.18.0.2) and accepting 540\tTCP/IP connections on port 5432?  . . .  django.db.utils.OperationalError: could not connect to server: Connection refused 587\tIs the server running on host \"db\" (172.18.0.2) and accepting 588\tTCP/IP connections on port 5432?  The command \"docker-compose run web python manage.py test\" exited with 1.   A solution for this issue is to introduce a delay until connection is ready before executing the test.   The delay has to be implemented in the docker-compose.yml file before migration &amp; running of Django’s server shown below:   command: bash -c 'while !&lt;/dev/tcp/db/5432; do sleep 1; done; python3 manage.py migrate'   command: bash -c 'while !&lt;/dev/tcp/db/5432; do sleep 1; done; python3 manage.py runserver 0.0.0.0:8000'     Config files:   These are the relevant config files used in a Django project with the delay introduced in the docker-compose.yml file. The actual command to run the test is in the .travis.yml file.   The database configuration in settings.py  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.postgresql',         'NAME': 'postgres',         'USER': 'postgres',         'HOST': 'db',         'PORT': 5432,         #'PORT': 5433,     } }   The Dockerfile:  FROM python:3 WORKDIR /usr/src/app ADD requirements.txt /usr/src/app RUN pip install -r requirements.txt ADD . /usr/src/app   The docker-compose.yml file:  version: '3'  services:     db:         image: postgres     migration:         build: . #        command: python3 manage.py migrate         command: bash -c 'while !&lt;/dev/tcp/db/5432; do sleep 1; done; python3 manage.py migrate'         volumes:             - .:/usr/src/app         depends_on:             - db     web:         build: . #        command: python3 manage.py runserver 0.0.0.0:8000         command: bash -c 'while !&lt;/dev/tcp/db/5432; do sleep 1; done; python3 manage.py runserver 0.0.0.0:8000'         volumes:             - .:/usr/src/app         ports:             - \"8000:8000\"         depends_on:             - db             - migration   The .travis.yml file:  language: python python:     - 3.6 services:     - docker #    - postgres install:     - pip install -r requirements.txt #before_script: #    - psql -c 'create database testdb;' -U postgres #    - psql -c 'create database travisci;' -U postgres script: #    - docker-compose build #    - docker-compose run web python manage.py migrate     - docker-compose run web python manage.py test #    - python manage.py test     After introducing the delay, this is the successful test output in Travis’s job log.   . . . ....... 528---------------------------------------------------------------------- 529Ran 10 tests in 0.126s 530 531OK 532Destroying test database for alias 'default'... 533The command \"docker-compose run web python manage.py test\" exited with 0.     References:   See this post in stackoverflow.        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/docker_travis/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Django + Postgres + Docker + Travis CI + Heroku CD",
        "excerpt":"Basic workflow of testing a dockerized Django &amp; Postgres web app with Travis (continuous integration) &amp; deployment to Heroku (continuous deployment).     Prerequisite:   1) This post assumes that the reader has accounts with Github, Travis &amp; Heroku &amp; already has the accounts configured. For example, linking Travis with Github, adding a Postgres server in Heroku &amp; setting OS environment variables in Travis &amp; Heroku websites.   2) Basic working knowledge of Django &amp; Docker.     Code on my Github     Which copy of Postgres to use during the different stages in the workflow?   During development, we won’t be using a local copy of Postgres database server. The Docker’s copy of Postgres is used.   When testing in Travis, we don’t have to ask Travis for a copy of Postgres, we won’t be using Travis’s copy, we’ll be using the Docker’s copy.   During deployment, we have to use Heroku’s copy.     makemigrations   Always run docker-compose web run python manage.py makemigrations before deployment to Heroku or in our case, before pushing to Github.   The actual python manage.py migrate for the Postgres server addon from Heroku will be run in the Procfile file.     Listing files &amp; directories in a tree:   cd web_app_DPDTH  $ tree -a -I \"CS50_web_dev|staticfiles|static|templates|LICENSE|README.md|__init__.py|settings_DPTH_.py|urls.py|wsgi.py|db.sqlite3|airline4_tests_.py|apps.py|migrations|views.py|models.py|flights.csv|manage.py|wait-for-it.sh|admin.py|.git|.travis_DPTH_.yml|__pycache__\"  . ├── .travis.yml ├── .travis_DPDTH_.yml ├── .travis_old.yml ├── Dockerfile ├── Procfile ├── airline │   └── settings.py ├── docker-compose.yml ├── docker_push.sh ├── flights │   └── tests.py ├── heroku-container-release.sh └── requirements.txt    As shown in the tree above, the 9 files that matter in the workflow:   1) tests.py   2) settings.py   3) requirements.txt   4) Dockerfile   5) docker-compose.yml   6) .travis.yml   7) docker_push.sh   8) heroku-container-release.sh   9) Procfile   We will look at the contents of each of the 9 files in the sections below.     tests.py   This is the test file that Travis will use for testing the app. You write whatever test you want for Travis to run with.   from django.db.models import Max from django.test import Client, TestCase  from .models import Airport, Flight, Passenger  # Create your tests here. class FlightsTestCase(TestCase):      def setUp(self):          # Create airports.         a1 = Airport.objects.create(code=\"AAA\", city=\"City A\")         a2 = Airport.objects.create(code=\"BBB\", city=\"City B\")          # Create flights.         Flight.objects.create(origin=a1, destination=a2, duration=100)         Flight.objects.create(origin=a1, destination=a1, duration=200)         Flight.objects.create(origin=a2, destination=a1, duration=300)      # 1     def test_departures_count(self):         a = Airport.objects.get(code=\"AAA\")         self.assertEqual(a.departures.count(), 2)      # 2     def test_arrivals_count(self):         a = Airport.objects.get(code=\"AAA\")         self.assertEqual(a.arrivals.count(), 2)      # 3     def test_valid_flight(self):         a1 = Airport.objects.get(code=\"AAA\")         a2 = Airport.objects.get(code=\"BBB\")         f = Flight.objects.get(origin=a1, destination=a2)         self.assertTrue(f.is_valid_flight())     settings.py   Under the database section, the DATABASES['default'] sets the default database so the default database is the one connected by the OS environment variable DATABASE_URL, however if this is unavailable, we’ll use the one defined with 'HOST': 'db'.   This setup allows us to use 'HOST': 'db' which is the Docker’s copy of postgres during development phase &amp; also during tesing phase with Travis while using the Heroku’s copy during deployment which is provided by connecting to the DATABASE_URL.   The DATABASE_URL, as an OS environment variable which is generated by Heroku after a Database is added to the web app in the Heroku website.   Add and/or edit the following to the settings.py file:   import django_heroku import dj_database_url   MIDDLEWARE = [     'django.middleware.security.SecurityMiddleware',      'whitenoise.middleware.WhiteNoiseMiddleware',  # new      'django.contrib.sessions.middleware.SessionMiddleware',     'django.middleware.common.CommonMiddleware',     'django.middleware.csrf.CsrfViewMiddleware',     'django.contrib.auth.middleware.AuthenticationMiddleware',     'django.contrib.messages.middleware.MessageMiddleware',     'django.middleware.clickjacking.XFrameOptionsMiddleware', ]   DATABASES = {     'default': {         'ENGINE': 'django.db.backends.postgresql',         'NAME': 'postgres',         'USER': 'postgres',         'PASSWORD': 'postgres',         'HOST': 'db', # Docker's copy of postgres         'PORT': 5432,         #'PORT': 5433,     } }  DATABASE_URL = os.environ.get('DATABASE_URL') db_from_env = dj_database_url.config(default=DATABASE_URL, conn_max_age=500, ssl_require=True) DATABASES['default'].update(db_from_env)   STATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')   django_heroku.settings(locals())      requirements.txt   This file lets Docker &amp; Travis know what packages are needed for the app. This is needed in Dockerfile &amp; .travis.yml files.   django&gt;=2.0.11 psycopg2 psycopg2-binary dj-database-url==0.5.0 gunicorn whitenoise django-heroku pytz sqlparse     Dockerfile   This contains the instructions for building a Docker image.   The CMD gunicorn airline.wsgi:application --bind 0.0.0.0:$PORT tells Docker to use gunicorn as the web server. See here for details.   FROM python:3  WORKDIR /usr/src/app  ADD requirements.txt /usr/src/app  RUN pip install -r requirements.txt  ADD . /usr/src/app  # collect static files RUN python manage.py collectstatic --noinput  CMD gunicorn airline.wsgi:application --bind 0.0.0.0:$PORT     docker-compose.yml   This contains the instructions on how to run a Docker containers which is an instance of a Docker image.   Notice the sleep delay introduced in the 2 command: sections. See here for details.   version: '3'  services:     db:         image: postgres     migration:         build: .         command: bash -c 'while !&lt;/dev/tcp/db/5432; do sleep 1; done; python3 manage.py migrate'         volumes:             - .:/usr/src/app         depends_on:             - db     web:         build: . #        container_name: webapp-dpdth         image: webapp-dpdth         command: bash -c 'while !&lt;/dev/tcp/db/5432; do sleep 1; done; python3 manage.py runserver 0.0.0.0:8000'         volumes:             - .:/usr/src/app         ports:             - \"8000:8000\"         depends_on:             - db             - migration      .travis.yml   This file contains instructions for Travis. Notice that we’re not using the Postgres from Travis because we’re using Postgres from Docker directly. The postgresql is therefore commented out under the services: section.   Under script:, we ask Travis to run the test using Docker with the docker-compose command.   Under deploy:, we execute a docker_push.sh script, more details in the sections below.   The skip_cleanup: true tells Travis not to remove any files that it deems unnecessary after deployment. Travis does not have permission to do that on Heroku anyway.   language: python python:     - 3.6 services:     - docker #    - postgresql install:     - pip install -r requirements.txt script:     - docker-compose run web python manage.py test deploy:     provider: script     script: bash docker_push.sh     skip_cleanup: true     on:         branch: master     Workflow for after testing with Travis to deployment to Heroku   The main workflow after testing to deployment is as such:   tag image -&gt; push image to registry -&gt; release image   We’ll see how to do that in the following script files:   1) docker_push.sh   2) heroku-container-release.sh     docker_push.sh   This file does several things listed as follows:   1) Login to the Heroku’s image registry.   2) tag the source image webapp-dpdth:latest to the target image registry.heroku.com/webapp-dpdth/web. Replace webapp-dpdth with your app name on Heroku.   3) Push the target image to Heroku’s registry if the branch tested on Travis is a master branch &amp; that it’s not a PR.   4) Change ownership &amp; permission of files to allow Travis to execute the heroku-container-release.sh script.   #!/bin/bash  sudo docker login --username $HEROKU_DOCKER_USERNAME --password $HEROKU_AUTH_TOKEN registry.heroku.com sudo docker tag webapp-dpdth:latest registry.heroku.com/webapp-dpdth/web if [ $TRAVIS_BRANCH == \"master\" ] &amp;&amp; [ $TRAVIS_PULL_REQUEST == \"false\" ]; then sudo docker push registry.heroku.com/webapp-dpdth/web; fi  chmod +x heroku-container-release.sh sudo chown $USER:docker ~/.docker sudo chown $USER:docker ~/.docker/config.json sudo chmod g+rw ~/.docker/config.json  ./heroku-container-release.sh      heroku-container-release.sh   This file is for releasing a Docker image via Heroku’s API. Replace webapp-dpdth with your app name on Heroku.   #!/bin/bash imageId=$(docker inspect registry.heroku.com/webapp-dpdth/web --format={{.Id}}) payload='{\"updates\":[{\"type\":\"web\",\"docker_image\":\"'\"$imageId\"'\"}]}' curl -n -X PATCH https://api.heroku.com/apps/webapp-dpdth/formation \\ -d \"$payload\" \\ -H \"Content-Type: application/json\" \\ -H \"Accept: application/vnd.heroku+json; version=3.docker-releases\" \\ -H \"Authorization: Bearer $HEROKU_AUTH_TOKEN\"   See here for details.     Procfile   This file is for Heroku. The command in the release: section will run after a Docker image is released. It will run the migrate command with --noinput option. Without running migrate, the database on Heroku may not function correctly.   It also tells Heroku to deploy the web app using Gunicorn as the production server.   Note that airline is the Django project name. It’s not the web app name in the Django project &amp; is also not the web app name in Heroku.   release: python manage.py migrate --noinput web: gunicorn airline.wsgi     The deployed web app   With the above files in place, push to Github &amp; Travis will start testing. After all tests passed, deployment starts. If there isn’t any failures, the web app will be running on:   https://webapp-dpdth.herokuapp.com   This link brings you to the admin page. It is using the Heroku’s copy of Postgres.   This link brings you to my built log in Travis.com which shows how a successful test/deploy built looks like.     Web security:   Please note that web security has not been throughly consider in this basic workflow describe above. Do NOT simply use the above workflow for production.   For example the SECRET_KEY in the settings.py isn’t dealt with at all and web security is really beyond the scope of this post.        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/DPDTH/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Django + Postgres + Travis CI + Heroku CD",
        "excerpt":"Basic workflow of testing a Django &amp; Postgres web app with Travis (continuous integration) &amp; deployment to Heroku (continuous deployment).     Prerequisite:   This post assumes that the reader has accounts with Github, Travis &amp; Heroku &amp; already has the accounts configured. For example, linking Travis with Github, setting up Postgres server in Heroku &amp; setting OS environment variables in Travis &amp; Heroku websites.     Code on my Github     See here for a Dockerized version.     Which copy of Postgres to use during the different stages in the workflow?   During development, the local Postgres database server is used. When testing in Travis, we’ll used Travis’s copy of Postgres &amp; when deploying, we’ll have to use Heroku’s copy.     makemigrations   Always run python manage.py makemigrations before deployment to Heroku or in our case, before pushing to Github.   The actual python manage.py migrate for the Postgres server addon from Heroku will be run by the deploy section in the .travis.yml file.     Listing files &amp; directories in a tree:   cd web_app_DPTH  $ tree -a -I \"CS50_web_dev|staticfiles|static|templates|LICENSE|README.md|__init__.py|settings_DPTH_.py|urls.py|wsgi.py|db.sqlite3|airline4_tests_.py|apps.py|migrations|views.py|models.py|flights.csv|manage.py|wait-for-it.sh|admin.py|.git|.travis_DPTH_.yml|Dockerfile|docker-compose.yml\"    . ├── .travis.yml ├── Procfile ├── airline │   └── settings.py ├── flights │   └── tests.py └── requirements.txt   As shown in the tree above, the 5 files that matter in the workflow:   1) tests.py   2) settings.py   3) requirements.txt   4) .travis.yml   5) Procfile   We will look at the contents of each of the 5 files in the sections below.     tests.py   This is the test file that Travis will use for testing the app. You write whatever test you want for Travis to run with.   from django.db.models import Max from django.test import Client, TestCase  from .models import Airport, Flight, Passenger  # Create your tests here. class FlightsTestCase(TestCase):      def setUp(self):          # Create airports.         a1 = Airport.objects.create(code=\"AAA\", city=\"City A\")         a2 = Airport.objects.create(code=\"BBB\", city=\"City B\")          # Create flights.         Flight.objects.create(origin=a1, destination=a2, duration=100)         Flight.objects.create(origin=a1, destination=a1, duration=200)         Flight.objects.create(origin=a2, destination=a1, duration=300)      # 1     def test_departures_count(self):         a = Airport.objects.get(code=\"AAA\")         self.assertEqual(a.departures.count(), 2)      # 2     def test_arrivals_count(self):         a = Airport.objects.get(code=\"AAA\")         self.assertEqual(a.arrivals.count(), 2)      # 3     def test_valid_flight(self):         a1 = Airport.objects.get(code=\"AAA\")         a2 = Airport.objects.get(code=\"BBB\")         f = Flight.objects.get(origin=a1, destination=a2)         self.assertTrue(f.is_valid_flight())     settings.py   Under the database section, the database credentials, as OS environment variables, has to be made available to Travis &amp; Heroku. They can be set in their respective websites.   Add and/or edit the following to the settings.py file:   import django_heroku import dj_database_url   MIDDLEWARE = [     'django.middleware.security.SecurityMiddleware',      'whitenoise.middleware.WhiteNoiseMiddleware',  # new      'django.contrib.sessions.middleware.SessionMiddleware',     'django.middleware.common.CommonMiddleware',     'django.middleware.csrf.CsrfViewMiddleware',     'django.contrib.auth.middleware.AuthenticationMiddleware',     'django.contrib.messages.middleware.MessageMiddleware',     'django.middleware.clickjacking.XFrameOptionsMiddleware', ]   DATABASES = {     'default': {         'ENGINE': 'django.db.backends.postgresql',         'NAME': os.environ['DATABASE_NAME'],         'USER': os.environ['DATABASE_USER'],         'PASSWORD': os.environ['DATABASE_PASSWORD'],         'HOST': os.environ['DATABASE_HOST'],         'PORT': os.environ['DATABASE_PORT'],     } }   STATIC_ROOT = os.path.join(BASE_DIR, 'staticfiles')   django_heroku.settings(locals())      requirements.txt   This file lets Travis know what packages are needed for the app.   django&gt;=2.0.11 psycopg2 psycopg2-binary dj-database-url==0.5.0 gunicorn whitenoise django-heroku pytz sqlparse     .travis.yml   This file contains instructions for Travis &amp; is needed when Travis starts running. $HEROKU_API_KEY can be generated from the Heroku website &amp; stored as an OS environment variable in the Travis website. The test is done with the Travis’s copy of Postgres.   language: python python:     - 3.6 services:     - postgresql install:     - pip install -r requirements.txt script:     - python manage.py test deploy:     provider: heroku     api_key: $HEROKU_API_KEY     app: webapp-dpth     run: python manage.py migrate     on: master     Procfile   This file is for Heroku. It tells Heroku to deploy the web app using Gunicorn as the production server.   Note that airline is the Django project name.   web: gunicorn airline.wsgi     The deployed web app   With the above files in place, push to Github &amp; Travis will start testing. After all tests passed, deployment starts. If there isn’t any failures, the web app will be running on:   https://webapp-dpth.herokuapp.com   This link brings you to the admin page. It is using the Heroku’s copy of Postgres.   This link brings you to my built log in Travis.com which shows how a successful test/deploy built looks like.     Web security:   Please note that web security has not been throughly consider in this basic workflow describe above. Do NOT simply use the above workflow for production.   For example the SECRET_KEY in the settings.py isn’t dealt with at all and web security is really beyond the scope of this post.        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/DPTH/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Reinforcement learning custom environment in Sagemaker with Ray (RLlib)",
        "excerpt":"Demo setup for simple (reinforcement learning) custom environment in Sagemaker. This example uses Proximal Policy Optimization with Ray (RLlib).     Code on my Github     The training script:   import json import os  import gym import ray from ray.tune import run_experiments import ray.rllib.agents.a3c as a3c import ray.rllib.agents.ppo as ppo from ray.tune.registry import register_env from mod_op_env import ArrivalSim  from sagemaker_rl.ray_launcher import SageMakerRayLauncher  \"\"\" def create_environment(env_config):     import gym #     from gym.spaces import Space     from gym.envs.registration import register     # This import must happen inside the method so that worker processes import this code     register(         id='ArrivalSim-v0',         entry_point='env:ArrivalSim',         kwargs= {'price' : 40}     )     return gym.make('ArrivalSim-v0') \"\"\" def create_environment(env_config):     price = 30.0     # This import must happen inside the method so that worker processes import this code     from mod_op_env import ArrivalSim     return ArrivalSim(price)   class MyLauncher(SageMakerRayLauncher):     def __init__(self):                 super(MyLauncher, self).__init__()         self.num_gpus = int(os.environ.get(\"SM_NUM_GPUS\", 0))         self.hosts_info = json.loads(os.environ.get(\"SM_RESOURCE_CONFIG\"))[\"hosts\"]         self.num_total_gpus = self.num_gpus * len(self.hosts_info)      def register_env_creator(self):         register_env(\"ArrivalSim-v0\", create_environment)      def get_experiment_config(self):         return {           \"training\": {             \"env\": \"ArrivalSim-v0\",             \"run\": \"PPO\",             \"stop\": {               \"training_iteration\": 3,             },              \"local_dir\": \"/opt/ml/model/\",             \"checkpoint_freq\" : 3,              \"config\": {                                               #\"num_workers\": max(self.num_total_gpus-1, 1),               \"num_workers\": max(self.num_cpus-1, 1),               #\"use_gpu_for_workers\": False,               \"train_batch_size\": 128, #5,               \"sample_batch_size\": 32, #1,               \"gpu_fraction\": 0.3,               \"optimizer\": {                 \"grads_per_step\": 10               },             },             #\"trial_resources\": {\"cpu\": 1, \"gpu\": 0, \"extra_gpu\": max(self.num_total_gpus-1, 1), \"extra_cpu\": 0},             #\"trial_resources\": {\"cpu\": 1, \"gpu\": 0, \"extra_gpu\": max(self.num_total_gpus-1, 0),             #                    \"extra_cpu\": max(self.num_cpus-1, 1)},             \"trial_resources\": {\"cpu\": 1,                                 \"extra_cpu\": max(self.num_cpus-1, 1)},                         }         }  if __name__ == \"__main__\":     os.environ[\"LC_ALL\"] = \"C.UTF-8\"     os.environ[\"LANG\"] = \"C.UTF-8\"     os.environ[\"RAY_USE_XRAY\"] = \"1\"     print(ppo.DEFAULT_CONFIG)     MyLauncher().train_main()     The Jupyter notebook:   !/bin/bash ./setup.sh  from time import gmtime, strftime import sagemaker role = sagemaker.get_execution_role()  sage_session = sagemaker.session.Session() s3_bucket = sage_session.default_bucket()   s3_output_path = 's3://{}/'.format(s3_bucket) print(\"S3 bucket path: {}\".format(s3_output_path))  job_name_prefix = 'ArrivalSim'  from sagemaker.rl import RLEstimator, RLToolkit, RLFramework  estimator = RLEstimator(entry_point=\"mod_op_train.py\", # Our launcher code                         source_dir='src', # Directory where the supporting files are at. All of this will be                                           # copied into the container.                         dependencies=[\"common/sagemaker_rl\"], # some other utils files.                         toolkit=RLToolkit.RAY, # We want to run using the Ray toolkit against the ray container image.                         framework=RLFramework.TENSORFLOW, # The code is in tensorflow backend.                         toolkit_version='0.5.3', # Toolkit version. This will also choose an apporpriate tf version.                                                                        #toolkit_version='0.6.5', # Toolkit version. This will also choose an apporpriate tf version.                                                 role=role, # The IAM role that we created at the begining.                         #train_instance_type=\"ml.m4.xlarge\", # Since we want to run fast, lets run on GPUs.                         train_instance_type=\"local\", # Since we want to run fast, lets run on GPUs.                         train_instance_count=1, # Single instance will also work, but running distributed makes things                                                 # fast, particularly in the case of multiple rollout training.                         output_path=s3_output_path, # The path where we can expect our trained model.                         base_job_name=job_name_prefix, # This is the name we setup above to be to track our job.                         hyperparameters = {      # Some hyperparameters for Ray toolkit to operate.                           \"s3_bucket\": s3_bucket,                           \"rl.training.stop.training_iteration\": 2, # Number of iterations.                           \"rl.training.checkpoint_freq\": 2,                         },                         #metric_definitions=metric_definitions, # This will bring all the logs out into the notebook.                     )  estimator.fit()     The container in Sagemaker Jupyter notebook instance:        See related issue/motivation.        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/sagemaker_RL_custom_env/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Custom Sagemaker reinforcement learning container",
        "excerpt":"Building &amp; testing custom Sagemaker RL container.   Instead of using the official SageMaker supported version of Ray RLlib (version 0.5.3 &amp; 0.6.5), I want to use version 0.7.3. In order to do so, I have to build &amp; test my custom Sagemaker RL container.     The Dockerfile:   Add the Dockerfile below to sagemaker-rl-container/ray/docker/0.7.3/:   ARG processor #FROM 520713654638.dkr.ecr.us-west-2.amazonaws.com/sagemaker-tensorflow-scriptmode:1.14.0-$processor-py3 FROM 520713654638.dkr.ecr.us-west-2.amazonaws.com/sagemaker-tensorflow-scriptmode:1.12.0-$processor-py3  RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\         build-essential \\         jq \\         libav-tools \\         libjpeg-dev \\         libxrender1 \\         python3.6-dev \\         python3-opengl \\         wget \\         xvfb &amp;&amp; \\     apt-get clean &amp;&amp; \\     rm -rf /var/lib/apt/lists/*  RUN pip install --no-cache-dir \\     Cython==0.29.7 \\     gym==0.14.0 \\     lz4==2.1.10 \\     opencv-python-headless==4.1.0.25 \\     PyOpenGL==3.1.0 \\     pyyaml==5.1.1 \\     redis&gt;=3.2.2 \\     ray==0.7.3 \\     ray[rllib]==0.7.3 \\     scipy==1.3.0 \\     requests  # https://click.palletsprojects.com/en/7.x/python3/ ENV LC_ALL=C.UTF-8 ENV LANG=C.UTF-8  # Copy workaround script for incorrect hostname COPY lib/changehostname.c /  COPY lib/start.sh /usr/local/bin/start.sh RUN chmod +x /usr/local/bin/start.sh  # Starts framework ENTRYPOINT [\"bash\", \"-m\", \"start.sh\"]     Remove unneeded test files:   Backup the test folder as test_bkup in sagemaker-rl-container/.   Remove the following files not used in testing in sagemaker-rl-container/test/integration/local/:   test_coach.py test_vw_cb_explore.py test_vw_cbify.py test_vw_serving.py     Add/replace codes in test files to get role:   In the sagemaker-rl-container/test/conftest.pyfile, add/replace the following:   from sagemaker import get_execution_role   #parser.addoption('--role', default='SageMakerContainerBuildIntegrationTests') parser.addoption('--role', default=get_execution_role()),    In the following files:   sagemaker-rl-container/test/integration/local/test_gym.py sagemaker-rl-container/test/integration/local/test_ray.py   Add/replace the following:   from sagemaker import get_execution_role   #role='SageMakerRole', role = get_execution_role(),     Build the image:   In SageMaker, start a Jupyter notebook instance &amp; open a terminal.   Login into SageMaker ECR account:   $ (aws ecr get-login --no-include-email --region &lt;region&gt; --registry-ids &lt;AWS_ACC_ID&gt;) $ (aws ecr get-login --no-include-email --region us-west-2 --registry-ids 520713654638)   Copy &amp; paste the output from the above command into the terminal &amp; press Enter.     Pull the base Tensorflow image from the aws ecr:   $ docker pull 520713654638.dkr.ecr.us-west-2.amazonaws.com/sagemaker-tensorflow-scriptmode:1.12.0-cpu-py3   Build the Ray image using the Dockerfile.tf from above:   $ docker build -t custom-smk-rl-ctn:tf-1.12.0-ray-0.7.3-cpu-py3 -f ray/docker/0.7.3/Dockerfile.tf --build-arg processor=cpu .     Local testing:   Install dependencies for testing:   $ cd sagemaker-rl-container $ pip install .   Run the command below for local testing:   clear &amp;&amp; \\ docker images &amp;&amp; \\ pytest test/integration/local --framework tensorflow \\                               --toolkit ray \\                               --toolkit-version 0.7.3 \\                               --docker-base-name custom-smk-rl-ctn \\                               --tag tf-1.12.0-ray-0.7.3-cpu-py3 \\                               --processor cpu | tee local_test_output.txt   The output from the test will be saved in local_test_output.txt.     Pushing to registry on AWS ECR:   $ (aws ecr get-login --no-include-email --region &lt;region&gt; --registry-ids &lt;AWS_ACC_ID&gt;) $ (aws ecr get-login --no-include-email --region us-west-2  --registry-ids 123456789012) # Copy &amp; paste output to terminal &amp; press enter.  $ aws ecr create-repository --repository-name &lt;repo_name&gt; $ aws ecr create-repository --repository-name custom-smk-rl-ctn  $ docker tag &lt;image_ID&gt; &lt;AWS_ACC_ID&gt;.dkr.ecr.us-west-2.amazonaws.com/&lt;repo_name&gt;:&lt;tag&gt; $ docker tag ba542f0b9706 &lt;123456789012&gt;.dkr.ecr.us-west-2.amazonaws.com/custom-smk-rl-ctn:tf-1.12.0-cpu-py3 $ docker tag ba542f0b9706 &lt;123456789012&gt;.dkr.ecr.us-west-2.amazonaws.com/custom-smk-rl-ctn:tf-1.12.0-ray-0.7.3-cpu-py3  $ docker push &lt;AWS_ACC_ID&gt;.dkr.ecr.us-west-2.amazonaws.com/&lt;repo_name&gt;:&lt;tag&gt; $ docker push &lt;123456789012&gt;.dkr.ecr.us-west-2.amazonaws.com/custom-smk-rl-ctn:tf-1.12.0-cpu-py3 $ docker push &lt;123456789012&gt;.dkr.ecr.us-west-2.amazonaws.com/custom-smk-rl-ctn:tf-1.12.0-ray-0.7.3-cpu-py3   $ aws ecr describe-repositories  $ aws ecr list-images --repository-name &lt;repo_name&gt; $ aws ecr list-images --repository-name custom-smk-rl-ctn     Testing with AWS SageMaker ML instance:   Run the command below for testing with SageMaker:   clear &amp;&amp; \\ docker images &amp;&amp; \\ pytest test/integration/sagemaker --aws-id 123456789012 \\                                   --instance-type ml.m4.xlarge \\                                   --framework tensorflow \\                                   --toolkit ray \\                                   --toolkit-version 0.7.3 \\                                   --docker-base-name custom-smk-rl-ctn \\                                   --tag tf-1.12.0-ray-0.7.3-cpu-py3 | tee SageMaker_test_output.txt   The output from the test will be saved in SageMaker_test_output.txt.     Pushing to registry on Docker hub:   $ docker login  $ docker tag &lt;image_ID&gt; &lt;DockerHubUserName&gt;/&lt;repo_name&gt;:&lt;tag&gt; $ docker tag ba542f0b9706 &lt;DockerHubUserName&gt;/custom-smk-rl-ctn:tf-1.12.0-cpu-py3 $ docker tag ba542f0b9706 &lt;DockerHubUserName&gt;/custom-smk-rl-ctn:tf-1.12.0-ray-0.7.3-cpu-py3  $ docker push &lt;DockerHubUserName&gt;/&lt;repo_name&gt;:&lt;tag&gt; $ docker push &lt;DockerHubUserName&gt;/custom-smk-rl-ctn:tf-1.12.0-cpu-py3 $ docker push &lt;DockerHubUserName&gt;/custom-smk-rl-ctn:tf-1.12.0-ray-0.7.3-cpu-py3     Training with custom SageMaker RL container:     Useful Docker commands:   $ docker ps -a  $ docker images  $ docker rm &lt;container&gt;  $ docker rmi &lt;image&gt;     Useful AWS commands:   $ aws ecr delete-repository --force --repository-name &lt;repo_name&gt;     References:   https://github.com/aws/sagemaker-rl-container        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/custom_sagemaker_RL_container/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Filter rows with same column values in Pandas dataframe",
        "excerpt":"Filter rows with same column values in a Pandas dataframe.     Assume the following Pandas dataframe where Q1 &amp; Q2 are charges. A charge can either be +1 or -1.   df = pd.DataFrame([[1,1],[-1,-1],[1,-1],[-1,1]], columns=['Q1', 'Q2'])     Q1  Q2 0   1   1 1  -1  -1 2   1  -1 3  -1   1   The following code produces a dataframe where each row only contains opposite charges, i.e if Q1 is 1, Q2 is -1 &amp; vice versa.   df = df[ (df.Q1 == 1) &amp; (df.Q2 == -1) | (df.Q1 == -1) &amp; (df.Q2 == 1) ]     Q1  Q2 2   1  -1 3  -1   1        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/pandas_df_rm_row_with_same_col_val/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "RNN backprop thru time(BPTT)",
        "excerpt":"Notes on the math for RNN back propagation through time(BPTT).       is the prediction.    is the weight matrix after the hidden layer.        is the hidden layer at time t.    is the non linear function in the hidden layer.    is the weight matrix before the hidden layer.    is the weight matrix in the hidden layer at the previous time step.    is the bias at the hidden layer.       No need to BPTT for this:       BPTT   The loss at time t with respect to the weights in the hidden layer.   The terms in square brackets  are written in such a way that the leftmost term is the most recent term while the rightmost is the oldest term.   When k = t, the product sequence (or factor) on the left of , equals 1.             BPTT also needs to be done for calculating the loss with respect to weight matrix V. The dependence between hidden units and weight matrix V is  not only in one place. Hidden units from all the previous time steps also depend on V so we need to go backwards in time to calculate this gradient.        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/RNN_BPTT/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "RNN backprop thru time(BPTT part 2) $$\\frac{\\delta h_{t}} {\\delta h_{t-1}}$$",
        "excerpt":"Notes on the math for RNN back propagation through time(BPTT), part 2. The 1st derivative of  with respect to .    Given a series:    Given a set of functions that takes in :     We have a vector of functions:       A Jacobian is a matrix of 1st derivatives of the functions:     For the calculations below, we are only interested in the diagonal derivatives which are terms with the same subscript in the nominator &amp; denominator (i.e: derivatives with respect to the same time step).       Deriving :                  ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/RNN_BPTT_2/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Linear regression (Bayesian)",
        "excerpt":"Notes on the probability for linear regression (Bayesian)     Bayes’ theorem:     So,     The 1st part of the nominator from 1 is:     From joint probability:       Apply 3 to 2:     Plug 4 back to 1:           If w and x are independent:     Also from 5, if we switch w with y, we can obtain:         We try to maximize 7 with respect to w. Only the nominator depends on w, so we can ignore the denominator P(y|x) and we get:   Maximize with respect to w in the following equations 8, 9, 10:     Therefore, from 6 &amp; 8 we get:            ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/linear_regression_bayesian/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Changing G drive directory in Colab",
        "excerpt":"Changing Google drive directory in Colab.     from google.colab import drive drive.mount('/content/gdrive')  %cd \"/content/gdrive/My Drive/Colab Notebooks/courses/deep_learning/\"  !pwd        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/colab/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "Output dimension from convolution layer",
        "excerpt":"How to calculate dimension of output from a convolution layer?     No padding (aka valid padding):   input  = n x n = 6 x 6   kernel = f x f = 3 x 3   output = m x m = 4 x 4   How do we get output = 4 x 4 ?   Ans: Use the formula: (n - f + 1) x (n - f + 1)     With padding of size 1:   p = 1   input  = n x n = 6 x 6   kernel = f x f = 3 x 3   output = m x m = 6 x 6   How do we get output = 6 x 6 ?   Ans: Use the formula: (n + 2p - f + 1) x (n + 2p - f + 1)     Meaning of valid padding &amp; same padding:   1) No padding is also known as valid padding.   2) Same padding means pad input so that the resulting output dimension after convolution will be the same as input.     Size of padding needed to achieve same padding:   Size of padding needed to achieve same padding depends on the kernel size, f.   Using p = (f - 1) / 2 will produce output dimension = input dimension.       ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/conv_output/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"},{
        "title": "RLlib trainer common config",
        "excerpt":"Ray (0.8.2) RLlib trainer common config from:   source     # yapf: disable # __sphinx_doc_begin__ COMMON_CONFIG = {     # === Settings for Rollout Worker processes ===     # Number of rollout worker actors to create for parallel sampling. Setting     # this to 0 will force rollouts to be done in the trainer actor.     \"num_workers\": 2,     # Number of environments to evaluate vectorwise per worker. This enables     # model inference batching, which can improve performance for inference     # bottlenecked workloads.     \"num_envs_per_worker\": 1,     # Divide episodes into fragments of this many steps each during rollouts.     # Sample batches of this size are collected from rollout workers and     # combined into a larger batch of `train_batch_size` for learning.     #     # For example, given rollout_fragment_length=100 and train_batch_size=1000:     #   1. RLlib collects 10 fragments of 100 steps each from rollout workers.     #   2. These fragments are concatenated and we perform an epoch of SGD.     #     # When using multiple envs per worker, the fragment size is multiplied by     # `num_envs_per_worker`. This is since we are collecting steps from     # multiple envs in parallel. For example, if num_envs_per_worker=5, then     # rollout workers will return experiences in chunks of 5*100 = 500 steps.     #     # The dataflow here can vary per algorithm. For example, PPO further     # divides the train batch into minibatches for multi-epoch SGD.     \"rollout_fragment_length\": 200,     # Deprecated; renamed to `rollout_fragment_length` in 0.8.4.     \"sample_batch_size\": DEPRECATED_VALUE,     # Whether to rollout \"complete_episodes\" or \"truncate_episodes\" to     # `rollout_fragment_length` length unrolls. Episode truncation guarantees     # evenly sized batches, but increases variance as the reward-to-go will     # need to be estimated at truncation boundaries.     \"batch_mode\": \"truncate_episodes\",      # === Settings for the Trainer process ===     # Number of GPUs to allocate to the trainer process. Note that not all     # algorithms can take advantage of trainer GPUs. This can be fractional     # (e.g., 0.3 GPUs).     \"num_gpus\": 0,     # Training batch size, if applicable. Should be &gt;= rollout_fragment_length.     # Samples batches will be concatenated together to a batch of this size,     # which is then passed to SGD.     \"train_batch_size\": 200,     # Arguments to pass to the policy model. See models/catalog.py for a full     # list of the available model options.     \"model\": MODEL_DEFAULTS,     # Arguments to pass to the policy optimizer. These vary by optimizer.     \"optimizer\": {},      # === Environment Settings ===     # Discount factor of the MDP.     \"gamma\": 0.99,     # Number of steps after which the episode is forced to terminate. Defaults     # to `env.spec.max_episode_steps` (if present) for Gym envs.     \"horizon\": None,     # Calculate rewards but don't reset the environment when the horizon is     # hit. This allows value estimation and RNN state to span across logical     # episodes denoted by horizon. This only has an effect if horizon != inf.     \"soft_horizon\": False,     # Don't set 'done' at the end of the episode. Note that you still need to     # set this if soft_horizon=True, unless your env is actually running     # forever without returning done=True.     \"no_done_at_end\": False,     # Arguments to pass to the env creator.     \"env_config\": {},     # Environment name can also be passed via config.     \"env\": None,     # Unsquash actions to the upper and lower bounds of env's action space     \"normalize_actions\": False,     # Whether to clip rewards prior to experience postprocessing. Setting to     # None means clip for Atari only.     \"clip_rewards\": None,     # Whether to np.clip() actions to the action space low/high range spec.     \"clip_actions\": True,     # Whether to use rllib or deepmind preprocessors by default     \"preprocessor_pref\": \"deepmind\",     # The default learning rate.     \"lr\": 0.0001,      # === Debug Settings ===     # Whether to write episode stats and videos to the agent log dir. This is     # typically located in ~/ray_results.     \"monitor\": False,     # Set the ray.rllib.* log level for the agent process and its workers.     # Should be one of DEBUG, INFO, WARN, or ERROR. The DEBUG level will also     # periodically print out summaries of relevant internal dataflow (this is     # also printed out once at startup at the INFO level). When using the     # `rllib train` command, you can also use the `-v` and `-vv` flags as     # shorthand for INFO and DEBUG.     \"log_level\": \"WARN\",     # Callbacks that will be run during various phases of training. These all     # take a single \"info\" dict as an argument. For episode callbacks, custom     # metrics can be attached to the episode by updating the episode object's     # custom metrics dict (see examples/custom_metrics_and_callbacks.py). You     # may also mutate the passed in batch data in your callback.     \"callbacks\": {         \"on_episode_start\": None,     # arg: {\"env\": .., \"episode\": ...}         \"on_episode_step\": None,      # arg: {\"env\": .., \"episode\": ...}         \"on_episode_end\": None,       # arg: {\"env\": .., \"episode\": ...}         \"on_sample_end\": None,        # arg: {\"samples\": .., \"worker\": ...}         \"on_train_result\": None,      # arg: {\"trainer\": ..., \"result\": ...}         \"on_postprocess_traj\": None,  # arg: {                                       #   \"agent_id\": ..., \"episode\": ...,                                       #   \"pre_batch\": (before processing),                                       #   \"post_batch\": (after processing),                                       #   \"all_pre_batches\": (other agent ids),                                       # }     },     # Whether to attempt to continue training if a worker crashes. The number     # of currently healthy workers is reported as the \"num_healthy_workers\"     # metric.     \"ignore_worker_failures\": False,     # Log system resource metrics to results. This requires `psutil` to be     # installed for sys stats, and `gputil` for GPU metrics.     \"log_sys_usage\": True,      # === Framework Settings ===     # Use PyTorch (instead of tf). If using `rllib train`, this can also be     # enabled with the `--torch` flag.     # NOTE: Some agents may not support `torch` yet and throw an error.     \"use_pytorch\": False,      # Enable TF eager execution (TF policies only). If using `rllib train`,     # this can also be enabled with the `--eager` flag.     \"eager\": False,     # Enable tracing in eager mode. This greatly improves performance, but     # makes it slightly harder to debug since Python code won't be evaluated     # after the initial eager pass.     \"eager_tracing\": False,     # Disable eager execution on workers (but allow it on the driver). This     # only has an effect if eager is enabled.     \"no_eager_on_workers\": False,      # === Exploration Settings ===     # Default exploration behavior, iff `explore`=None is passed into     # compute_action(s).     # Set to False for no exploration behavior (e.g., for evaluation).     \"explore\": True,     # Provide a dict specifying the Exploration object's config.     \"exploration_config\": {         # The Exploration class to use. In the simplest case, this is the name         # (str) of any class present in the `rllib.utils.exploration` package.         # You can also provide the python class directly or the full location         # of your class (e.g. \"ray.rllib.utils.exploration.epsilon_greedy.         # EpsilonGreedy\").         \"type\": \"StochasticSampling\",         # Add constructor kwargs here (if any).     },     # === Evaluation Settings ===     # Evaluate with every `evaluation_interval` training iterations.     # The evaluation stats will be reported under the \"evaluation\" metric key.     # Note that evaluation is currently not parallelized, and that for Ape-X     # metrics are already only reported for the lowest epsilon workers.     \"evaluation_interval\": None,     # Number of episodes to run per evaluation period. If using multiple     # evaluation workers, we will run at least this many episodes total.     \"evaluation_num_episodes\": 10,     # Internal flag that is set to True for evaluation workers.     \"in_evaluation\": False,     # Typical usage is to pass extra args to evaluation env creator     # and to disable exploration by computing deterministic actions.     # IMPORTANT NOTE: Policy gradient algorithms are able to find the optimal     # policy, even if this is a stochastic one. Setting \"explore=False\" here     # will result in the evaluation workers not using this optimal policy!     \"evaluation_config\": {         # Example: overriding env_config, exploration, etc:         # \"env_config\": {...},         # \"explore\": False     },     # Number of parallel workers to use for evaluation. Note that this is set     # to zero by default, which means evaluation will be run in the trainer     # process. If you increase this, it will increase the Ray resource usage     # of the trainer since evaluation workers are created separately from     # rollout workers.     \"evaluation_num_workers\": 0,     # Customize the evaluation method. This must be a function of signature     # (trainer: Trainer, eval_workers: WorkerSet) -&gt; metrics: dict. See the     # Trainer._evaluate() method to see the default implementation. The     # trainer guarantees all eval workers have the latest policy state before     # this function is called.     \"custom_eval_function\": None,     # EXPERIMENTAL: use the execution plan based API impl of the algo. Can also     # be enabled by setting RLLIB_EXEC_API=1.     \"use_exec_api\": False,      # === Advanced Rollout Settings ===     # Use a background thread for sampling (slightly off-policy, usually not     # advisable to turn on unless your env specifically requires it).     \"sample_async\": False,     # Element-wise observation filter, either \"NoFilter\" or \"MeanStdFilter\".     \"observation_filter\": \"NoFilter\",     # Whether to synchronize the statistics of remote filters.     \"synchronize_filters\": True,     # Configures TF for single-process operation by default.     \"tf_session_args\": {         # note: overriden by `local_tf_session_args`         \"intra_op_parallelism_threads\": 2,         \"inter_op_parallelism_threads\": 2,         \"gpu_options\": {             \"allow_growth\": True,         },         \"log_device_placement\": False,         \"device_count\": {             \"CPU\": 1         },         \"allow_soft_placement\": True,  # required by PPO multi-gpu     },     # Override the following tf session args on the local worker     \"local_tf_session_args\": {         # Allow a higher level of parallelism by default, but not unlimited         # since that can cause crashes with many concurrent drivers.         \"intra_op_parallelism_threads\": 8,         \"inter_op_parallelism_threads\": 8,     },     # Whether to LZ4 compress individual observations     \"compress_observations\": False,     # Wait for metric batches for at most this many seconds. Those that     # have not returned in time will be collected in the next train iteration.     \"collect_metrics_timeout\": 180,     # Smooth metrics over this many episodes.     \"metrics_smoothing_episodes\": 100,     # If using num_envs_per_worker &gt; 1, whether to create those new envs in     # remote processes instead of in the same worker. This adds overheads, but     # can make sense if your envs can take much time to step / reset     # (e.g., for StarCraft). Use this cautiously; overheads are significant.     \"remote_worker_envs\": False,     # Timeout that remote workers are waiting when polling environments.     # 0 (continue when at least one env is ready) is a reasonable default,     # but optimal value could be obtained by measuring your environment     # step / reset and model inference perf.     \"remote_env_batch_wait_ms\": 0,     # Minimum time per train iteration (frequency of metrics reporting).     \"min_iter_time_s\": 0,     # Minimum env steps to optimize for per train call. This value does     # not affect learning, only the length of train iterations.     \"timesteps_per_iteration\": 0,  # TODO(ekl) deprecate this     # This argument, in conjunction with worker_index, sets the random seed of     # each worker, so that identically configured trials will have identical     # results. This makes experiments reproducible.     \"seed\": None,      # === Advanced Resource Settings ===     # Number of CPUs to allocate per worker.     \"num_cpus_per_worker\": 1,     # Number of GPUs to allocate per worker. This can be fractional. This is     # usually needed only if your env itself requires a GPU (i.e., it is a     # GPU-intensive video game), or model inference is unusually expensive.     \"num_gpus_per_worker\": 0,     # Any custom Ray resources to allocate per worker.     \"custom_resources_per_worker\": {},     # Number of CPUs to allocate for the trainer. Note: this only takes effect     # when running in Tune. Otherwise, the trainer runs in the main program.     \"num_cpus_for_driver\": 1,     # You can set these memory quotas to tell Ray to reserve memory for your     # training run. This guarantees predictable execution, but the tradeoff is     # if your workload exceeeds the memory quota it will fail.     # Heap memory to reserve for the trainer process (0 for unlimited). This     # can be large if your are using large train batches, replay buffers, etc.     \"memory\": 0,     # Object store memory to reserve for the trainer process. Being large     # enough to fit a few copies of the model weights should be sufficient.     # This is enabled by default since models are typically quite small.     \"object_store_memory\": 0,     # Heap memory to reserve for each worker. Should generally be small unless     # your environment is very heavyweight.     \"memory_per_worker\": 0,     # Object store memory to reserve for each worker. This only needs to be     # large enough to fit a few sample batches at a time. This is enabled     # by default since it almost never needs to be larger than ~200MB.     \"object_store_memory_per_worker\": 0,      # === Offline Datasets ===     # Specify how to generate experiences:     #  - \"sampler\": generate experiences via online simulation (default)     #  - a local directory or file glob expression (e.g., \"/tmp/*.json\")     #  - a list of individual file paths/URIs (e.g., [\"/tmp/1.json\",     #    \"s3://bucket/2.json\"])     #  - a dict with string keys and sampling probabilities as values (e.g.,     #    {\"sampler\": 0.4, \"/tmp/*.json\": 0.4, \"s3://bucket/expert.json\": 0.2}).     #  - a function that returns a rllib.offline.InputReader     \"input\": \"sampler\",     # Specify how to evaluate the current policy. This only has an effect when     # reading offline experiences. Available options:     #  - \"wis\": the weighted step-wise importance sampling estimator.     #  - \"is\": the step-wise importance sampling estimator.     #  - \"simulation\": run the environment in the background, but use     #    this data for evaluation only and not for learning.     \"input_evaluation\": [\"is\", \"wis\"],     # Whether to run postprocess_trajectory() on the trajectory fragments from     # offline inputs. Note that postprocessing will be done using the *current*     # policy, not the *behavior* policy, which is typically undesirable for     # on-policy algorithms.     \"postprocess_inputs\": False,     # If positive, input batches will be shuffled via a sliding window buffer     # of this number of batches. Use this if the input data is not in random     # enough order. Input is delayed until the shuffle buffer is filled.     \"shuffle_buffer_size\": 0,     # Specify where experiences should be saved:     #  - None: don't save any experiences     #  - \"logdir\" to save to the agent log dir     #  - a path/URI to save to a custom output directory (e.g., \"s3://bucket/\")     #  - a function that returns a rllib.offline.OutputWriter     \"output\": None,     # What sample batch columns to LZ4 compress in the output data.     \"output_compress_columns\": [\"obs\", \"new_obs\"],     # Max output file size before rolling over to a new file.     \"output_max_file_size\": 64 * 1024 * 1024,      # === Settings for Multi-Agent Environments ===     \"multiagent\": {         # Map from policy ids to tuples of (policy_cls, obs_space,         # act_space, config). See rollout_worker.py for more info.         \"policies\": {},         # Function mapping agent ids to policy ids.         \"policy_mapping_fn\": None,         # Optional whitelist of policies to train, or None for all policies.         \"policies_to_train\": None,     }, } # __sphinx_doc_end__ # yapf: enable        ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/RLlib_trainer_config/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/blog/ELG.png"}]
