var store = [{
        "title": "DQN",
        "excerpt":"A Deep Q Network implementation in tensorflow with target network &amp; random experience replay.   Environment from OpenAI’s gym: CartPole-v0   Full code        Notations:   Model network =    Model parameter =    Model network Q value =  (s, a)     Target network =    Target parameter =    Target network Q value =  (, )        Equations:   TD target = r (s, a)     (, )     TD  error = (TD target)  (Model network Q value)    = [r (s, a)     (, )]   (s, a)        Implementation details:   Update target parameter  with model parameter  :   Copy  to  with either soft or hard parameter update.     Hard parameter update:  with tf.variable_scope('hard_replace'):   self.target_replace_hard = [t.assign(m) for t, m in zip(self.target_net_params, self.model_net_params)]     # hard params replacement if self.learn_step % self.tau_step == 0:     self.sess.run(self.target_replace_hard)   self.learn_step += 1      Soft parameter update:   polyak    + (1  polyak)       with tf.variable_scope('soft_replace'):               self.target_replace_soft = [t.assign(self.polyak * m + (1 - self.polyak) * t)                               for t, m in zip(self.target_net_params, self.model_net_params)]        Stop TD target from contributing to gradient computation:  # exclude td_target in gradient computation td_target = tf.stop_gradient(td_target)       References:   Human-level control through deep reinforcement learning (Mnih et al., 2015)    ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/DQN/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/foggy.jpg"},{
        "title": "DDQN",
        "excerpt":"A Double Deep Q Network (DDQN) implementation in tensorflow with random experience replay.   Environment from OpenAI’s gym: CartPole-v0   Full code      Notations:   Model network =    Model parameter =    Model network Q value =  (s, a)     Target network =    Target parameter =    Target network Q value =  (, )      Equations:   TD target = r (s, a)    (,   (s, a))     TD  error = (TD target)  (Model network Q value)    = [r (s, a)    (,   (s, a))]   (s, a)      Implementation details:   Create a placeholder to feed Q values from model network:   self.model_s_next_Q_val = tf.placeholder(tf.float32, [None,self.num_actions], name='model_s_next_Q_val')   Select Q values from model network using  as features &amp; feed them to the training session:   # select actions from model network model_s_next_Q_val = self.sess.run(self.model_Q_val, feed_dict={self.s: s_next})  # training _, loss = self.sess.run([self.optimizer, self.loss],                         feed_dict = {self.s: s,                                      self.a: a,                                      self.r: r,                                      self.s_next: s_next,                                      self.done: done,                                      self.model_s_next_Q_val: model_s_next_Q_val})   Select minibatch actions with largest Q values from model network, create indices &amp; select corresponding minibatch actions from target network:   def td_target(self, s_next, r, done, model_s_next_Q_val, target_Q_val):     # select action with largest Q value from model network     model_max_a = tf.argmax(model_s_next_Q_val, axis=1, output_type=tf.dtypes.int32)      arr = tf.range(tf.shape(model_max_a)[0], dtype=tf.int32) # create row indices     indices = tf.stack([arr, model_max_a], axis=1) # create 2D indices             max_target_Q_val = tf.gather_nd(target_Q_val, indices) # select minibatch actions from target network     max_target_Q_val = tf.reshape(max_target_Q_val, (self.minibatch_size,1))      References:   Deep Reinforcement Learning with Double Q-learning (Hasselt, Guez &amp; Silver, 2016)      ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/DDQN/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/foggy.jpg"},{
        "title": "Dueling DDQN",
        "excerpt":"A Dueling Double Deep Q Network (Dueling DDQN) implementation in tensorflow with random experience replay.   Environment from OpenAI’s gym: CartPole-v0   Full code      Notations:   Network =    Parameter =    Network Q value =  (s, a)     Value function = V(s)   Advantage function = A(s, a)     Parameter from the Advantage function layer =    Parameter from the Value function layer =       Equations:   (eqn 9) from the original paper (Wang et al., 2015):   Q(s, a; , , ) = V(s; , )     [ A(s, a; , )    A(s, ; , ) ]      Implementation details:   V represents the value function layer, A represents the Advantage function layer:  # contruct neural network def built_net(self, var_scope, w_init, b_init, features, num_hidden, num_output):                   with tf.variable_scope(var_scope):                 feature_layer = tf.contrib.layers.fully_connected(features, num_hidden,                                                         activation_fn = tf.nn.relu,                                                         weights_initializer = w_init,                                                         biases_initializer = b_init)       V = tf.contrib.layers.fully_connected(feature_layer, 1,                                             activation_fn = None,                                             weights_initializer = w_init,                                             biases_initializer = b_init)       A = tf.contrib.layers.fully_connected(feature_layer, num_output,                                             activation_fn = None,                                             weights_initializer = w_init,                                             biases_initializer = b_init)          Q_val = V + (A - tf.reduce_mean(A, reduction_indices=1, keepdims=True)) # refer to eqn 9 from the original paper               return Q_val         References:   Dueling Network Architectures for Deep Reinforcement Learning (Wang et al., 2015)      ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/Duel_DDQN/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/foggy.jpg"},{
        "title": "Dueling DDQN with PER",
        "excerpt":"A Dueling Double Deep Q Network with Priority Experience Replay (Duel DDQN with PER) implementation in tensorflow.   Environment from OpenAI’s gym: CartPole-v0   Full code      Notations:   Model network =    Model parameter =    Model network Q value =  (s, a)     Target network =    Target parameter =    Target network Q value =  (, )     A small constant to ensure that no sample has 0 probability to be selected = e   Hyper parameter  =      Decides how to sample, range from 0 to 1, where 0 corresponds to fully uniformly random sample selection &amp; 1 corresponding to selecting samples based on highest priority.    Hyper parameter  =      Starts close to 0, gradually annealed  to 1, slowly giving more importance to weights during training.    Minibatch size = k   Replay memory size = N      Equations:   TD target = r (s, a)    (,   (s, a))     TD  error =     = (TD target)  (Model network Q value)    = [r (s, a)    (,   (s, a))]   (s, a)        =     =   e     probability(i) = P(i)    =      weights =  = (N  P(i))       Implementation details:   Sum tree:      Assume an example of a sum tree with 7 nodes (with 4 leaves which corresponds to the replay memory size):            At initialization:                                 When item 1 is added:                                 When item 2 is added:                                 When item 3 is added:                                 When item 4 is added:                                 When item 5 is added:                            Figure below shows the corresponding code &amp; array contents. The tree represents the entire sum tree while data represents the leaves.                   In the implementation, only one sumTree object is needed to store the collected experiences, this sumTree object resides in the Replay_memory class. The sumTree object has number of leaves = replay memory size = capacity. The data array in sumTree object stores an Exp object, which is a sample of experience.      The following code decides how to sample:  def sample(self, k): # k = minibatch size     batch = []      # total_p() gives the total sum of priorities of the leaves in the sumTree     # which is the value stored in the root node     segment = self.tree.total_p() / k      for i in range(k):         a = segment * i # start of segment         b = segment * (i + 1) # end of segment         s = np.random.uniform(a, b) # rand value between a, b          (idx, p, data) = self.tree.get(s)         batch.append( (idx, p, data) )                  return batch       Refer to appendix B.2.1, under the section, “Proportional prioritization”, from the original (Schaul et al., 2016) paper for sampling details.      References:   Prioritized experience replay (Schaul et al., 2016)      ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/Duel_DDQN_with_PER/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/foggy.jpg"},{
        "title": "Numpy array manipulation",
        "excerpt":"Simple numpy array manipulation examples.   Full code   This Jupyter notebook contains simple examples on how to manipulate numpy arrays. The code blocks below shows the codes &amp; it’s corresponding display output.   Setting up a numpy array:  buffer=[0,1] print('buffer=', buffer) $buffer= [0, 1]  new=2 print('new=', new) $new= 2  buffer = np.array(buffer + [new]) # append a new item &amp; create a numpy array print('np.array(buffer + [new])=', buffer) $np.array(buffer + [new])= [0 1 2]   Slicing examples:  # numpy array slicing syntax # buffer[start:stop:step]  print('buffer[1:]=', buffer[1:]) # starting from index 1 $buffer[1:]= [1 2]  print('buffer[-1:]=', buffer[-1:]) # getting item in last index $buffer[-1:]= [2]  print('buffer[:1]=', buffer[:1]) # stop at index 1 (exclusive), keep only 1st item $buffer[:1]= [0]  print('buffer[:-1]=', buffer[:-1]) # stop at last index (exclusive), discard item in last index $buffer[:-1]= [0 1]  print('buffer[::-1]=', buffer[::-1]) # start from last index (reversal) $buffer[::-1]= [2 1 0]  print('buffer[1::-1]=', buffer[1::-1]) # reverse starting from index 1 $buffer[1::-1]= [1 0]  # Starting from index 1 will return [1 2], reversing will return [2,1] print('buffer[1:][::-1]=', buffer[1:][::-1]) $buffer[1:][::-1]= [2 1]   np.newaxis is an alias for None:  # np.newaxis = None  print('buffer[:, np.newaxis]=', buffer[:, np.newaxis]) $buffer[:, np.newaxis]= [[0][1][2]]  print('buffer[:, None]=', buffer[:, None]) $buffer[:, None]= [[0][1][2]]  print('buffer[np.newaxis, :]=', buffer[np.newaxis, :]) $buffer[np.newaxis, :]= [[0 1 2]]  print('buffer[None, :]=', buffer[None, :]) $buffer[None, :]= [[0 1 2]]   Stacking:  a = [1,2,3] b = [4,5,6] c = [7,8,9]  r = np.hstack((a,b,c)) # horizontal stacking print(\"r=\", r) $r= [1 2 3 4 5 6 7 8 9]  QUEUE = queue.Queue() QUEUE.put(a) QUEUE.put(b) QUEUE.put(c)  r = [QUEUE.get() for _ in range(QUEUE.qsize())] print(r) $[[1, 2, 3], [4, 5, 6], [7, 8, 9]]  r = np.vstack(r) # vertical stacking print(r) $[[1 2 3]   [4 5 6]   [7 8 9]]  print(r[:, ::-1]) # col reversal $[[3 2 1]   [6 5 4]   [9 8 7]]  ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/np_array_manipulation/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/foggy.jpg"},{
        "title": "Python's multiprocessing package",
        "excerpt":"Python’s multiprocessing package for parallel data generation.   Full code   This simple example program demonstrates how to use the Python’s multiprocessing package to achieve parallel data generation.   The main program has a chief that spawns multiple worker processes. Each worker spawns a single work process. The work process generates random integer data [1,3].   Each worker has it’s own local queue. When data is generated, it is stored in it’s local queue. When the local queue’s size is greater than 5, the data is retrieved &amp; 0.1 is added to the data, this result is stored in the Chief’s global queue. When the Chief’s global queue’s size is greater than 3, the result is retrieved &amp; printed on screen.   The Worker class:  class Worker(object):   def __init__(self, worker_id, g_queue):     self.g_queue = g_queue     self.worker_id = worker_id     self.queue = Queue() # local worker queue     self.work_process = Process(target=self.work, args=())     self.work_process.start()     info(worker_id, self.work_process, \"Worker\")    def work(self):      info(self.worker_id, self.work_process, \"work\")      while True:       data = np.random.randint(1,4)       self.queue.put(data)        # process data in queue       if self.queue.qsize() &gt; 5:         data = self.queue.get()         result = data + 0.1         self.g_queue.put(result) # send result to global queue        time.sleep(1) # work every x sec interval      return self.w_id     The Chief class:  class Chief(object):   def __init__(self, num_workers):     self.g_queue = Queue() # global queue         self.num_workers = num_workers    def dispatch_workers(self):        worker_processes = [Process(target=Worker(w_id, self.g_queue), args=()) for w_id in range(num_workers)]     return worker_processes    def result(self):     if self.g_queue.qsize() &gt; 3:       result = self.g_queue.get()       print(\"result\", result)   The main program:  if __name__ == '__main__':     print('main parent process id:', os.getppid())   print('main process id:', os.getpid())    num_workers = 2   chief = Chief(num_workers)   workers_processes = chief.dispatch_workers()    i = 0   while True:         time.sleep(2) # chk g_queue every x sec interval to get result     chief.result()     print(\"i=\", i)      if i&gt;9:       break     i+=1       A helper display function:  def info(worker_id, process, function_name):     print(\"worker_id=\", worker_id,           'module name:', __name__,           'function name:', function_name,           'parent process:', os.getppid(),           'current process id:', os.getpid(),           'spawn process id:', process.pid)  ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/py_mpp/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/foggy.jpg"},{
        "title": "N-step targets",
        "excerpt":"N-step Q-values estimation.   Full code   The following two functions computes truncated Q-values estimates:   1) n_step_targets_missing   treats missing terms as 0.   2) n_step_targets_max   use maximum terms possible.       1-step truncated estimate :    = E( +  V())   2-step truncated estimate :    = E( +   +   V())   3-step truncated estimate :    = E( +   +   +  V())   N-step truncated estimate :    = E( +   +   + … +  V())       Assuming we have the following variables setup:  N=2 # N steps gamma=2 t=5 v_s_ = 10 # value of next state  epr=np.arange(t).reshape(t,1) print(\"epr=\", epr)  baselines=np.arange(t).reshape(t,1) print(\"baselines=\", baselines)   Display output of episodic rewards(epr) &amp; baselines:  epr= [[0]  [1]  [2]  [3]  [4]]  baselines= [[0]  [1]  [2]  [3]  [4]]   This function computes the n-step targets, treats missing terms as zero:  # if number of steps unavailable, missing terms treated as 0. def n_step_targets_missing(epr, baselines, gamma, N):   N = N+1   targets = np.zeros_like(epr)       if N &gt; epr.size:     N = epr.size   for t in range(epr.size):        print(\"t=\", t)     for n in range(N):       print(\"n=\", n)       if t+n == epr.size:                     print('missing terms treated as 0, break') # last term for those with insufficient steps.         break # missing terms treated as 0       if n == N-1: # last term         targets[t] += (gamma**n) * baselines[t+n] # last term for those with sufficient steps         print('last term for those with sufficient steps, end inner n loop')       else:         targets[t] += (gamma**n) * epr[t+n] # non last terms   return targets   Run the function n_step_targets_missing:  print('n_step_targets_missing:') T = n_step_targets_missing(epr, baselines, gamma, N) print(T)   Display the output:  n_step_targets_missing: t= 0 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 1 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 2 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 3 n= 0 n= 1 n= 2 missing terms treated as 0, break t= 4 n= 0 n= 1 missing terms treated as 0, break [[10]  [17]  [24]  [11]  [ 4]]  For the output above, note that when t+n = 5 which is greater than the last index 4, missing terms are treated as 0.       This function computes the n-step targets, it will use maximum number of terms possible:  # if number of steps unavailable, use max steps available. # uses v_s_ as input def n_step_targets_max(epr, baselines, v_s_, gamma, N):   N = N+1   targets = np.zeros_like(epr)       if N &gt; epr.size:     N = epr.size   for t in range(epr.size):       print(\"t=\", t)     for n in range(N):       print(\"n=\", n)       if t+n == epr.size:                     targets[t] += (gamma**n) * v_s_ # last term for those with insufficient steps.         print('last term for those with INSUFFICIENT steps, break')         break       if n == N-1:         targets[t] += (gamma**n) * baselines[t+n] # last term for those with sufficient steps         print('last term for those with sufficient steps, end inner n loop')       else:         targets[t] += (gamma**n) * epr[t+n] # non last terms   return targets   Run the function n_step_targets_max:  print('n_step_targets_max:') T = n_step_targets_max(epr, baselines, v_s_, gamma, N) print(T)   Display the output:  n_step_targets_max: t= 0 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 1 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 2 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 3 n= 0 n= 1 n= 2 last term for those with INSUFFICIENT steps, break t= 4 n= 0 n= 1 last term for those with INSUFFICIENT steps, break [[10]  [17]  [24]  [51]  [24]]  For the output above, note that when t+n = 5 which is greater than the last index 4, maximum terms are used where possible. ( Last term for those with INSUFFICIENT steps is given by (gamma**n) * v_s_ =  V()), where v_s_ = V()   t=2, normal 2 steps estimation:    = E( +   +   V())   t=3, 2 steps estimation with insufficient step, using v_s_ in the last term:    = E( +   +   V())   t=4, insufficient step for 2 steps estimation, resorting to 1 step estimation:    = E( +  V())  ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/n_step_targets/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/foggy.jpg"},{
        "title": "Distributed Tensorflow",
        "excerpt":"Distributed Tensorflow with Python multiprocessing package.   Full code   This post demonstrates how to use distributed Tensorflow with Python’s multiprocessing package. A tf.FIFOQueue is used as a storage across processes.   Cluster definition:   2 workers, 1 parameter server.  cluster = tf.train.ClusterSpec({     \"worker\": [\"localhost:2223\",                \"localhost:2224\"               ],     \"ps\": [\"localhost:2225\"] })   Parameter server function:   A tf.Variable (var) &amp; a tf.FIFOQueue (q) is declared with the parameter server. They are both sharable across processes.   For tf.FIFOQueue to be sharable, it has to be declared with the same device (in this case, the ps device) in both the parameter_server function and the worker function. A shared_name has to be given as well.   The tf.Variable (var) is also declared under the ps device. The value of var is displayed in the first for loop.   At the end of the function, the values stored in q will be displayed in the last for loop.  def parameter_server():     with tf.device(\"/job:ps/task:0\"):         var = tf.Variable(0.0, name='var')                 q = tf.FIFOQueue(10, tf.float32, shared_name=\"shared_queue\")      server = tf.train.Server(cluster,                              job_name=\"ps\",                              task_index=0)     sess = tf.Session(target=server.target)      print(\"Parameter server: waiting for cluster connection...\")     sess.run(tf.report_uninitialized_variables())     print(\"Parameter server: cluster ready!\")      print(\"Parameter server: initializing variables...\")     sess.run(tf.global_variables_initializer())     print(\"Parameter server: variables initialized\")      for i in range(10):         print(\"Parameter server: var has value %.1f\" % sess.run(var))         sleep(1.0)         if sess.run(var) == 10.0:           break      sleep(3.0)     print(\"ps q.size(): \", sess.run(q.size()))        for j in range(sess.run(q.size())):         print(\"ps: r\", sess.run(q.dequeue()))      #print(\"Parameter server: blocking...\")     #server.join() # currently blocks forever         print(\"Parameter server: ended...\")   Worker function:   tf.FIFOQueue (q) is declared with the ps device. A same shared_name is also used.   The tf.Variable (var) is declared under the worker device. It does not have to be declared under the ps device.   The for loop increments the value of var and the values are stored in q.  def worker(worker_n):     with tf.device(\"/job:ps/task:0\"):         q = tf.FIFOQueue(10, tf.float32, shared_name=\"shared_queue\")          with tf.device(tf.train.replica_device_setter(                         worker_device='/job:worker/task:' + str(worker_n),                         cluster=cluster)):         var = tf.Variable(0.0, name='var')      server = tf.train.Server(cluster,                              job_name=\"worker\",                              task_index=worker_n)     sess = tf.Session(target=server.target)      print(\"Worker %d: waiting for cluster connection...\" % worker_n)     sess.run(tf.report_uninitialized_variables())     print(\"Worker %d: cluster ready!\" % worker_n)      while sess.run(tf.report_uninitialized_variables()):         print(\"Worker %d: waiting for variable initialization...\" % worker_n)         sleep(1.0)     print(\"Worker %d: variables initialized\" % worker_n)      for i in range(5):         print(\"Worker %d: incrementing var\" % worker_n, sess.run(var))         sess.run(var.assign_add(1.0))         qe = q.enqueue(sess.run(var))         sess.run(qe)         sleep(1.0)      print(\"Worker %d: ended...\" % worker_n)   Main program:   Create the processes, run them and finally terminate them in a for loop.  ps_proc = Process(target=parameter_server, daemon=True) w1_proc = Process(target=worker, args=(0, ), daemon=True) w2_proc = Process(target=worker, args=(1, ), daemon=True)  ps_proc.start() w1_proc.start() w2_proc.start()  ps_proc.join() # only ps need to call join()  for proc in [w1_proc, w2_proc, ps_proc]:     proc.terminate() # only way to kill server is to kill it's process  print('All done.')               Output:  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer.  WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer. Parameter server: waiting for cluster connection... Worker 0: waiting for cluster connection... Worker 1: waiting for cluster connection... Worker 1: cluster ready! Worker 1: waiting for variable initialization... Parameter server: cluster ready! Parameter server: initializing variables... Parameter server: variables initialized Parameter server: var has value 0.0 Worker 0: cluster ready! /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:65: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size &gt; 0` to check that an array is not empty. Worker 0: variables initialized Worker 0: incrementing var 0.0 /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:65: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size &gt; 0` to check that an array is not empty. Worker 1: variables initialized Worker 1: incrementing var 1.0 Parameter server: var has value 2.0 Worker 0: incrementing var 2.0 Worker 1: incrementing var 3.0 Parameter server: var has value 4.0 Worker 0: incrementing var 4.0 Worker 1: incrementing var 5.0 Parameter server: var has value 6.0 Worker 0: incrementing var 6.0 Worker 1: incrementing var 7.0 Parameter server: var has value 8.0 Worker 0: incrementing var 8.0 Worker 1: incrementing var 9.0 Worker 0: ended... Worker 1: ended... ps q.size():  10 ps: r 1.0 ps: r 2.0 ps: r 3.0 ps: r 4.0 ps: r 5.0 ps: r 6.0 ps: r 7.0 ps: r 8.0 ps: r 9.0 ps: r 10.0 Parameter server: ended... All done.  ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/dist_tf/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/foggy.jpg"},{
        "title": "Accumulate gradients with Tensorflow",
        "excerpt":"This post demonstrates how to accumulate gradients with Tensorflow.   Full code   import tensorflow as tf  def accumu_grad(self, OPT, loss, scope):     # retrieve trainable variables in scope of graph     #tvs = tf.trainable_variables(scope=scope + '/actor')     tvs = tf.trainable_variables(scope=scope)      # ceate a list of variables with the same shape as the trainable     accumu = [tf.Variable(tf.zeros_like(tv.initialized_value()), trainable=False) for tv in tvs]      zero_op = [tv.assign(tf.zeros_like(tv)) for tv in accumu] # initialized with 0s      gvs = OPT.compute_gradients(loss, tvs) # obtain list of gradients &amp; variables     #gvs = [(tf.where( tf.is_nan(grad), tf.zeros_like(grad), grad ), var) for grad, var in gvs]      # adds to each element from the list you initialized earlier with zeros its gradient     # accumu and gvs are in same shape, index 0 is grads, index 1 is vars     accumu_op = [accumu[i].assign_add(gv[0]) for i, gv in enumerate(gvs)]      apply_op = OPT.apply_gradients([(accumu[i], gv[1]) for i, gv in enumerate(gvs)]) # apply grads      return zero_op, accumu_op, apply_op, accumu                  ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/tf_accumulate_grad/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/foggy.jpg"},{
        "title": "A3C multi-threaded discrete version with N step targets",
        "excerpt":"An A3C (Asynchronous Advantage Actor Critic) implementation with Tensorflow. This is a multi-threaded discrete version.   Environment from OpenAI’s gym: CartPole-v0 (Discrete)   Full code: A3C (discrete) multi-threaded version with N-step targets(missing terms are treated as 0)   Full code: A3C (discrete) multi-threaded version with N-step targets(use maximum terms possible)     Notations:   Actor network =    Actor network parameter =    Critic network =    Critic network parameter =    Advantage function = A   Number of trajectories = m     Equations:   Actor component: log    Critic component = Advantage function = A =  -    Q values with N-step truncated estimate :    = E( +   +   + … +  V())   Check this post for more information on N-step truncated estimate.   Policy gradient estimator   =    =     log   -    =     log  A     Implementation details:   The ACNet class defines the models (Tensorflow graphs) and contains both the actor and the critic networks.   The Worker class contains the work function that does the main bulk of the computation.   A copy of ACNet is declared globally &amp; it’s parameters are shared by the threaded workers. Each worker also have it’s own local copy of ACNet.   Workers are instantiated &amp; threaded in the main program.   ACNet class:   Loss function for the actor network for the discrete environment:   with tf.name_scope('actor_loss'):     log_prob = tf.reduce_sum(tf.log(self.action_prob + 1e-5) * tf.one_hot(self.a, num_actions, dtype=tf.float32), axis=1, keep_dims=True)     actor_component = log_prob * tf.stop_gradient(self.baselined_returns)     # entropy for exploration     entropy = -tf.reduce_sum(self.action_prob * tf.log(self.action_prob + 1e-5), axis=1, keep_dims=True)  # encourage exploration     self.actor_loss = tf.reduce_mean( -(ENTROPY_BETA * entropy + actor_component) )                                          Loss function for the critic network for the discrete environment:   TD_err = tf.subtract(self.critic_target, self.V, name='TD_err')       .       .       . with tf.name_scope('critic_loss'):     self.critic_loss = tf.reduce_mean(tf.square(TD_err))   The following function in the ACNet class creates the actor and critic’s neural networks:   def _create_net(self, scope):     w_init = tf.glorot_uniform_initializer()     with tf.variable_scope('actor'):         hidden = tf.layers.dense(self.s, actor_hidden, tf.nn.relu6, kernel_initializer=w_init, name='hidden')         action_prob = tf.layers.dense(hidden, num_actions, tf.nn.softmax, kernel_initializer=w_init, name='action_prob')             with tf.variable_scope('critic'):         hidden = tf.layers.dense(self.s, critic_hidden, tf.nn.relu6, kernel_initializer=w_init, name='hidden')         V = tf.layers.dense(hidden, 1, kernel_initializer=w_init, name='V')              actor_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')     critic_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')            return action_prob, V, actor_params, critic_params   Worker class:   Discounted rewards are used as critic’s targets:   critic_target = self.discount_rewards(buffer_r, GAMMA, V_s)   N-step targets are used in the computation of the Advantage function(baselined_returns):   # Advantage function baselined_returns = n_step_targets - baseline   2 versions of N-step targets could be used:   Version 1) missing terms are treated as 0.   Version 2) use maximum terms possible.   Check this post for more information on N-step targets.   The following code segment accumulates gradients &amp; apply them to the local critic network:   self.AC.accumu_grad_critic(feed_dict) # accumulating gradients for local critic   self.AC.apply_accumu_grad_critic(feed_dict)   The following code segment computes the advantage function(baselined_returns):   baseline = SESS.run(self.AC.V, {self.AC.s: buffer_s}) # Value function epr = np.vstack(buffer_r).astype(np.float32) n_step_targets = self.compute_n_step_targets_missing(epr, baseline, GAMMA, N_step) # Q values # Advantage function baselined_returns = n_step_targets - baseline   The following code segment accumulates gradients for the local actor network:   self.AC.accumu_grad_actor(feed_dict) # accumulating gradients for local actor     The following code segment push the parameters from the local networks to the global networks and then pulls the updated global parameters to the local networks:   # update self.AC.push_global_actor(feed_dict)                 self.AC.push_global_critic(feed_dict)     .     .     . self.AC.pull_global()   The following code segment initialize storage for accumulated local gradients.   self.AC.init_grad_storage_actor() # initialize storage for accumulated gradients. self.AC.init_grad_storage_critic()               Check this post for more information on how to accumulate gradients in Tensorflow.   Main program:   The following code segment creates the workers:   workers = [] for i in range(num_workers): # Create worker     i_name = 'W_%i' % i # worker name     workers.append(Worker(i_name, GLOBAL_AC))   The following code segment threads the workers:   worker_threads = [] for worker in workers:     job = lambda: worker.work()     t = threading.Thread(target=job)     t.start()     worker_threads.append(t) COORD.join(worker_threads)   References:   Asynchronous Methods for Deep Reinforcement Learning (Mnih, Badia, Mirza, Graves, Harley, Lillicrap, et al., 2016)      ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/A3C_disc_thread_nStep/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/foggy.jpg"},{
        "title": "A3C multi-threaded continuous version with N step targets",
        "excerpt":"An A3C (Asynchronous Advantage Actor Critic) implementation with Tensorflow. This is a multi-threaded continuous version.   Environment from OpenAI’s gym: Pendulum-v0 (Continuous)   Full code: A3C (continuous) multi-threaded version with N-step targets(use maximum terms possible)   The majority of the code is very similar to the discrete version with the exceptions highlighted in the following sections:   Action selection:   with tf.name_scope('select_action'):     #mean = mean * action_bound[1]                        mean = mean * ( action_bound[1] - action_bound[0] ) / 2     sigma += 1e-4     normal_dist = tf.distributions.Normal(mean, sigma)                          self.choose_a = tf.clip_by_value(tf.squeeze(normal_dist.sample(1), axis=[0, 1]), action_bound[0], action_bound[1])                     Loss function of the actor network:   with tf.name_scope('actor_loss'):     log_prob = normal_dist.log_prob(self.a)     #actor_component = log_prob * tf.stop_gradient(TD_err)     actor_component = log_prob * tf.stop_gradient(self.baselined_returns)     entropy = -tf.reduce_mean(normal_dist.entropy()) # Compute the differential entropy of the multivariate normal.                        self.actor_loss = -tf.reduce_mean( ENTROPY_BETA * entropy + actor_component)   The following code segment creates a LSTM layer:   def _lstm(self, Inputs, cell_size):         # [time_step, feature] =&gt; [time_step, batch, feature]         s = tf.expand_dims(Inputs, axis=1, name='time_major')           lstm_cell = tf.nn.rnn_cell.LSTMCell(cell_size)         self.init_state = lstm_cell.zero_state(batch_size=1, dtype=tf.float32)         outputs, self.final_state = tf.nn.dynamic_rnn(cell=lstm_cell, inputs=s, initial_state=self.init_state, time_major=True)         # joined state representation                   lstm_out = tf.reshape(outputs, [-1, cell_size], name='flatten_rnn_outputs')           return lstm_out   The following function in the ACNet class creates the actor and critic’s neural networks(note that the critic’s network contains a LSTM layer):   def _create_net(self, scope):     w_init = tf.glorot_uniform_initializer()     #w_init = tf.random_normal_initializer(0., .1)     with tf.variable_scope('actor'):                                 hidden = tf.layers.dense(self.s, actor_hidden, tf.nn.relu6, kernel_initializer=w_init, name='hidden')                     #lstm_out = self._lstm(hidden, cell_size)         # tanh range = [-1,1]         mean = tf.layers.dense(hidden, num_actions, tf.nn.tanh, kernel_initializer=w_init, name='mean')         # softplus range = {0,inf}         sigma = tf.layers.dense(hidden, num_actions, tf.nn.softplus, kernel_initializer=w_init, name='sigma')     with tf.variable_scope('critic'):         hidden = tf.layers.dense(self.s, critic_hidden, tf.nn.relu6, kernel_initializer=w_init, name='hidden')         lstm_out = self._lstm(hidden, cell_size)         V = tf.layers.dense(lstm_out, 1, kernel_initializer=w_init, name='V')       actor_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')     critic_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')     return mean, sigma, V, actor_params, critic_params  ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/A3C_cont_thread_nStep/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/foggy.jpg"},{
        "title": "A3C distributed tensorflow",
        "excerpt":"An A3C (Asynchronous Advantage Actor Critic) implementation with distributed Tensorflow &amp; Python multiprocessing package. This is a discrete version.   Environment from OpenAI’s gym: Cartpole-v0 (discrete)   Full code: A3C (discrete) distributed Tensorflow version with N-step targets(use maximum terms possible)   The majority of the code is very similar to the discrete version with the exceptions highlighted in the following sections:   Updating the global episode counter &amp; adding the episodic return to a tf.FIFOqueue at the end of the work() function.   SESS.run(GLOBAL_EP.assign_add(1.0)) qe = GLOBAL_RUNNING_R.enqueue(ep_r) SESS.run(qe)   The distributed Tensorflow part is very similar to a simple example described in this post.   Pin the global variables under the parameter server in both the parameter_server() &amp; worker(worker_n) function:   with tf.device(\"/job:ps/task:0\"):     GLOBAL_AC = ACNet(net_scope, sess, globalAC=None) # only need its params     GLOBAL_EP = tf.Variable(0.0, name='GLOBAL_EP') # num of global episodes        # a queue of ep_r     GLOBAL_RUNNING_R = tf.FIFOQueue(max_global_episodes, tf.float32, shared_name=\"GLOBAL_RUNNING_R\")           In the parameter_server() function, check the size of the tf.FIFOqueue every 1 sec. If it’s full, dequeue the items in a list. the list will be used for display.   while True:     time.sleep(1.0)     #print(\"ps 1 GLOBAL_EP: \", sess.run(GLOBAL_EP))     #print(\"ps 1 GLOBAL_RUNNING_R.size(): \", sess.run(GLOBAL_RUNNING_R.size()))       if sess.run(GLOBAL_RUNNING_R.size()) &gt;= max_global_episodes: # GLOBAL_EP starts from 0, hence +1 to max_global_episodes                   time.sleep(5.0)         #print(\"ps 2 GLOBAL_RUNNING_R.size(): \", sess.run(GLOBAL_RUNNING_R.size()))           GLOBAL_RUNNING_R_list = []         for j in range(sess.run(GLOBAL_RUNNING_R.size())):             ep_r = sess.run(GLOBAL_RUNNING_R.dequeue())             GLOBAL_RUNNING_R_list.append(ep_r) # for display         break  ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/A3C_dist_tf/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/foggy.jpg"},{
        "title": "DPPO distributed tensorflow",
        "excerpt":"Distributed Proximal Policy Optimization (Distributed PPO or DPPO) continuous version  implementation with distributed Tensorflow and Python’s multiprocessing package. This implementation uses normalized running rewards with GAE.   Environment from OpenAI’s gym: Pendulum-v0 (Continuous)   Full code:   Notations:   current policy =    old policy =    epsilon =    Advantage function = A        Equations:   Truncated version of generalized advantage estimation(GAE) =    =    where  =    when  = 1,    =    Probability ratio =    =    Clipped Surrogate Objective function =    =         Implementation details:   The following class is adapted from OpenAI’s baseline: This class is used for the normalization of rewards in this program before GAE computation.   class RunningStats(object):     def __init__(self, epsilon=1e-4, shape=()):         self.mean = np.zeros(shape, 'float64')         self.var = np.ones(shape, 'float64')         self.std = np.ones(shape, 'float64')         self.count = epsilon      def update(self, x):         batch_mean = np.mean(x, axis=0)         batch_var = np.var(x, axis=0)         batch_count = x.shape[0]         self.update_from_moments(batch_mean, batch_var, batch_count)      def update_from_moments(self, batch_mean, batch_var, batch_count):         delta = batch_mean - self.mean         new_mean = self.mean + delta * batch_count / (self.count + batch_count)         m_a = self.var * self.count         m_b = batch_var * batch_count         M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)         new_var = M2 / (self.count + batch_count)          self.mean = new_mean         self.var = new_var         self.std = np.maximum(np.sqrt(self.var), 1e-6)         self.count = batch_count + self.count   This function in the PPO class is adapted from OpenAI’s Baseline, returns TD lamda return &amp; advantage       def add_vtarg_and_adv(self, R, done, V, v_s_, gamma, lam):         # Compute target value using TD(lambda) estimator, and advantage with GAE(lambda)         # last element is only used for last vtarg, but we already zeroed it if last new = 1         done = np.append(done, 0)         V_plus = np.append(V, v_s_)         T = len(R)         adv = gaelam = np.empty(T, 'float32')         lastgaelam = 0         for t in reversed(range(T)):             nonterminal = 1-done[t+1]                     delta = R[t] + gamma * V_plus[t+1] * nonterminal - V_plus[t]             gaelam[t] = lastgaelam = delta + gamma * lam * nonterminal * lastgaelam            #print(\"adv=\", adv.shape)         #print(\"V=\", V.shape)         #print(\"V_plus=\", V_plus.shape)         tdlamret = np.vstack(adv) + V         #print(\"tdlamret=\", tdlamret.shape)         return tdlamret, adv # tdlamret is critic_target or Qs         The following code segment from the PPO class defines the Clipped Surrogate Objective function:   with tf.variable_scope('surrogate'):                     ratio = self.pi.prob(self.act) / self.oldpi.prob(self.act)                     surr = ratio * self.adv                     self.aloss = -tf.reduce_mean(tf.minimum(surr, tf.clip_by_value(ratio, 1.-epsilon, 1.+epsilon)*self.adv))   The following code segment from the work() function in the worker class normalized the running rewards for each worker:   self.running_stats_r.update(np.array(buffer_r))                     buffer_r = np.clip( (np.array(buffer_r) - self.running_stats_r.mean) / self.running_stats_r.std, -stats_CLIP, stats_CLIP )   The following code segment from the work() function in the worker class computes the TD lamda return &amp; advantage:   tdlamret, adv = self.ppo.add_vtarg_and_adv(np.vstack(buffer_r), np.vstack(buffer_done), np.vstack(buffer_V), v_s_, GAMMA, lamda)    The following update function in the PPO class does the training &amp; the updating of global &amp; local parameters (Note the at the beginning of training, probability ratio = 1):   def update(self, s, a, r, adv):         self.sess.run(self.update_oldpi_op)      for _ in range(A_EPOCH): # train actor         self.sess.run(self.atrain_op, {self.state: s, self.act: a, self.adv: adv})         # update actor         self.sess.run([self.push_actor_pi_params,                        self.pull_actor_pi_params],                       {self.state: s, self.act: a, self.adv: adv})     for _ in range(C_EPOCH): # train critic         # update critic         self.sess.run(self.ctrain_op, {self.state: s, self.discounted_r: r})         self.sess.run([self.push_critic_params,                        self.pull_critic_params],                       {self.state: s, self.discounted_r: r})      The distributed Tensorflow &amp; multiprocessing code sections are very similar to the ones describe in the following posts:   Distributed Tensorflow   A3C distributed tensorflow        References:   Proximal Policy Optimization Algorithms (Schulman, Wolski, Dhariwal, Radford, Klimov, 2017)   Emergence of Locomotion Behaviours in Rich Environments (Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, et al., 2017)  ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/DPPO_dist_tf/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/foggy.jpg"},{
        "title": "RDN (Random Distillation Network) with Proximal Policy Optimization (PPO) Tensorflow",
        "excerpt":"Random Distillation Network (RDN) with Proximal Policy Optimization (PPO) implentation in Tensorflow. This is a continuous version which solves the mountain car continuous problem (MountainCarContinuous-v0). The RDN helps learning with curiosity driven exploration.   The agent starts to converge correctly at around 30 episodes &amp; reached the flag 291 times out of 300 episodes (97% hit rate). It takes 385.09387278556824 seconds to complete 300 episodes on Google’s Colab.   Checkout the resulting charts from program output.   Full code: Python file, Jupyter notebook (The Jupyter notebook, which also contain the resulting charts at the end, can be run directly on Google’s Colab.)     Notations &amp; equations   fixed feature from target network =    predicted feature from predictor network =    intrinsic reward =  = ||  -  ||    For notations &amp; equations regarding PPO, refer to this post.     Implementation details:   Preprocessing, state featurization:   Prior to training, the states are featurized with the RBF kernel.   (states are also featurized during every training batch.)   Refer to scikit-learn.org documentation: 5.7.2. Radial Basis Function Kernel for more information on RBF kernel.   if state_ftr == True: \"\"\" The following code for state featurization is adapted &amp; modified from dennybritz's repository located at: https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb \"\"\"     # Feature Preprocessing: Normalize to zero mean and unit variance     # We use a few samples from the observation space to do this     states = np.array([env.observation_space.sample() for x in range(sample_size)]) # pre-trained, states preprocessing     scaler = sklearn.preprocessing.StandardScaler()     scaler.fit(states) # Compute the mean and std to be used for later scaling.      # convert states to a featurizes representation.     # We use RBF kernels with different variances to cover different parts of the space     featurizer = sklearn.pipeline.FeatureUnion([ # Concatenates results of multiple transformer objects.             (\"rbf1\", RBFSampler(gamma=5.0, n_components=n_comp)),             (\"rbf2\", RBFSampler(gamma=2.0, n_components=n_comp)),             (\"rbf3\", RBFSampler(gamma=1.0, n_components=n_comp)),             (\"rbf4\", RBFSampler(gamma=0.5, n_components=n_comp))             ])     featurizer.fit(         scaler.transform(states)) # Perform standardization by centering and scaling  # state featurization of state(s) only, # not used on s_ for RDN's target &amp; predictor networks def featurize_state(state):     scaled = scaler.transform([state]) # Perform standardization by centering and scaling     featurized = featurizer.transform(scaled) # Transform X separately by each transformer, concatenate results.     return featurized[0]  def featurize_batch_state(batch_states):     fs_list = []     for s in batch_states:         fs = featurize_state(s)         fs_list.append(fs)     return fs_list   Preprocessing, next state normalization for RDN:   Variance is computed for the next states buffer_s_ using the RunningStats class.   During every training batch, the next states are normalize and clipped.   def state_next_normalize(sample_size, running_stats_s_):    buffer_s_ = []   s = env.reset()   for i in range(sample_size):     a = env.action_space.sample()     s_, r, done, _ = env.step(a)     buffer_s_.append(s_)    running_stats_s_.update(np.array(buffer_s_))   if state_next_normal == True:   state_next_normalize(sample_size, running_stats_s_)     Tensorboard graphs:   Big picture:   There are two main modules, the PPO and the RDN.   Current state, state is passed into PPO.   Next state, state_ is passed into RDN.        PPO module:   PPO module contains the actor network &amp; the critic network.        PPO’s actor:   At every iteration, an action is sampled from policy network pi. Entropy is NOT used in the computation of the loss in this program.      PPO’s critic:   The critic contains two value function networks. One for extrinsic rewards &amp; one for intrinsic rewards. Two sets of TD lambda returns &amp; advantages are also computed.   For extrinsic rewards: tdlamret adv   For intrinsic rewards: tdlamret_i adv_i   The TD lambda returns are used as the PPO’s critics targets in their respective networks while the advantages are summed &amp; used as the advantage in the actor’s loss computation.        RDN module:   RDN module contains the target network &amp; the predictor network.        RDN target network:   The target network is a fixed network, meaning that it’s never trained. It’s weights are randomized once during initialization. The target network is used to encode next states state_. It’s output are encoded next states.        RDN predictor network:   The predictor_loss is the intrinsic reward. It is the difference between the predictor network’s output with the target network’s output. The predictor network is trying to guess the target network’s encoded output.        Key to note:   All networks used in this program are linear.   The actor module is basically similar to this DPPO code documented in this post.   The difference is in the critic module. This implementation has two value functions in the critic module rather than one.   The predictor_loss is the intrinsic reward.        Problems to be resolved:   The actor’s network occasionally returns nan for action. This happens randomly. Random initialization of the network &amp; a smaller learning rate seems to help but does not resolve the issue completely.        Program output:   hit_counter 291 0.97   Number of steps per episode:      Reward per episode:      Moving average reward per episode:      — 385.09387278556824 seconds —     References:   Exploration by Random Network Distillation (Yuri Burda, Harrison Edwards, Amos Storkey, Oleg Klimov, 2018)    ","categories": [],
        "tags": [],
        "url": "https://chuacheowhuan.github.io/RDN/",
        "teaser":"https://chuacheowhuan.github.io/assets/images/foggy.jpg"}]
