var store = [{
        "title": "DQN",
        "excerpt":"A Deep Q Network implementation in tensorflow with target network &amp; random experience replay.   Environment from OpenAI’s gym: CartPole-v0   Full code        Notations:   Model network =    Model parameter =    Model network Q value =  (s, a)     Target network =    Target parameter =    Target network Q value =  (, )        Equations:   TD target = r (s, a)     (, )     TD  error = (TD target)  (Model network Q value)    = [r (s, a)     (, )]   (s, a)        Implementation details:   Update target parameter  with model parameter  :   Copy  to  with either soft or hard parameter update.     Hard parameter update:  with tf.variable_scope('hard_replace'):   self.target_replace_hard = [t.assign(m) for t, m in zip(self.target_net_params, self.model_net_params)]     # hard params replacement if self.learn_step % self.tau_step == 0:     self.sess.run(self.target_replace_hard)   self.learn_step += 1      Soft parameter update:   polyak    + (1  polyak)       with tf.variable_scope('soft_replace'):               self.target_replace_soft = [t.assign(self.polyak * m + (1 - self.polyak) * t)                               for t, m in zip(self.target_net_params, self.model_net_params)]        Stop TD target from contributing to gradient computation:  # exclude td_target in gradient computation td_target = tf.stop_gradient(td_target)       References:   Human-level control through deep reinforcement learning (Mnih et al., 2015)    ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/DQN/",
        "teaser":"http://localhost:4000/assets/images/foggy.jpg"},{
        "title": "DDQN",
        "excerpt":"A Double Deep Q Network (DDQN) implementation in tensorflow with random experience replay.   Environment from OpenAI’s gym: CartPole-v0   Full code      Notations:   Model network =    Model parameter =    Model network Q value =  (s, a)     Target network =    Target parameter =    Target network Q value =  (, )      Equations:   TD target = r (s, a)    (,   (s, a))     TD  error = (TD target)  (Model network Q value)    = [r (s, a)    (,   (s, a))]   (s, a)      Implementation details:   Create a placeholder to feed Q values from model network:   self.model_s_next_Q_val = tf.placeholder(tf.float32, [None,self.num_actions], name='model_s_next_Q_val')   Select Q values from model network using  as features &amp; feed them to the training session:   # select actions from model network model_s_next_Q_val = self.sess.run(self.model_Q_val, feed_dict={self.s: s_next})  # training _, loss = self.sess.run([self.optimizer, self.loss],                         feed_dict = {self.s: s,                                      self.a: a,                                      self.r: r,                                      self.s_next: s_next,                                      self.done: done,                                      self.model_s_next_Q_val: model_s_next_Q_val})   Select minibatch actions with largest Q values from model network, create indices &amp; select corresponding minibatch actions from target network:   def td_target(self, s_next, r, done, model_s_next_Q_val, target_Q_val):     # select action with largest Q value from model network     model_max_a = tf.argmax(model_s_next_Q_val, axis=1, output_type=tf.dtypes.int32)      arr = tf.range(tf.shape(model_max_a)[0], dtype=tf.int32) # create row indices     indices = tf.stack([arr, model_max_a], axis=1) # create 2D indices             max_target_Q_val = tf.gather_nd(target_Q_val, indices) # select minibatch actions from target network     max_target_Q_val = tf.reshape(max_target_Q_val, (self.minibatch_size,1))      References:   Deep Reinforcement Learning with Double Q-learning (Hasselt, Guez &amp; Silver, 2016)      ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/DDQN/",
        "teaser":"http://localhost:4000/assets/images/foggy.jpg"},{
        "title": "Dueling DDQN",
        "excerpt":"A Dueling Double Deep Q Network (Dueling DDQN) implementation in tensorflow with random experience replay.   Environment from OpenAI’s gym: CartPole-v0   Full code      Notations:   Network =    Parameter =    Network Q value =  (s, a)     Value function = V(s)   Advantage function = A(s, a)     Parameter from the Advantage function layer =    Parameter from the Value function layer =       Equations:   (eqn 9) from the original paper (Wang et al., 2015):   Q(s, a; , , ) = V(s; , )     [ A(s, a; , )    A(s, ; , ) ]      Implementation details:   V represents the value function layer, A represents the Advantage function layer:  # contruct neural network def built_net(self, var_scope, w_init, b_init, features, num_hidden, num_output):                   with tf.variable_scope(var_scope):                 feature_layer = tf.contrib.layers.fully_connected(features, num_hidden,                                                         activation_fn = tf.nn.relu,                                                         weights_initializer = w_init,                                                         biases_initializer = b_init)       V = tf.contrib.layers.fully_connected(feature_layer, 1,                                             activation_fn = None,                                             weights_initializer = w_init,                                             biases_initializer = b_init)       A = tf.contrib.layers.fully_connected(feature_layer, num_output,                                             activation_fn = None,                                             weights_initializer = w_init,                                             biases_initializer = b_init)          Q_val = V + (A - tf.reduce_mean(A, reduction_indices=1, keepdims=True)) # refer to eqn 9 from the original paper               return Q_val         References:   Dueling Network Architectures for Deep Reinforcement Learning (Wang et al., 2015)      ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/Duel_DDQN/",
        "teaser":"http://localhost:4000/assets/images/foggy.jpg"},{
        "title": "Dueling DDQN with PER",
        "excerpt":"A Dueling Double Deep Q Network with Priority Experience Replay (Duel DDQN with PER) implementation in tensorflow.   Environment from OpenAI’s gym: CartPole-v0   Full code      Notations:   Model network =    Model parameter =    Model network Q value =  (s, a)     Target network =    Target parameter =    Target network Q value =  (, )     A small constant to ensure that no sample has 0 probability to be selected = e   Hyper parameter  =      Decides how to sample, range from 0 to 1, where 0 corresponds to fully uniformly random sample selection &amp; 1 corresponding to selecting samples based on highest priority.    Hyper parameter  =      Starts close to 0, gradually annealed  to 1, slowly giving more importance to weights during training.    Minibatch size = k   Replay memory size = N      Equations:   TD target = r (s, a)    (,   (s, a))     TD  error =     = (TD target)  (Model network Q value)    = [r (s, a)    (,   (s, a))]   (s, a)        =     =   e     probability(i) = P(i)    =      weights =  = (N  P(i))       Implementation details:   Sum tree:      Assume an example of a sum tree with 7 nodes (with 4 leaves which corresponds to the replay memory size):            At initialization:                                 When item 1 is added:                                 When item 2 is added:                                 When item 3 is added:                                 When item 4 is added:                                 When item 5 is added:                            Figure below shows the corresponding code &amp; array contents. The tree represents the entire sum tree while data represents the leaves.                   In the implementation, only one sumTree object is needed to store the collected experiences, this sumTree object resides in the Replay_memory class. The sumTree object has number of leaves = replay memory size = capacity. The data array in sumTree object stores an Exp object, which is a sample of experience.      The following code decides how to sample:  def sample(self, k): # k = minibatch size     batch = []      # total_p() gives the total sum of priorities of the leaves in the sumTree     # which is the value stored in the root node     segment = self.tree.total_p() / k      for i in range(k):         a = segment * i # start of segment         b = segment * (i + 1) # end of segment         s = np.random.uniform(a, b) # rand value between a, b          (idx, p, data) = self.tree.get(s)         batch.append( (idx, p, data) )                  return batch       Refer to appendix B.2.1, under the section, “Proportional prioritization”, from the original (Schaul et al., 2016) paper for sampling details.      References:   Prioritized experience replay (Schaul et al., 2016)      ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/Duel_DDQN_with_PER/",
        "teaser":"http://localhost:4000/assets/images/foggy.jpg"},{
        "title": "Numpy array manipulation",
        "excerpt":"Simple numpy array manipulation examples   Full code   This Jupyter notebook contains simple examples on how to manipulate numpy arrays. The code block below shows the codes &amp; it’s corresponding display output.   Setting up a numpy array:  buffer=[0,1] print('buffer=', buffer) $buffer= [0, 1]  new=2 print('new=', new) $new= 2  buffer = np.array(buffer + [new]) # append a new item &amp; create a numpy array print('np.array(buffer + [new])=', buffer) $np.array(buffer + [new])= [0 1 2]   Slicing examples:  # numpy array slicing syntax # buffer[start:stop:step]  print('buffer[1:]=', buffer[1:]) # starting from index 1 $buffer[1:]= [1 2]  print('buffer[-1:]=', buffer[-1:]) # getting item in last index $buffer[-1:]= [2]  print('buffer[:1]=', buffer[:1]) # stop at index 1 (exclusive), keep only 1st item $buffer[:1]= [0]  print('buffer[:-1]=', buffer[:-1]) # stop at last index (exclusive), discard item in last index $buffer[:-1]= [0 1]  print('buffer[::-1]=', buffer[::-1]) # start from last index (reversal) $buffer[::-1]= [2 1 0]  print('buffer[1::-1]=', buffer[1::-1]) # reverse starting from index 1 $buffer[1::-1]= [1 0]  # Starting from index 1 will return [1 2], reversing will return [2,1] print('buffer[1:][::-1]=', buffer[1:][::-1]) $buffer[1:][::-1]= [2 1]   np.newaxis is an alias for None:  # np.newaxis = None  print('buffer[:, np.newaxis]=', buffer[:, np.newaxis]) $buffer[:, np.newaxis]= [[0][1][2]]  print('buffer[:, None]=', buffer[:, None]) $buffer[:, None]= [[0][1][2]]  print('buffer[np.newaxis, :]=', buffer[np.newaxis, :]) $buffer[np.newaxis, :]= [[0 1 2]]  print('buffer[None, :]=', buffer[None, :]) $buffer[None, :]= [[0 1 2]]   Stacking:  a = [1,2,3] b = [4,5,6] c = [7,8,9]  r = np.hstack((a,b,c)) # horizontal stacking print(\"r=\", r) $r= [1 2 3 4 5 6 7 8 9]  QUEUE = queue.Queue() QUEUE.put(a) QUEUE.put(b) QUEUE.put(c)  r = [QUEUE.get() for _ in range(QUEUE.qsize())] print(r) $[[1, 2, 3], [4, 5, 6], [7, 8, 9]]  r = np.vstack(r) # vertical stacking print(r) $[[1 2 3]   [4 5 6]   [7 8 9]]  print(r[:, ::-1]) # col reversal $[[3 2 1]   [6 5 4]   [9 8 7]]  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/np_array_manipulation/",
        "teaser":"http://localhost:4000/assets/images/foggy.jpg"},{
        "title": "Python's multiprocessing package",
        "excerpt":"Python’s multiprocessing package for parallel data generation.   Full code   In the context of reinforcement learning algorithms such as A3C or DPPO, data generation is done in parallel. This simple example program demonstrates how to use the Python’s multiprocessing package to achieve parallel data generation.   The main program has a chief that spawns multiple worker processes. Each worker spawns a single work process. The work process generates random integer data [1,3].   Each worker has it’s own local queue. When data is generated, it is stored in it’s local queue. When the local queue’s size is greater than 5, the data is retrieved &amp; 0.1 is added to the data, this result is stored in the Chief’s global queue. When the Chief’s global queue’s size is greater than 3, the result is retrieved &amp; printed on screen.   The Worker class:  class Worker(object):   def __init__(self, worker_id, g_queue):     self.g_queue = g_queue     self.worker_id = worker_id     self.queue = Queue() # local worker queue     self.work_process = Process(target=self.work, args=())     self.work_process.start()     info(worker_id, self.work_process, \"Worker\")    def work(self):      info(self.worker_id, self.work_process, \"work\")      while True:       data = np.random.randint(1,4)       self.queue.put(data)        # process data in queue       if self.queue.qsize() &gt; 5:         data = self.queue.get()         result = data + 0.1         self.g_queue.put(result) # send result to global queue        time.sleep(1) # work every x sec interval      return self.w_id     The Chief class:  class Chief(object):   def __init__(self, num_workers):     self.g_queue = Queue() # global queue         self.num_workers = num_workers    def dispatch_workers(self):        worker_processes = [Process(target=Worker(w_id, self.g_queue), args=()) for w_id in range(num_workers)]     return worker_processes    def result(self):     if self.g_queue.qsize() &gt; 3:       result = self.g_queue.get()       print(\"result\", result)   The main program:  if __name__ == '__main__':     print('main parent process id:', os.getppid())   print('main process id:', os.getpid())    num_workers = 2   chief = Chief(num_workers)   workers_processes = chief.dispatch_workers()    i = 0   while True:         time.sleep(2) # chk g_queue every x sec interval to get result     chief.result()     print(\"i=\", i)      if i&gt;9:       break     i+=1       A helper display function:  def info(worker_id, process, function_name):     print(\"worker_id=\", worker_id,           'module name:', __name__,           'function name:', function_name,           'parent process:', os.getppid(),           'current process id:', os.getpid(),           'spawn process id:', process.pid)  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/py_mpp/",
        "teaser":"http://localhost:4000/assets/images/foggy.jpg"},{
        "title": "N-step targets",
        "excerpt":"N-step Q-values estimation.   Full code   The following two functions computes truncated Q-values estimates:   1) n_step_targets_missing   treats missing terms as 0.   2) n_step_targets_max   use maximum terms possible.       1-step truncated estimate :    = E( +  V())   2-step truncated estimate :    = E( +   +   V())   3-step truncated estimate :    = E( +   +   +  V())   N-step truncated estimate :    = E( +   +   + … +  V())       Assuming we have the following variables setup:  N=2 # N steps gamma=2 t=5 v_s_ = 10 # value of next state  epr=np.arange(t).reshape(t,1) print(\"epr=\", epr)  baselines=np.arange(t).reshape(t,1) print(\"baselines=\", baselines)   Display output of episodic rewards(epr) &amp; baselines:  epr= [[0]  [1]  [2]  [3]  [4]]  baselines= [[0]  [1]  [2]  [3]  [4]]   This function computes the n-step targets, treats missing terms as zero:  # if number of steps unavailable, missing terms treated as 0. def n_step_targets_missing(epr, baselines, gamma, N):   N = N+1   targets = np.zeros_like(epr)       if N &gt; epr.size:     N = epr.size   for t in range(epr.size):        print(\"t=\", t)     for n in range(N):       print(\"n=\", n)       if t+n == epr.size:                     print('missing terms treated as 0, break') # last term for those with insufficient steps.         break # missing terms treated as 0       if n == N-1: # last term         targets[t] += (gamma**n) * baselines[t+n] # last term for those with sufficient steps         print('last term for those with sufficient steps, end inner n loop')       else:         targets[t] += (gamma**n) * epr[t+n] # non last terms   return targets   Run the function n_step_targets_missing:  print('n_step_targets_missing:') T = n_step_targets_missing(epr, baselines, gamma, N) print(T)   Display the output:  n_step_targets_missing: t= 0 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 1 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 2 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 3 n= 0 n= 1 n= 2 missing terms treated as 0, break t= 4 n= 0 n= 1 missing terms treated as 0, break [[10]  [17]  [24]  [11]  [ 4]]  For the output above, note that when t+n = 5 which is greater than the last index 4, missing terms are treated as 0.       This function computes the n-step targets, it will use maximum number of terms possible:  # if number of steps unavailable, use max steps available. # uses v_s_ as input def n_step_targets_max(epr, baselines, v_s_, gamma, N):   N = N+1   targets = np.zeros_like(epr)       if N &gt; epr.size:     N = epr.size   for t in range(epr.size):       print(\"t=\", t)     for n in range(N):       print(\"n=\", n)       if t+n == epr.size:                     targets[t] += (gamma**n) * v_s_ # last term for those with insufficient steps.         print('last term for those with INSUFFICIENT steps, break')         break       if n == N-1:         targets[t] += (gamma**n) * baselines[t+n] # last term for those with sufficient steps         print('last term for those with sufficient steps, end inner n loop')       else:         targets[t] += (gamma**n) * epr[t+n] # non last terms   return targets   Run the function n_step_targets_max:  print('n_step_targets_max:') T = n_step_targets_max(epr, baselines, v_s_, gamma, N) print(T)   Display the output:  n_step_targets_max: t= 0 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 1 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 2 n= 0 n= 1 n= 2 last term for those with sufficient steps, end inner n loop t= 3 n= 0 n= 1 n= 2 last term for those with INSUFFICIENT steps, break t= 4 n= 0 n= 1 last term for those with INSUFFICIENT steps, break [[10]  [17]  [24]  [51]  [24]]  For the output above, note that when t+n = 5 which is greater than the last index 4, maximum terms are used where possible. ( Last term for those with INSUFFICIENT steps is given by (gamma**n) * v_s_ =  V()), where v_s_ = V()   t=2, normal 2 steps estimation:    = E( +   +   V())   t=3, 2 steps estimation with insufficient step, using v_s_ as last term:    = E( +   +   V())   t=4, insufficient step for 2 steps estimation, resorting to 1 step estimation:    = E( +  V())  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/n_step_targets/",
        "teaser":"http://localhost:4000/assets/images/foggy.jpg"}]
