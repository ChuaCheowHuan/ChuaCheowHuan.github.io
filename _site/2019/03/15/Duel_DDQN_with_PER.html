<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Duel_ddqn_with_per</title>
    <link rel="stylesheet" href="/assets/css/styles.css">

    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async>
    </script>

  </head>
  <body>
    <nav>
  
    <a href="/" >Home</a>
  
    <a href="/about.html" >About</a>
  
    <a href="/blog.html" >Blog</a>
  
</nav>

    <!--
<h1>DUEL_DDQN_WITH_PER</h1>
-->
<p>
  15 Mar 2019
  
  
    <!--
      - <a href="/authors/ChuaCheowHuan.html">Chua Cheow Huan</a>
      -->
  
</p>

<p>Duel DDQN with PER</p>

<p>A <strong>Dueling Double DQN with Priority Experience Replay (Duel DDQN with PER)</strong> implementation in tensorflow.</p>

<p>Environment from openai gym: CartPole-v0</p>

<hr />
<p><br />
<strong>Notations</strong></p>

<p>Model network = <script type="math/tex">Q_{\theta}</script> <br />
Model parameter = <script type="math/tex">\theta</script> <br />
Model network Q value = <script type="math/tex">Q_{\theta}</script> (s, a) <br />
<br />
Target network = <script type="math/tex">Q_{\phi}</script> <br />
Target parameter = <script type="math/tex">\phi</script> <br />
Target network Q value = <script type="math/tex">Q_{\phi}</script> (<script type="math/tex">s^{'}</script>, <script type="math/tex">a^{'}</script>) <br />
<br />
A small constant to ensure no sample has 0 probability to be selected = e</p>

<p>Hyper parameter  = <script type="math/tex">\alpha</script></p>
<blockquote>
  <p>Decides how to sample, range from 0 to 1, where 0 corresponds to fully uniformly random sample selection &amp; 1 corresponding to selecting samples based on highest priority.</p>
</blockquote>

<p>Hyper parameter  = <script type="math/tex">\beta</script></p>
<blockquote>
  <p>Starts close to 0, gradually annealed  to 1, slowly giving more importance to weights during training.</p>
</blockquote>

<p>Minibatch size = k <br />
Replay memory size = N</p>

<hr />
<p><br />
<strong>Equations</strong></p>

<p>TD target = r (s, a) <script type="math/tex">+</script> <script type="math/tex">\gamma</script> <script type="math/tex">Q_{\phi}</script> (<script type="math/tex">s^{'}</script>, <script type="math/tex">argmax_{a^{'}}</script> <script type="math/tex">Q_{\theta}</script> (s<script type="math/tex">^{'}</script>, a<script type="math/tex">^{'}</script>)) <br />
<br />
TD  error = <script type="math/tex">{\delta}</script> <br />
<script type="math/tex">\hspace{26pt}</script>
= (TD target) <script type="math/tex">-</script> (Model network Q value) <br />
<script type="math/tex">\hspace{26pt}</script>
= [r (s, a) <script type="math/tex">+</script> <script type="math/tex">\gamma</script> <script type="math/tex">Q_{\phi}</script> (<script type="math/tex">s^{'}</script>, <script type="math/tex">argmax_{a^{'}}</script> <script type="math/tex">Q_{\theta}</script> (s<script type="math/tex">^{'}</script>, a<script type="math/tex">^{'}</script>))] <script type="math/tex">-</script> <script type="math/tex">Q_{\theta}</script> (s, a) <br />
<br /></p>

<p><script type="math/tex">priority_{i}</script> = <script type="math/tex">p_{i}</script> <br />
<script type="math/tex">\hspace{32pt}</script>
= <script type="math/tex">{|\delta_{i}|}</script> <script type="math/tex">+</script> e <br />
<br />
probability(i) = P(i) <br />
<script type="math/tex">\hspace{41pt}</script>
= <script type="math/tex">\frac{p_{i}^{\alpha}}  {\sum_{k}p_{k}^{\alpha}}</script> <br />
<br />
weights = <script type="math/tex">w_{i}</script> = (N <script type="math/tex">\cdot</script> P(i)) <script type="math/tex">^{-\beta}</script></p>

<hr />
<p><br />
<strong>Implementation details</strong></p>

<p>Sum tree:</p>

<blockquote>
  <p>Assume an example of a sum tree with 7 nodes (with 4 leaves which corresponds to the replay memory size):</p>
</blockquote>

<blockquote>
  <blockquote>
    <p>At initialization:</p>
    <blockquote>
      <p><img src="https://drive.google.com/uc?export=view&amp;id=1-quXFm1UnNnaThHxhaMoYl5RTAJnJUVI" alt="alt text" /></p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <p>When item 1 is added:</p>
    <blockquote>
      <p><img src="https://drive.google.com/uc?export=view&amp;id=1Jk-RO9Yqeq2DQKO1CKD9e_KQTxWgtMOu" alt="alt text" /></p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <p>When item 2 is added:</p>
    <blockquote>
      <p><img src="https://drive.google.com/uc?export=view&amp;id=1fTopGfDSeQj3uEKZPlo_2KSTWaBHrFfK" alt="alt text" /></p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <p>When item 3 is added:</p>
    <blockquote>
      <p><img src="https://drive.google.com/uc?export=view&amp;id=1d37aBtukIExVU7k84XjUPPphiFJlKXBZ" alt="alt text" /></p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <p>When item 4 is added:</p>
    <blockquote>
      <p><img src="https://drive.google.com/uc?export=view&amp;id=1V7B3vODsz2ELpW5--oQPh1vxmPMLYxOz" alt="alt text" /></p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <blockquote>
    <p>When item 5 is added:</p>
    <blockquote>
      <p><img src="https://drive.google.com/uc?export=view&amp;id=1KBPd61jU4nNug7b475gbKLe5sBJhC_l-" alt="alt text" /></p>
    </blockquote>
  </blockquote>
</blockquote>

<blockquote>
  <p>Figure below shows the corresponding code &amp; array contents. The tree represents the entire sum tree while data represents the leaves.</p>
  <blockquote>
    <p><img src="https://drive.google.com/uc?export=view&amp;id=1kk60DiIQOEkR03wakk2Qwyj2xcK7ac3k" alt="alt text" /></p>
  </blockquote>
</blockquote>

<blockquote>
  <p>In the implementation, only one sumTree object is needed to store the collected experiences, this sumTree object resides in the Replay_memory class. The sumTree object has number of leaves = replay memory size = capacity
The data array in sumTree object stores an object Exp, which is a sample of experience.</p>
</blockquote>

<p><br />
The following code decides how to sample:</p>
<blockquote>
  <p><img src="https://drive.google.com/uc?export=view&amp;id=1KlrlE3KANwmO56vtUdgAI6djfWcDfgWf" alt="alt text" /></p>
</blockquote>

<p>Refer to appendix B.2.1, under the section, “Proportional prioritization”, from the original (Schaul et al., 2016) <a href="https://arxiv.org/pdf/1511.05952.pdf">paper</a> for sampling details.</p>

<hr />
<p><br />
<strong>References</strong></p>

<p><a href="https://arxiv.org/pdf/1511.05952.pdf">Prioritized experience replay (Schaul et al., 2016)</a></p>


  </body>
</html>
