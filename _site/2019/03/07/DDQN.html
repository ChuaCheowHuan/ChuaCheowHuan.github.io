<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Ddqn</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
    <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" />
    <!-- Begin Jekyll SEO tag v2.6.0 -->
<title>Ddqn</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Ddqn" />
<meta name="author" content="Huan" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Double DQN (DDQN)" />
<meta property="og:description" content="Double DQN (DDQN)" />
<link rel="canonical" href="http://localhost:4000/2019/03/07/DDQN.html" />
<meta property="og:url" content="http://localhost:4000/2019/03/07/DDQN.html" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-03-07T00:00:00+08:00" />
<script type="application/ld+json">
{"url":"http://localhost:4000/2019/03/07/DDQN.html","headline":"Ddqn","dateModified":"2019-03-07T00:00:00+08:00","datePublished":"2019-03-07T00:00:00+08:00","author":{"@type":"Person","name":"Huan"},"description":"Double DQN (DDQN)","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2019/03/07/DDQN.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async>
    </script>

  </head>
  <body>
    <nav>
  
    <a href="/" >Home</a>
  
    <a href="/about.html" >About</a>
  
    <a href="/blog.html" >Blog</a>
  
</nav>

    <!--
<h1>DDQN</h1>
-->
<p>
  07 Mar 2019
  
  
    <!--
      - <a href="/authors/ChuaCheowHuan.html">Chua Cheow Huan</a>
      -->
  
</p>

<p>Double DQN (DDQN)</p>

<p>A <strong>Double DQN (DDQN)</strong> implementation in tensorflow with random experience replay.</p>

<p>Environment from openai gym: CartPole-v0</p>

<hr />
<p><br />
<strong>Notations</strong></p>

<p>Model network = <script type="math/tex">Q_{\theta}</script> <br />
Model parameter = <script type="math/tex">\theta</script> <br />
Model network Q value = <script type="math/tex">Q_{\theta}</script> (s, a) <br />
<br />
Target network = <script type="math/tex">Q_{\phi}</script> <br />
Target parameter = <script type="math/tex">\phi</script> <br />
Target network Q value = <script type="math/tex">Q_{\phi}</script> (<script type="math/tex">s^{'}</script>, <script type="math/tex">a^{'}</script>)</p>

<hr />
<p><br />
<strong>Equations</strong></p>

<p>TD target = r (s, a) <script type="math/tex">+</script> <script type="math/tex">\gamma</script> <script type="math/tex">Q_{\phi}</script> (<script type="math/tex">s^{'}</script>, <script type="math/tex">argmax_{a^{'}}</script> <script type="math/tex">Q_{\theta}</script> (s<script type="math/tex">^{'}</script>, a<script type="math/tex">^{'}</script>)) <br />
<br />
TD  error = (TD target) <script type="math/tex">-</script> (Model network Q value) <br />
<script type="math/tex">\hspace{26pt}</script>
= [r (s, a) <script type="math/tex">+</script> <script type="math/tex">\gamma</script> <script type="math/tex">Q_{\phi}</script> (<script type="math/tex">s^{'}</script>, <script type="math/tex">argmax_{a^{'}}</script> <script type="math/tex">Q_{\theta}</script> (s<script type="math/tex">^{'}</script>, a<script type="math/tex">^{'}</script>))] <script type="math/tex">-</script> <script type="math/tex">Q_{\theta}</script> (s, a)</p>

<hr />
<p><br />
<strong>Implementation details</strong></p>

<p>Create a placeholder to feed Q values from model network:</p>
<blockquote>
  <p><img src="https://drive.google.com/uc?export=view&amp;id=1CcZVw82JRQRWYmTFFN9PvLKjd4b5BOAF" alt="alt text" /></p>
</blockquote>

<p>Select Q values from model network using <script type="math/tex">s^{'}</script> as features &amp; feed them to the training session:</p>
<blockquote>
  <p><img src="https://drive.google.com/uc?export=view&amp;id=15uOc3uOz83V76X5s3PmgzzVWYJkkwR0Z" alt="alt text" /></p>
</blockquote>

<p>Select minibatch actions with largest Q values from model network, create indices &amp; select corresponding minibatch actions from target network:</p>
<blockquote>
  <p><img src="https://drive.google.com/uc?export=view&amp;id=1YelpKjS68nPBWtg8oeLiZV4mpzkmTPT_" alt="alt text" /></p>
</blockquote>

<hr />
<p><br />
<strong>References</strong></p>

<p><a href="https://arxiv.org/pdf/1509.06461.pdf">Deep Reinforcement Learning with Double Q-learning
(Hasselt, Guez &amp; Silver, 2016)</a></p>


  </body>
</html>
