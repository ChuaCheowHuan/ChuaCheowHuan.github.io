<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Ddqn</title>
    <link rel="stylesheet" href="/assets/css/styles.css">

    <script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async>
    </script>

  </head>
  <body>
    <nav>
  
    <a href="/" >Home</a>
  
    <a href="/about.html" >About</a>
  
    <a href="/blog.html" >Blog</a>
  
</nav>

    <!--
<h1>DDQN</h1>
-->
<p>
  07 Mar 2019
  
  
    <!--
      - <a href="/authors/ChuaCheowHuan.html">Chua Cheow Huan</a>
      -->
  
</p>

<p>Double DQN (DDQN)</p>

<p>A <strong>Double DQN (DDQN)</strong> implementation in tensorflow with random experience replay.</p>

<p>Environment from openai gym: CartPole-v0</p>

<hr />
<p><br />
<strong>Notations</strong></p>

<p>Model network = <script type="math/tex">Q_{\theta}</script> <br />
Model parameter = <script type="math/tex">\theta</script> <br />
Model network Q value = <script type="math/tex">Q_{\theta}</script> (s, a) <br />
<br />
Target network = <script type="math/tex">Q_{\phi}</script> <br />
Target parameter = <script type="math/tex">\phi</script> <br />
Target network Q value = <script type="math/tex">Q_{\phi}</script> (<script type="math/tex">s^{'}</script>, <script type="math/tex">a^{'}</script>)</p>

<hr />
<p><br />
<strong>Equations</strong></p>

<p>TD target = r (s, a) <script type="math/tex">+</script> <script type="math/tex">\gamma</script> <script type="math/tex">Q_{\phi}</script> (<script type="math/tex">s^{'}</script>, <script type="math/tex">argmax_{a^{'}}</script> <script type="math/tex">Q_{\theta}</script> (s<script type="math/tex">^{'}</script>, a<script type="math/tex">^{'}</script>)) <br />
<br />
TD  error = (TD target) <script type="math/tex">-</script> (Model network Q value) <br />
<script type="math/tex">\hspace{26pt}</script>
= [r (s, a) <script type="math/tex">+</script> <script type="math/tex">\gamma</script> <script type="math/tex">Q_{\phi}</script> (<script type="math/tex">s^{'}</script>, <script type="math/tex">argmax_{a^{'}}</script> <script type="math/tex">Q_{\theta}</script> (s<script type="math/tex">^{'}</script>, a<script type="math/tex">^{'}</script>))] <script type="math/tex">-</script> <script type="math/tex">Q_{\theta}</script> (s, a)</p>

<hr />
<p><br />
<strong>Implementation details</strong></p>

<p>Create a placeholder to feed Q values from model network:</p>
<blockquote>
  <p><img src="https://drive.google.com/uc?export=view&amp;id=1CcZVw82JRQRWYmTFFN9PvLKjd4b5BOAF" alt="alt text" /></p>
</blockquote>

<p>Select Q values from model network using <script type="math/tex">s^{'}</script> as features &amp; feed them to the training session:</p>
<blockquote>
  <p><img src="https://drive.google.com/uc?export=view&amp;id=15uOc3uOz83V76X5s3PmgzzVWYJkkwR0Z" alt="alt text" /></p>
</blockquote>

<p>Select minibatch actions with largest Q values from model network, create indices &amp; select corresponding minibatch actions from target network:</p>
<blockquote>
  <p><img src="https://drive.google.com/uc?export=view&amp;id=1YelpKjS68nPBWtg8oeLiZV4mpzkmTPT_" alt="alt text" /></p>
</blockquote>

<hr />
<p><br />
<strong>References</strong></p>

<p><a href="https://arxiv.org/pdf/1509.06461.pdf">Deep Reinforcement Learning with Double Q-learning
(Hasselt, Guez &amp; Silver, 2016)</a></p>


  </body>
</html>
