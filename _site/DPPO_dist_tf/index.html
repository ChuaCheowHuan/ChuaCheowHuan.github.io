<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.15.2 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">



<link rel="stylesheet" href="/assets/css/main.css">
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async>
</script>



  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>DPPO distributed tensorflow -</title>
<meta name="description" content="Distributed Proximal Policy Optimization (Distributed PPO or DPPO) continuousversion implementation with distributed Tensorflow and Python’s multiprocessingpackage. This implementation uses normalized running rewards with GAE. The codeis tested with Gym’s continuous action space environment, Pendulum-v0 on Colab.">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="">
<meta property="og:title" content="DPPO distributed tensorflow">
<meta property="og:url" content="https://chuacheowhuan.github.io/DPPO_dist_tf/">


  <meta property="og:description" content="Distributed Proximal Policy Optimization (Distributed PPO or DPPO) continuousversion implementation with distributed Tensorflow and Python’s multiprocessingpackage. This implementation uses normalized running rewards with GAE. The codeis tested with Gym’s continuous action space environment, Pendulum-v0 on Colab.">



  <meta property="og:image" content="https://chuacheowhuan.github.io/assets/images/bio-photo.jpg">





  <meta property="article:published_time" content="2019-06-25T00:00:00+08:00">





  

  


<link rel="canonical" href="https://chuacheowhuan.github.io/DPPO_dist_tf/">





  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Organization",
      "url": "https://chuacheowhuan.github.io",
      "logo": "https://chuacheowhuan.github.io/assets/images/bio-photo.jpg"
    }
  </script>



  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "name": "Chua Cheow Huan",
      "url": "https://chuacheowhuan.github.io",
      "sameAs": null
    }
  </script>



  <meta name="google-site-verification" content="googlec75336ce8806a8d5.html" />





<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title=" Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE ]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--posts">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/"></a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/" >Home</a>
            </li><li class="masthead__menu-item">
              <a href="/blog/index.html" >Blog</a>
            </li><li class="masthead__menu-item">
              <a href="/about/index.html" >About</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name"></h3>
    
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>

  <div class="archive">
    
      <h1 id="page-title" class="page__title">DPPO distributed tensorflow</h1>
    
    <p>Distributed Proximal Policy Optimization (Distributed PPO or DPPO) continuous
version implementation with distributed Tensorflow and Python’s multiprocessing
package. This implementation uses normalized running rewards with GAE. The code
is tested with Gym’s continuous action space environment, Pendulum-v0 on Colab.</p>

<p><a href="https://github.com/ChuaCheowHuan/reinforcement_learning/blob/master/DPPO/DPPO_cont_GAE_dist_GPU.ipynb">Full code</a>:</p>

<hr />

<h2 id="notations">Notations:</h2>

<p>current policy =
<script type="math/tex">{\pi}_{\theta}
(a_{t}
  {\mid} s_{t})</script></p>

<p>old policy =
<script type="math/tex">{\pi}_{\theta_{old}}
(a_{t}
  {\mid} s_{t})</script></p>

<p>epsilon =
<script type="math/tex">{\epsilon}</script></p>

<p>Advantage function = A</p>

<hr />

<h2 id="equations">Equations:</h2>

<p>Truncated version of generalized advantage estimation (GAE) =</p>

<p><script type="math/tex">A_{t}</script>
=
<script type="math/tex">{\delta}_{t}
+
({\gamma}
{\lambda})
{\delta}_{t}
+
...
+
({\gamma}
{\lambda})
^{T-t+1}
{\delta}_{T-1}</script></p>

<p>where
<script type="math/tex">{\delta}_{t}</script> =
<script type="math/tex">{r}_{t} +
{\gamma}
V(s_{t+1}) -
V(s_{t})</script></p>

<p>when <script type="math/tex">{\lambda}</script> = 1,</p>

<p><script type="math/tex">A_{t}</script> =
<script type="math/tex">-V(s_{t}) +
r_{t} +
{\gamma}r_{t+1} +
... +
{\gamma}^{T-t+1}
r_{T-1} +
{\gamma}^{T-t}
V(s_{T})</script></p>

<p>Probability ratio =</p>

<p><script type="math/tex">R_{t}({\theta})</script> = <script type="math/tex">{\dfrac{ {\pi}_{\theta} (a_{t} {\mid} s_{t}) } { {\pi}_{\theta_{old}} (a_{t} {\mid} s_{t}) } }</script></p>

<p>Clipped Surrogate Objective function =</p>

<p><script type="math/tex">L^{CLIP}
({\theta})</script>
=
<script type="math/tex">\mathop{\mathbb{E_{t}}}
\lbrack
min(
  R_{t}({\theta})
  A_{t}
  ,
  clip
  (
    R_{t}({\theta}),
    1+{\epsilon},
    1-{\epsilon}
    )
    A_{t}
  )
\rbrack</script></p>

<hr />

<h2 id="key-implementation-details">Key implementation details:</h2>

<p>The following class is adapted from OpenAI’s baseline:
This class is used for the normalization of rewards in this program before GAE
computation.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class RunningStats(object):
    def __init__(self, epsilon=1e-4, shape=()):
        self.mean = np.zeros(shape, 'float64')
        self.var = np.ones(shape, 'float64')
        self.std = np.ones(shape, 'float64')
        self.count = epsilon

    def update(self, x):
        batch_mean = np.mean(x, axis=0)
        batch_var = np.var(x, axis=0)
        batch_count = x.shape[0]
        self.update_from_moments(batch_mean, batch_var, batch_count)

    def update_from_moments(self, batch_mean, batch_var, batch_count):
        delta = batch_mean - self.mean
        new_mean = self.mean + delta * batch_count / (self.count + batch_count)
        m_a = self.var * self.count
        m_b = batch_var * batch_count
        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)
        new_var = M2 / (self.count + batch_count)

        self.mean = new_mean
        self.var = new_var
        self.std = np.maximum(np.sqrt(self.var), 1e-6)
        self.count = batch_count + self.count
</code></pre></div></div>

<p>This function in the <code class="highlighter-rouge">PPO</code> class is adapted from OpenAI’s Baseline,
returns TD lamda return &amp; advantage</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    def add_vtarg_and_adv(self, R, done, V, v_s_, gamma, lam):
        # Compute target value using TD(lambda) estimator, and advantage with GAE(lambda)
        # last element is only used for last vtarg, but we already zeroed it if last new = 1
        done = np.append(done, 0)
        V_plus = np.append(V, v_s_)
        T = len(R)
        adv = gaelam = np.empty(T, 'float32')
        lastgaelam = 0
        for t in reversed(range(T)):
            nonterminal = 1-done[t+1]        
            delta = R[t] + gamma * V_plus[t+1] * nonterminal - V_plus[t]
            gaelam[t] = lastgaelam = delta + gamma * lam * nonterminal * lastgaelam   
        #print("adv=", adv.shape)
        #print("V=", V.shape)
        #print("V_plus=", V_plus.shape)
        tdlamret = np.vstack(adv) + V
        #print("tdlamret=", tdlamret.shape)
        return tdlamret, adv # tdlamret is critic_target or Qs      
</code></pre></div></div>

<p>The following code segment from the <code class="highlighter-rouge">PPO</code> class defines the Clipped Surrogate
Objective function:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>with tf.variable_scope('surrogate'):
                    ratio = self.pi.prob(self.act) / self.oldpi.prob(self.act)
                    surr = ratio * self.adv
                    self.aloss = -tf.reduce_mean(tf.minimum(surr, tf.clip_by_value(ratio, 1.-epsilon, 1.+epsilon)*self.adv))
</code></pre></div></div>

<p>The following code segment from the <code class="highlighter-rouge">work()</code> function in the worker class
normalized the running rewards for each worker:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>self.running_stats_r.update(np.array(buffer_r))
                    buffer_r = np.clip( (np.array(buffer_r) - self.running_stats_r.mean) / self.running_stats_r.std, -stats_CLIP, stats_CLIP )
</code></pre></div></div>

<p>The following code segment from the <code class="highlighter-rouge">work()</code> function in the worker class computes
 the TD lamda return &amp; advantage:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tdlamret, adv = self.ppo.add_vtarg_and_adv(np.vstack(buffer_r), np.vstack(buffer_done), np.vstack(buffer_V), v_s_, GAMMA, lamda)

</code></pre></div></div>

<p>The following update function in the <code class="highlighter-rouge">PPO</code> class does the training &amp; the
updating of global &amp; local parameters (Note the at the beginning of training,
  probability ratio = 1):</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def update(self, s, a, r, adv):    
    self.sess.run(self.update_oldpi_op)

    for _ in range(A_EPOCH): # train actor
        self.sess.run(self.atrain_op, {self.state: s, self.act: a, self.adv: adv})
        # update actor
        self.sess.run([self.push_actor_pi_params,
                       self.pull_actor_pi_params],
                      {self.state: s, self.act: a, self.adv: adv})
    for _ in range(C_EPOCH): # train critic
        # update critic
        self.sess.run(self.ctrain_op, {self.state: s, self.discounted_r: r})
        self.sess.run([self.push_critic_params,
                       self.pull_critic_params],
                      {self.state: s, self.discounted_r: r})   
</code></pre></div></div>

<hr />

<p>The distributed Tensorflow &amp; multiprocessing code sections are very similar to
the ones describe in the following posts:</p>

<p><a href="https://chuacheowhuan.github.io/A3C_dist_tf/">A3C distributed tensorflow</a></p>

<p><a href="https://chuacheowhuan.github.io/dist_tf/">Distributed Tensorflow</a></p>

<hr />

<h2 id="references">References:</h2>

<p><a href="https://arxiv.org/pdf/1707.06347.pdf">Proximal Policy Optimization Algorithms</a>
(Schulman, Wolski, Dhariwal, Radford, Klimov, 2017)</p>

<p><a href="https://arxiv.org/pdf/1707.02286.pdf">Emergence of Locomotion Behaviours in Rich Environments</a>
(Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, et al., 2017)</p>

<hr />

<p><br /></p>


<ul class="taxonomy__index">
  
  
    <li>
      <a href="#2019">
        <strong>2019</strong> <span class="taxonomy__count">16</span>
      </a>
    </li>
  
</ul>



  <section id="2019" class="taxonomy__section">
    <h2 class="archive__subtitle">2019</h2>
    <div class="entries-list">
      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/tf_graph/" rel="permalink">Tensorflow graphs in Tensorboard
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  8 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">You need to use Tensorboard which comes will Tensorflow installed.

</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/bash_script/" rel="permalink">.bash_profile for Mac
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  13 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Edit .bash_profile for Mac.

</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/RND/" rel="permalink">RND (Random Network Distillation) with Proximal Policy Optimization (PPO) Tensorflow
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  83 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Random Network Distillation (RND) with Proximal Policy Optimization (PPO)
implentation in Tensorflow. This is a continuous version which solves the
mountain ...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/DPPO_dist_tf/" rel="permalink">DPPO distributed tensorflow
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  57 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Distributed Proximal Policy Optimization (Distributed PPO or DPPO) continuous
version implementation with distributed Tensorflow and Python’s multiprocessing...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/A3C_dist_tf/" rel="permalink">A3C distributed tensorflow
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  22 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">A3C (Asynchronous Advantage Actor Critic) implementation with distributed
Tensorflow &amp; Python multiprocessing package. This is a discrete
version with N-...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/A3C_cont_thread_nStep/" rel="permalink">A3C multi-threaded continuous version with N step targets
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  28 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">An A3C (Asynchronous Advantage Actor Critic) implementation with
Tensorflow. This is a multi-threaded continuous version. The code is tested with
Gym’s conti...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/A3C_disc_thread_nStep/" rel="permalink">A3C multi-threaded discrete version with N step targets
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  60 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">An A3C (Asynchronous Advantage Actor Critic) implementation with
Tensorflow. This is a multi-threaded discrete version. The code is tested with
Gym’s discret...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/tf_accumulate_grad/" rel="permalink">Accumulate gradients with Tensorflow
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  14 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">This post demonstrates how to accumulate gradients with Tensorflow.

</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/dist_tf/" rel="permalink">Distributed Tensorflow
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  73 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Distributed Tensorflow with Python multiprocessing package. This post
demonstrates how to use distributed Tensorflow with Python’s
multiprocessing package. A...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/n_step_targets/" rel="permalink">N-step targets
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  69 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">N-step Q-values estimation.

</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/py_mpp/" rel="permalink">Python’s multiprocessing package
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  30 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Python’s multiprocessing package for parallel data generation.

</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/np_array_manipulation/" rel="permalink">Numpy array manipulation
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  29 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Simple numpy array manipulation examples.

</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/Duel_DDQN_with_PER/" rel="permalink">Dueling DDQN with PER
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  42 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">A Dueling Double Deep Q Network with Priority Experience Replay (Duel DDQN with PER)
implementation in tensorflow. The code is tested with Gym’s discrete act...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/Duel_DDQN/" rel="permalink">Dueling DDQN
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  19 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">A Dueling Double Deep Q Network (Dueling DDQN) implementation in tensorflow
with random experience replay. The code is tested with Gym’s discrete action
spac...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/DDQN/" rel="permalink">DDQN
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  23 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">A Double Deep Q Network (DDQN) implementation in tensorflow with random experience replay.
The code is tested with Gym’s discrete action space environment, C...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/DQN/" rel="permalink">DQN
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  18 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">A Deep Q Network implementation in tensorflow with target network &amp; random
experience replay. The code is tested with Gym’s discrete action space
environ...</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>


  </div>
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><input type="text" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
    <div id="results" class="results"></div></div>

      </div>
    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
          <li><a href="https://github.com/ChuaCheowHuan" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2019 Chua Cheow Huan. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script defer src="https://use.fontawesome.com/releases/v5.7.1/js/all.js" integrity="sha384-eVEQC9zshBn0rFj4+TU78eNA19HMNigMviK/PU/FFjLXqa/GKPgX58rvt5Z8PLs7" crossorigin="anonymous"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    
  <script>
    var disqus_config = function () {
      this.page.url = "https://chuacheowhuan.github.io/DPPO_dist_tf/";  // Replace PAGE_URL with your page's canonical URL variable
      this.page.identifier = "/DPPO_dist_tf"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    (function() { // DON'T EDIT BELOW THIS LINE
      var d = document, s = d.createElement('script');
      s.src = 'https://https-chuacheowhuan-github-io.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>




<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://https-chuacheowhuan-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

  


  </body>
</html>
