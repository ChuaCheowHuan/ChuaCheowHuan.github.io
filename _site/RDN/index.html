<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.15.2 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">



<link rel="stylesheet" href="/assets/css/main.css">
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async>
</script>



  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>RDN (Random Distillation Network) with Proximal Policy Optimization (PPO) Tensorflow -</title>
<meta name="description" content="Random Distillation Network (RDN) with Proximal Policy Optimization (PPO)implentation in Tensorflow. This is a continuous version which solves themountain car continuous problem (MountainCarContinuous-v0).The RDN helps learning with curiosity driven exploration.">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="">
<meta property="og:title" content="RDN (Random Distillation Network) with Proximal Policy Optimization (PPO) Tensorflow">
<meta property="og:url" content="https://chuacheowhuan.github.io/RDN/">


  <meta property="og:description" content="Random Distillation Network (RDN) with Proximal Policy Optimization (PPO)implentation in Tensorflow. This is a continuous version which solves themountain car continuous problem (MountainCarContinuous-v0).The RDN helps learning with curiosity driven exploration.">



  <meta property="og:image" content="https://chuacheowhuan.github.io/assets/images/bio-photo.jpg">





  <meta property="article:published_time" content="2019-06-25T00:00:00+08:00">





  

  


<link rel="canonical" href="https://chuacheowhuan.github.io/RDN/">





  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Organization",
      "url": "https://chuacheowhuan.github.io",
      "logo": "https://chuacheowhuan.github.io/assets/images/bio-photo.jpg"
    }
  </script>



  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "name": "Chua Cheow Huan",
      "url": "https://chuacheowhuan.github.io",
      "sameAs": null
    }
  </script>



  <meta name="google-site-verification" content="googlec75336ce8806a8d5.html" />





<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title=" Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE ]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--posts">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/"></a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/" >Home</a>
            </li><li class="masthead__menu-item">
              <a href="/blog/index.html" >Blog</a>
            </li><li class="masthead__menu-item">
              <a href="/about/index.html" >About</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <svg class="icon" width="16" height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 15.99 16">
            <path d="M15.5,13.12L13.19,10.8a1.69,1.69,0,0,0-1.28-.55l-0.06-.06A6.5,6.5,0,0,0,5.77,0,6.5,6.5,0,0,0,2.46,11.59a6.47,6.47,0,0,0,7.74.26l0.05,0.05a1.65,1.65,0,0,0,.5,1.24l2.38,2.38A1.68,1.68,0,0,0,15.5,13.12ZM6.4,2A4.41,4.41,0,1,1,2,6.4,4.43,4.43,0,0,1,6.4,2Z" transform="translate(-.01)"></path>
          </svg>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name"></h3>
    
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>

  <div class="archive">
    
      <h1 id="page-title" class="page__title">RDN (Random Distillation Network) with Proximal Policy Optimization (PPO) Tensorflow</h1>
    
    <p>Random Distillation Network (RDN) with Proximal Policy Optimization (PPO)
implentation in Tensorflow. This is a continuous version which solves the
mountain car continuous problem (MountainCarContinuous-v0).
The RDN helps learning with curiosity driven exploration.</p>

<p>The agent starts to converge correctly at around 30 episodes &amp; reached the flag
291 times out of 300 episodes (97% hit rate). It takes 385.09387278556824
seconds to complete 300 episodes on Google’s Colab.</p>

<p>Edit: A new version which corrects a numerical error(causes nan action) takes
780.2065596580505 seconds for 300 episodes. Both versions have similar results.
The URL for the new version is updated.</p>

<p>Checkout the <a href="https://chuacheowhuan.github.io/RDN/#charts">resulting
charts</a> from the program output.</p>

<p>Full code:</p>

<ul>
  <li>
    <p><a href="https://github.com/ChuaCheowHuan/reinforcement_learning/blob/master/RDN_PPO/RDN_PPO_cont_ftr_nsn_mtcar_php.py">Python file</a>,</p>
  </li>
  <li>
    <p><a href="https://github.com/ChuaCheowHuan/reinforcement_learning/blob/master/RDN_PPO/RDN_PPO_cont_ftr_nsn_mtCar_php.ipynb">Jupyter notebook</a>
(The Jupyter notebook, which also contain the resulting charts at the end, can be run directly on Google’s Colab.)</p>
  </li>
</ul>

<hr />

<h2 id="notations--equations">Notations &amp; equations</h2>

<p>fixed feature from target network =
<script type="math/tex">{ f (s_{t+1}) }</script></p>

<p>predicted feature from predictor network =
<script type="math/tex">{ f ^\prime  (s_{t+1}) }</script></p>

<p>intrinsic reward =
<script type="math/tex">r_{i}</script> =
||
<script type="math/tex">{ f ^\prime  (s_{t+1}) }</script> -
<script type="math/tex">{ f (s_{t+1}) }</script>
||
<script type="math/tex">{}{^2}</script></p>

<p>For notations &amp; equations regarding PPO, refer to this
<a href="https://chuacheowhuan.github.io/DPPO_dist_tf/">post</a>.</p>

<hr />

<h2 id="key-implementation-details">Key implementation details:</h2>

<p><strong>Preprocessing, state featurization:</strong></p>

<p>Prior to training, the states are featurized with the RBF kernel.</p>

<p>(states are also featurized during every training batch.)</p>

<p>Refer to scikit-learn.org documentation: <a href="https://scikit-learn.org/stable/modules/kernel_approximation.html#rbf-kernel-approx">5.7.2. Radial Basis Function Kernel</a> for more information on RBF kernel.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>if state_ftr == True:
"""
The following code for state featurization is adapted &amp; modified from dennybritz's repository located at:
https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb
"""
    # Feature Preprocessing: Normalize to zero mean and unit variance
    # We use a few samples from the observation space to do this
    states = np.array([env.observation_space.sample() for x in range(sample_size)]) # pre-trained, states preprocessing
    scaler = sklearn.preprocessing.StandardScaler()
    scaler.fit(states) # Compute the mean and std to be used for later scaling.

    # convert states to a featurizes representation.
    # We use RBF kernels with different variances to cover different parts of the space
    featurizer = sklearn.pipeline.FeatureUnion([ # Concatenates results of multiple transformer objects.
            ("rbf1", RBFSampler(gamma=5.0, n_components=n_comp)),
            ("rbf2", RBFSampler(gamma=2.0, n_components=n_comp)),
            ("rbf3", RBFSampler(gamma=1.0, n_components=n_comp)),
            ("rbf4", RBFSampler(gamma=0.5, n_components=n_comp))
            ])
    featurizer.fit(
        scaler.transform(states)) # Perform standardization by centering and scaling

# state featurization of state(s) only,
# not used on s_ for RDN's target &amp; predictor networks
def featurize_state(state):
    scaled = scaler.transform([state]) # Perform standardization by centering and scaling
    featurized = featurizer.transform(scaled) # Transform X separately by each transformer, concatenate results.
    return featurized[0]

def featurize_batch_state(batch_states):
    fs_list = []
    for s in batch_states:
        fs = featurize_state(s)
        fs_list.append(fs)
    return fs_list
</code></pre></div></div>

<p><strong>Preprocessing, next state normalization for RDN:</strong></p>

<p>Variance is computed for the next states <code class="highlighter-rouge">buffer_s_</code> using
the <code class="highlighter-rouge">RunningStats</code> class. During every training batch, the next states are
normalize and clipped.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def state_next_normalize(sample_size, running_stats_s_):

  buffer_s_ = []
  s = env.reset()
  for i in range(sample_size):
    a = env.action_space.sample()
    s_, r, done, _ = env.step(a)
    buffer_s_.append(s_)

  running_stats_s_.update(np.array(buffer_s_))
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>if state_next_normal == True:
  state_next_normalize(sample_size, running_stats_s_)
</code></pre></div></div>

<hr />

<h2 id="tensorboard-graphs">Tensorboard graphs:</h2>

<p><strong>Big picture:</strong></p>

<p>There are two main modules, the PPO and the RDN.</p>

<p>Current state, <code class="highlighter-rouge">state</code> is passed into PPO.</p>

<p>Next state, <code class="highlighter-rouge">state_</code> is passed into RDN.</p>

<p><img src="/assets/images/RDN_PPO_tf_graph_img/main.png" alt="image" /></p>

<hr />

<p><strong>PPO module:</strong></p>

<p>PPO module contains the actor network &amp; the critic network.</p>

<p><img src="/assets/images/RDN_PPO_tf_graph_img/PPO.png" alt="image" /></p>

<hr />

<p><strong>PPO’s actor:</strong></p>

<p>At every iteration, an action is sampled from policy network <code class="highlighter-rouge">pi</code>.
<img src="/assets/images/RDN_PPO_tf_graph_img/PPO_a.png" alt="image" /></p>

<hr />

<p><strong>PPO’s critic:</strong></p>

<p>The critic contains two value function networks. One for extrinsic rewards &amp; one
 for intrinsic rewards. Two sets of TD lambda returns &amp; advantages are also
 computed.</p>

<p>For extrinsic rewards: <code class="highlighter-rouge">tdlamret adv</code></p>

<p>For intrinsic rewards: <code class="highlighter-rouge">tdlamret_i adv_i</code></p>

<p>The TD lambda returns are used as the PPO’s critics targets in their respective
networks while the advantages are summed &amp; used as the advantage in the actor’s
loss computation.</p>

<p><img src="/assets/images/RDN_PPO_tf_graph_img/PPO_c.png" alt="image" /></p>

<hr />

<p><strong>RDN module:</strong></p>

<p>RDN module contains the target network &amp; the predictor network.</p>

<p><img src="/assets/images/RDN_PPO_tf_graph_img/RDN.png" alt="image" /></p>

<hr />

<p><strong>RDN target network:</strong></p>

<p>The target network is a fixed network, meaning that it’s never trained.
It’s weights are randomized once during initialization. The target network is
used to encode next states <code class="highlighter-rouge">state_</code>. It’s output are encoded next states.</p>

<p><img src="/assets/images/RDN_PPO_tf_graph_img/RDN_t.png" alt="image" /></p>

<hr />

<p><strong>RDN predictor network:</strong></p>

<p>The <code class="highlighter-rouge">predictor_loss</code> is the intrinsic reward. It is the difference between
the predictor network’s output with the target network’s output. The predictor
network is trying to guess the target network’s encoded output.</p>

<p><img src="/assets/images/RDN_PPO_tf_graph_img/RDN_p.png" alt="image" /></p>

<hr />

<h2 id="key-to-note">Key to note:</h2>

<p>All networks used in this program are linear.</p>

<p>The actor module is basically similar to this DPPO <a href="https://github.com/ChuaCheowHuan/reinforcement_learning/blob/master/DPPO/DPPO_cont_GAE_dist_GPU.ipynb">code</a> documented in this <a href="https://chuacheowhuan.github.io/DPPO_dist_tf/">post</a>.</p>

<p>The difference is in the critic module. This implementation has two value
functions in the critic module rather than one.</p>

<p>The <code class="highlighter-rouge">predictor_loss</code> is the intrinsic reward.</p>

<p><img src="/assets/images/RDN_PPO_tf_graph_img/key.png" alt="image" /></p>

<hr />

<h2 id="problems-encountered">Problems encountered:</h2>

<p>The actor’s network occasionally returns ‘'’nan’’’ for action. This happens randomly, most likely caused by exploding gradients.
Not initializing or randomly initializing actor’s weights results in nan when outputting action.</p>

<hr />

<p><a name="charts"></a></p>

<h2 id="program-output">Program output:</h2>

<p>hit_counter 291 0.97</p>

<p>Number of steps per episode:</p>

<p><img src="/assets/images/RDN_PPO_tf_graph_img/output/steps.png" alt="image" /></p>

<p>Reward per episode:</p>

<p><img src="/assets/images/RDN_PPO_tf_graph_img/output/reward.png" alt="image" /></p>

<p>Moving average reward per episode:</p>

<p><img src="/assets/images/RDN_PPO_tf_graph_img/output/mv_avg.png" alt="image" /></p>

<p>— 385.09387278556824 seconds —</p>

<hr />

<h2 id="references">References:</h2>

<p><a href="https://arxiv.org/pdf/1810.12894.pdf">Exploration by Random Network Distillation</a>
(Yuri Burda, Harrison Edwards, Amos Storkey, Oleg Klimov, 2018)</p>

<hr />

<p><br /></p>


<ul class="taxonomy__index">
  
  
    <li>
      <a href="#2019">
        <strong>2019</strong> <span class="taxonomy__count">16</span>
      </a>
    </li>
  
</ul>



  <section id="2019" class="taxonomy__section">
    <h2 class="archive__subtitle">2019</h2>
    <div class="entries-list">
      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/tf_graph/" rel="permalink">Tensorflow graphs in Tensorboard
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  8 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">You need to use Tensorboard which comes will Tensorflow installed.

</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/bash_script/" rel="permalink">.bash_profile for Mac
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  13 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Edit .bash_profile for Mac.

</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/RDN/" rel="permalink">RDN (Random Distillation Network) with Proximal Policy Optimization (PPO) Tensorflow
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  77 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Random Distillation Network (RDN) with Proximal Policy Optimization (PPO)
implentation in Tensorflow. This is a continuous version which solves the
mountain ...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/DPPO_dist_tf/" rel="permalink">DPPO distributed tensorflow
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  57 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Distributed Proximal Policy Optimization (Distributed PPO or DPPO) continuous
version implementation with distributed Tensorflow and Python’s multiprocessing...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/A3C_dist_tf/" rel="permalink">A3C distributed tensorflow
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  22 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">A3C (Asynchronous Advantage Actor Critic) implementation with distributed
Tensorflow &amp; Python multiprocessing package. This is a discrete
version with N-...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/A3C_cont_thread_nStep/" rel="permalink">A3C multi-threaded continuous version with N step targets
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  28 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">An A3C (Asynchronous Advantage Actor Critic) implementation with
Tensorflow. This is a multi-threaded continuous version. The code is tested with
Gym’s conti...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/A3C_disc_thread_nStep/" rel="permalink">A3C multi-threaded discrete version with N step targets
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  60 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">An A3C (Asynchronous Advantage Actor Critic) implementation with
Tensorflow. This is a multi-threaded discrete version. The code is tested with
Gym’s discret...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/tf_accumulate_grad/" rel="permalink">Accumulate gradients with Tensorflow
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  14 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">This post demonstrates how to accumulate gradients with Tensorflow.

</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/dist_tf/" rel="permalink">Distributed Tensorflow
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  73 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Distributed Tensorflow with Python multiprocessing package. This post
demonstrates how to use distributed Tensorflow with Python’s
multiprocessing package. A...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/n_step_targets/" rel="permalink">N-step targets
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  69 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">N-step Q-values estimation.

</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/py_mpp/" rel="permalink">Python’s multiprocessing package
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  30 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Python’s multiprocessing package for parallel data generation.

</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/np_array_manipulation/" rel="permalink">Numpy array manipulation
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  29 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">Simple numpy array manipulation examples.

</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/Duel_DDQN_with_PER/" rel="permalink">Dueling DDQN with PER
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  42 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">A Dueling Double Deep Q Network with Priority Experience Replay (Duel DDQN with PER)
implementation in tensorflow. The code is tested with Gym’s discrete act...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/Duel_DDQN/" rel="permalink">Dueling DDQN
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  19 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">A Dueling Double Deep Q Network (Dueling DDQN) implementation in tensorflow
with random experience replay. The code is tested with Gym’s discrete action
spac...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/DDQN/" rel="permalink">DDQN
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  23 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">A Double Deep Q Network (DDQN) implementation in tensorflow with random experience replay.
The code is tested with Gym’s discrete action space environment, C...</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/DQN/" rel="permalink">DQN
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  18 minute read
</p>
    
    <p class="archive__item-excerpt" itemprop="description">A Deep Q Network implementation in tensorflow with target network &amp; random
experience replay. The code is tested with Gym’s discrete action space
environ...</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>


  </div>
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><input type="text" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
    <div id="results" class="results"></div></div>

      </div>
    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
          <li><a href="https://github.com/ChuaCheowHuan" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2019 Chua Cheow Huan. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script defer src="https://use.fontawesome.com/releases/v5.7.1/js/all.js" integrity="sha384-eVEQC9zshBn0rFj4+TU78eNA19HMNigMviK/PU/FFjLXqa/GKPgX58rvt5Z8PLs7" crossorigin="anonymous"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    
  <script>
    var disqus_config = function () {
      this.page.url = "https://chuacheowhuan.github.io/RDN/";  // Replace PAGE_URL with your page's canonical URL variable
      this.page.identifier = "/RDN"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    (function() { // DON'T EDIT BELOW THIS LINE
      var d = document, s = d.createElement('script');
      s.src = 'https://https-chuacheowhuan-github-io.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>




<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://https-chuacheowhuan-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

  


  </body>
</html>
