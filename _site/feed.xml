<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="https://chuacheowhuan.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://chuacheowhuan.github.io/" rel="alternate" type="text/html" /><updated>2019-08-29T13:10:11+08:00</updated><id>https://chuacheowhuan.github.io/feed.xml</id><title type="html">Every little gist</title><subtitle></subtitle><author><name>Chua Cheow Huan</name></author><entry><title type="html">Django &amp;amp; Postgres with Docker in Travis CI</title><link href="https://chuacheowhuan.github.io/docker_travis/" rel="alternate" type="text/html" title="Django &amp; Postgres with Docker in Travis CI" /><published>2019-08-29T00:00:00+08:00</published><updated>2019-08-29T00:00:00+08:00</updated><id>https://chuacheowhuan.github.io/docker_travis</id><content type="html" xml:base="https://chuacheowhuan.github.io/docker_travis/">&lt;p&gt;Connecting Django &amp;amp; Postgres with Docker in Travis CI.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Code on my &lt;a href=&quot;https://github.com/ChuaCheowHuan/web_app&quot;&gt;Github&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;If you see the following error in the Travis’s job log while attempting to test
dockerized Django apps with Travis, it means that the postgres container has
started but not yet ready to accept connections.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;psycopg2.OperationalError: could not connect to server: Connection refused
539	Is the server running on host &quot;db&quot; (172.18.0.2) and accepting
540	TCP/IP connections on port 5432?

.
.
.

django.db.utils.OperationalError: could not connect to server: Connection
refused 587	Is the server running on host &quot;db&quot; (172.18.0.2) and accepting
588	TCP/IP connections on port 5432?

The command &quot;docker-compose run web python manage.py test&quot; exited with 1.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A solution for this issue is to introduce a delay until connection is ready
before executing the test.&lt;/p&gt;

&lt;p&gt;The delay has to be implemented in the &lt;code class=&quot;highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt; file before
migration &amp;amp; running of Django’s server shown below:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;command: bash -c 'while !&amp;lt;/dev/tcp/db/5432; do sleep 1; done; python3 manage.py migrate'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;command: bash -c 'while !&amp;lt;/dev/tcp/db/5432; do sleep 1; done; python3 manage.py runserver 0.0.0.0:8000'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Config files:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;These are the relevant config files used in a Django project with the delay
introduced in the &lt;code class=&quot;highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt; file. The actual command to run the
test is in the &lt;code class=&quot;highlighter-rouge&quot;&gt;.travis.yml&lt;/code&gt; file.&lt;/p&gt;

&lt;p&gt;The database configuration in &lt;code class=&quot;highlighter-rouge&quot;&gt;settings.py&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql',
        'NAME': 'postgres',
        'USER': 'postgres',
        'HOST': 'db',
        'PORT': 5432,
        #'PORT': 5433,
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FROM python:3
WORKDIR /usr/src/app
ADD requirements.txt /usr/src/app
RUN pip install -r requirements.txt
ADD . /usr/src/app
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt; file:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;version: '3'

services:
    db:
        image: postgres
    migration:
        build: .
#        command: python3 manage.py migrate
        command: bash -c 'while !&amp;lt;/dev/tcp/db/5432; do sleep 1; done; python3 manage.py migrate'
        volumes:
            - .:/usr/src/app
        depends_on:
            - db
    web:
        build: .
#        command: python3 manage.py runserver 0.0.0.0:8000
        command: bash -c 'while !&amp;lt;/dev/tcp/db/5432; do sleep 1; done; python3 manage.py runserver 0.0.0.0:8000'
        volumes:
            - .:/usr/src/app
        ports:
            - &quot;8000:8000&quot;
        depends_on:
            - db
            - migration
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;.travis.yml&lt;/code&gt; file:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;language: python
python:
    - 3.6
services:
    - docker
#    - postgres
install:
    - pip install -r requirements.txt
#before_script:
#    - psql -c 'create database testdb;' -U postgres
#    - psql -c 'create database travisci;' -U postgres
script:
#    - docker-compose build
#    - docker-compose run web python manage.py migrate
    - docker-compose run web python manage.py test
#    - python manage.py test
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;p&gt;After introducing the delay, this is the successful test output in
Travis’s job log.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.
.
.
.......
528----------------------------------------------------------------------
529Ran 10 tests in 0.126s
530
531OK
532Destroying test database for alias 'default'...
533The command &quot;docker-compose run web python manage.py test&quot; exited with 0.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;references&quot;&gt;References:&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/35069027/docker-wait-for-postgresql-to-be-running&quot;&gt;stackoverflow&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">Connecting Django &amp;amp; Postgres with Docker in Travis CI.</summary></entry><entry><title type="html">Random policy in RLlib</title><link href="https://chuacheowhuan.github.io/RLlib_rand_policy/" rel="alternate" type="text/html" title="Random policy in RLlib" /><published>2019-08-29T00:00:00+08:00</published><updated>2019-08-29T00:00:00+08:00</updated><id>https://chuacheowhuan.github.io/RLlib_rand_policy</id><content type="html" xml:base="https://chuacheowhuan.github.io/RLlib_rand_policy/">&lt;p&gt;Creating &amp;amp; seeding a random policy class in RLlib.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Code on my &lt;a href=&quot;https://github.com/ChuaCheowHuan/gym-continuousDoubleAuction/blob/master/gym_continuousDoubleAuction/CDA_env_cont_RLlib.py&quot;&gt;Github&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Function:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def make_RandomPolicy(_seed):

    # a hand-coded policy that acts at random in the env (doesn't learn)
    class RandomPolicy(Policy):
        &quot;&quot;&quot;Hand-coded policy that returns random actions.&quot;&quot;&quot;
        def __init__(self, observation_space, action_space, config):
            self.observation_space = observation_space
            self.action_space = action_space
            self.action_space.seed(_seed)

        def compute_actions(self,
                            obs_batch,
                            state_batches,
                            prev_action_batch=None,
                            prev_reward_batch=None,
                            info_batch=None,
                            episodes=None,
                            **kwargs):
            &quot;&quot;&quot;Compute actions on a batch of observations.&quot;&quot;&quot;
            return [self.action_space.sample() for _ in obs_batch], [], {}

        def learn_on_batch(self, samples):
            &quot;&quot;&quot;No learning.&quot;&quot;&quot;
            #return {}
            pass

        def get_weights(self):
            pass

        def set_weights(self, weights):
            pass

    return RandomPolicy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Usage example:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Setup PPO with an ensemble of `num_policies` different policies
    policies = {&quot;policy_{}&quot;.format(i): gen_policy(i) for i in range(args.num_policies)} # contains many &quot;policy_graphs&quot; in a policies dictionary

    # override policy with random policy
    policies[&quot;policy_{}&quot;.format(args.num_policies-3)] = (make_RandomPolicy(1), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary
    policies[&quot;policy_{}&quot;.format(args.num_policies-2)] = (make_RandomPolicy(2), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary
    policies[&quot;policy_{}&quot;.format(args.num_policies-1)] = (make_RandomPolicy(3), obs_space, act_space, {}) # random policy stored as the last item in policies dictionary
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Huan</name></author><summary type="html">Creating &amp;amp; seeding a random policy class in RLlib.</summary></entry><entry><title type="html">Custom MARL (multi-agent reinforcement learning) CDA (continuous double auction) environment</title><link href="https://chuacheowhuan.github.io/MARL_CDA_env/" rel="alternate" type="text/html" title="Custom MARL (multi-agent reinforcement learning) CDA (continuous double auction) environment" /><published>2019-08-11T00:00:00+08:00</published><updated>2019-08-11T00:00:00+08:00</updated><id>https://chuacheowhuan.github.io/MARL_CDA_env</id><content type="html" xml:base="https://chuacheowhuan.github.io/MARL_CDA_env/">&lt;p&gt;A custom MARL (multi-agent reinforcement learning) environment where multiple
agents trade against one another in a CDA (continuous double auction).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Code on my &lt;a href=&quot;https://github.com/ChuaCheowHuan/gym-continuousDoubleAuction&quot;&gt;Github&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The environment doesn’t use any external data. Data is generated by self play
of the agents themselves through their interaction with the limit order book.&lt;/p&gt;

&lt;p&gt;At each time step, the environment emits the top k rows of the aggregated
order book as observations to the agents.&lt;/p&gt;

&lt;h1 id=&quot;example&quot;&gt;Example:&lt;/h1&gt;
&lt;p&gt;An example of using RLlib to pit 1 PPO (Proximal Policy Optimization) agent
against 3 random agents using this CDA environment is available in:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;CDA_env_disc_RLlib.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To run:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd gym-continuousDoubleAuction/gym_continuousDoubleAuction
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python CDA_env_disc_RLlib.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The figure below from Tensorboard shows the agents’ performance:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/MARL_CDA_env/agent0and1.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;/assets/images/MARL_CDA_env/agent2and3.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PPO agent is using policy 0 while policies 1 to 3 are used by the random agents.&lt;/p&gt;

&lt;h1 id=&quot;dependencies&quot;&gt;Dependencies:&lt;/h1&gt;
&lt;p&gt;1) Tensorflow&lt;/p&gt;

&lt;p&gt;2) OpenAI’s Gym&lt;/p&gt;

&lt;p&gt;3) Ray &amp;amp; RLlib&lt;/p&gt;

&lt;h1 id=&quot;installation&quot;&gt;Installation:&lt;/h1&gt;
&lt;p&gt;The environment is installable via pip.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd gym-continuousDoubleAuction
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;todo&quot;&gt;TODO:&lt;/h1&gt;
&lt;p&gt;1) custom RLlib workflow to include custom RND + PPO policies.&lt;/p&gt;

&lt;p&gt;2) parametric or hybrid action space&lt;/p&gt;

&lt;p&gt;3) more documentation&lt;/p&gt;

&lt;h1 id=&quot;acknowledgements&quot;&gt;Acknowledgements:&lt;/h1&gt;
&lt;p&gt;The orderbook matching engine is adapted from
https://github.com/dyn4mik3/OrderBook&lt;/p&gt;

&lt;h1 id=&quot;disclaimer&quot;&gt;Disclaimer:&lt;/h1&gt;
&lt;p&gt;This repository is only meant for research purposes &amp;amp; is &lt;strong&gt;never&lt;/strong&gt; meant to be
used in any form of trading. Past performance is no guarantee of future results.
If you suffer losses from using this repository, you are the sole person
responsible for the losses. The author will &lt;strong&gt;NOT&lt;/strong&gt; be held responsible in any
way.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">A custom MARL (multi-agent reinforcement learning) environment where multiple agents trade against one another in a CDA (continuous double auction).</summary></entry><entry><title type="html">Tensorflow graphs in Tensorboard</title><link href="https://chuacheowhuan.github.io/tf_graph/" rel="alternate" type="text/html" title="Tensorflow graphs in Tensorboard" /><published>2019-07-04T00:00:00+08:00</published><updated>2019-07-04T00:00:00+08:00</updated><id>https://chuacheowhuan.github.io/tf_graph</id><content type="html" xml:base="https://chuacheowhuan.github.io/tf_graph/">&lt;p&gt;This post demonstrate how setup &amp;amp; access Tensorflow graphs.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In order to access Tensorflow graphs, you need to use &lt;a href=&quot;https://www.tensorflow.org/tensorboard/r2/get_started&quot;&gt;Tensorboard&lt;/a&gt; which comes will Tensorflow
installed.&lt;/p&gt;

&lt;p&gt;The following snippet shows how to setup a &lt;code class=&quot;highlighter-rouge&quot;&gt;FileWriter&lt;/code&gt; with a Tensorflow
graph.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tf.reset_default_graph()
sess = tf.Session()

# Your Tensorflow graph goes here.
# ...

sess.run(tf.global_variables_initializer())

# Declare tf.summary.FileWriter where log is your output directory for
# Tensorboard &amp;amp; add the graph to the writer.
writer = tf.summary.FileWriter('log', sess.graph)

# Run your training loop
# sess.run(...)

writer.close()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run this command in terminal to start tensorboard:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tensorboard --logdir log
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Navigate to &lt;code class=&quot;highlighter-rouge&quot;&gt;http://127.0.0.1:6006&lt;/code&gt; in your browser to access Tensorflow.
Your graph is in the graph tab.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">This post demonstrate how setup &amp;amp; access Tensorflow graphs.</summary></entry><entry><title type="html">.bash_profile for Mac</title><link href="https://chuacheowhuan.github.io/bash_script/" rel="alternate" type="text/html" title=".bash_profile for Mac" /><published>2019-07-03T00:00:00+08:00</published><updated>2019-07-03T00:00:00+08:00</updated><id>https://chuacheowhuan.github.io/bash_script</id><content type="html" xml:base="https://chuacheowhuan.github.io/bash_script/">&lt;p&gt;This post demonstrates how to create customized functions to bundle commands in
a .bash_profile file on Mac.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Edit .bash_profile for Mac.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Start Terminal&lt;/li&gt;
  &lt;li&gt;Enter “cd ~/” to go to home folder&lt;/li&gt;
  &lt;li&gt;Edit .bash_profile with “open -e .bash_profile” to open in TextEdit.&lt;/li&gt;
  &lt;li&gt;Enter “. .bash_profile” to reload .bash_profile.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;examples&quot;&gt;Examples&lt;/h2&gt;

&lt;p&gt;To bundle common git operations, add the following to .bash_profile file:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function lazy_git() {
    git checkout test_ver
    git add .
    git commit -a -m &quot;$1&quot;
    git checkout master
    git merge test_ver
    git push
    git checkout test_ver
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;hr /&gt;

&lt;p&gt;To bundle common jekyll operations, add the following to .bash_profile file:&lt;/p&gt;

&lt;p&gt;The command &lt;code class=&quot;highlighter-rouge&quot;&gt;serve&lt;/code&gt; runs localhost.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function lazy_jekyll_serve() {
    cd /Users/tester/gitHubRepo/ChuaCheowHuan.github.io
    pwd
    bundle exec jekyll serve
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The command &lt;code class=&quot;highlighter-rouge&quot;&gt;build&lt;/code&gt; build the site. This command is neccessary for
generating sitemap.xml &amp;amp; robot.txt.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function lazy_jekyll_build() {
    cd /Users/tester/gitHubRepo/ChuaCheowHuan.github.io
    pwd
    bundle exec jekyll build
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">This post demonstrates how to create customized functions to bundle commands in a .bash_profile file on Mac.</summary></entry><entry><title type="html">RND (Random Network Distillation) with Proximal Policy Optimization (PPO) Tensorflow</title><link href="https://chuacheowhuan.github.io/RND/" rel="alternate" type="text/html" title="RND (Random Network Distillation) with Proximal Policy Optimization (PPO) Tensorflow" /><published>2019-06-25T00:00:00+08:00</published><updated>2019-06-25T00:00:00+08:00</updated><id>https://chuacheowhuan.github.io/RND</id><content type="html" xml:base="https://chuacheowhuan.github.io/RND/">&lt;p&gt;This post documents my implementation of the Random Network Distillation (RND)
with Proximal Policy Optimization (PPO) algorithm.
(&lt;strong&gt;continuous&lt;/strong&gt; version)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Random Network Distillation (RND) with Proximal Policy Optimization (PPO)
implentation in Tensorflow. This is a continuous version which solves the
mountain car continuous problem (MountainCarContinuous-v0).
The RND helps learning with curiosity driven exploration.&lt;/p&gt;

&lt;p&gt;The agent starts to converge correctly at around 30 episodes &amp;amp; reached the flag
291 times out of 300 episodes (97% hit rate). It takes 385.09387278556824
seconds to complete 300 episodes on Google’s Colab.&lt;/p&gt;

&lt;p&gt;Edit: A new version which corrects a numerical error(causes nan action) takes
780.2065596580505 seconds for 300 episodes. Both versions have similar results.
The URL for the new version is updated. Added random seeds for numpy &amp;amp; Tensorflow global seed &amp;amp; ops seed achieve better consistency &amp;amp; faster convergence.&lt;/p&gt;

&lt;p&gt;Checkout the &lt;a href=&quot;https://chuacheowhuan.github.io/RND/#charts&quot;&gt;resulting
charts&lt;/a&gt; from the program output.&lt;/p&gt;

&lt;p&gt;Code on my Github:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/ChuaCheowHuan/reinforcement_learning/blob/master/RND_PPO/RND_PPO_cont_ftr_nsn_mtcar_php.py&quot;&gt;Python file&lt;/a&gt;,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/ChuaCheowHuan/reinforcement_learning/blob/master/RND_PPO/RND_PPO_cont_ftr_nsn_mtCar_php.ipynb&quot;&gt;Jupyter notebook&lt;/a&gt;
(The Jupyter notebook, which also contain the resulting charts at the end, can be run directly on Google’s Colab.)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;notations--equations&quot;&gt;Notations &amp;amp; equations&lt;/h2&gt;

&lt;p&gt;fixed feature from target network =
&lt;script type=&quot;math/tex&quot;&gt;{ f (s_{t+1}) }&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;predicted feature from predictor network =
&lt;script type=&quot;math/tex&quot;&gt;{ f ^\prime  (s_{t+1}) }&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;intrinsic reward =
&lt;script type=&quot;math/tex&quot;&gt;r_{i}&lt;/script&gt; =
||
&lt;script type=&quot;math/tex&quot;&gt;{ f ^\prime  (s_{t+1}) }&lt;/script&gt; -
&lt;script type=&quot;math/tex&quot;&gt;{ f (s_{t+1}) }&lt;/script&gt;
||
&lt;script type=&quot;math/tex&quot;&gt;{}{^2}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;For notations &amp;amp; equations regarding PPO, refer to this
&lt;a href=&quot;https://chuacheowhuan.github.io/DPPO_dist_tf/&quot;&gt;post&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;key-implementation-details&quot;&gt;Key implementation details:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Preprocessing, state featurization:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Prior to training, the states are featurized with the RBF kernel.&lt;/p&gt;

&lt;p&gt;(states are also featurized during every training batch.)&lt;/p&gt;

&lt;p&gt;Refer to scikit-learn.org documentation: &lt;a href=&quot;https://scikit-learn.org/stable/modules/kernel_approximation.html#rbf-kernel-approx&quot;&gt;5.7.2. Radial Basis Function Kernel&lt;/a&gt; for more information on RBF kernel.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if state_ftr == True:
&quot;&quot;&quot;
The following code for state featurization is adapted &amp;amp; modified from dennybritz's repository located at:
https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb
&quot;&quot;&quot;
    # Feature Preprocessing: Normalize to zero mean and unit variance
    # We use a few samples from the observation space to do this
    states = np.array([env.observation_space.sample() for x in range(sample_size)]) # pre-trained, states preprocessing
    scaler = sklearn.preprocessing.StandardScaler()
    scaler.fit(states) # Compute the mean and std to be used for later scaling.

    # convert states to a featurizes representation.
    # We use RBF kernels with different variances to cover different parts of the space
    featurizer = sklearn.pipeline.FeatureUnion([ # Concatenates results of multiple transformer objects.
            (&quot;rbf1&quot;, RBFSampler(gamma=5.0, n_components=n_comp)),
            (&quot;rbf2&quot;, RBFSampler(gamma=2.0, n_components=n_comp)),
            (&quot;rbf3&quot;, RBFSampler(gamma=1.0, n_components=n_comp)),
            (&quot;rbf4&quot;, RBFSampler(gamma=0.5, n_components=n_comp))
            ])
    featurizer.fit(
        scaler.transform(states)) # Perform standardization by centering and scaling

# state featurization of state(s) only,
# not used on s_ for RND's target &amp;amp; predictor networks
def featurize_state(state):
    scaled = scaler.transform([state]) # Perform standardization by centering and scaling
    featurized = featurizer.transform(scaled) # Transform X separately by each transformer, concatenate results.
    return featurized[0]

def featurize_batch_state(batch_states):
    fs_list = []
    for s in batch_states:
        fs = featurize_state(s)
        fs_list.append(fs)
    return fs_list
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Preprocessing, next state normalization for RND:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Variance is computed for the next states &lt;code class=&quot;highlighter-rouge&quot;&gt;buffer_s_&lt;/code&gt; using
the &lt;code class=&quot;highlighter-rouge&quot;&gt;RunningStats&lt;/code&gt; class. During every training batch, the next states are
normalize and clipped.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def state_next_normalize(sample_size, running_stats_s_):

  buffer_s_ = []
  s = env.reset()
  for i in range(sample_size):
    a = env.action_space.sample()
    s_, r, done, _ = env.step(a)
    buffer_s_.append(s_)

  running_stats_s_.update(np.array(buffer_s_))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if state_next_normal == True:
  state_next_normalize(sample_size, running_stats_s_)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;tensorboard-graphs&quot;&gt;Tensorboard graphs:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Big picture:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There are two main modules, the PPO and the RND.&lt;/p&gt;

&lt;p&gt;Current state, &lt;code class=&quot;highlighter-rouge&quot;&gt;state&lt;/code&gt; is passed into PPO.&lt;/p&gt;

&lt;p&gt;Next state, &lt;code class=&quot;highlighter-rouge&quot;&gt;state_&lt;/code&gt; is passed into RND.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/RND_PPO_tf_graph_img/main.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;PPO module:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;PPO module contains the actor network &amp;amp; the critic network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/RND_PPO_tf_graph_img/PPO.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;PPO’s actor:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;At every iteration, an action is sampled from policy network &lt;code class=&quot;highlighter-rouge&quot;&gt;pi&lt;/code&gt;.
&lt;img src=&quot;/assets/images/RND_PPO_tf_graph_img/PPO_a.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;PPO’s critic:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The critic contains two value function networks. One for extrinsic rewards &amp;amp; one
 for intrinsic rewards. Two sets of TD lambda returns &amp;amp; advantages are also
 computed.&lt;/p&gt;

&lt;p&gt;For extrinsic rewards: &lt;code class=&quot;highlighter-rouge&quot;&gt;tdlamret adv&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;For intrinsic rewards: &lt;code class=&quot;highlighter-rouge&quot;&gt;tdlamret_i adv_i&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The TD lambda returns are used as the PPO’s critics targets in their respective
networks while the advantages are summed &amp;amp; used as the advantage in the actor’s
loss computation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/RND_PPO_tf_graph_img/PPO_c.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;RND module:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;RND module contains the target network &amp;amp; the predictor network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/RND_PPO_tf_graph_img/RND.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;RND target network:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The target network is a fixed network, meaning that it’s never trained.
It’s weights are randomized once during initialization. The target network is
used to encode next states &lt;code class=&quot;highlighter-rouge&quot;&gt;state_&lt;/code&gt;. It’s output are encoded next states.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/RND_PPO_tf_graph_img/RND_t.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;RND predictor network:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;predictor_loss&lt;/code&gt; is the intrinsic reward. It is the difference between
the predictor network’s output with the target network’s output. The predictor
network is trying to guess the target network’s encoded output.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/RND_PPO_tf_graph_img/RND_p.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;key-to-note&quot;&gt;Key to note:&lt;/h2&gt;

&lt;p&gt;All networks used in this program are linear.&lt;/p&gt;

&lt;p&gt;The actor module is basically similar to this DPPO &lt;a href=&quot;https://github.com/ChuaCheowHuan/reinforcement_learning/blob/master/DPPO/DPPO_cont_GAE_dist_GPU.ipynb&quot;&gt;code&lt;/a&gt; documented in this &lt;a href=&quot;https://chuacheowhuan.github.io/DPPO_dist_tf/&quot;&gt;post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The difference is in the critic module. This implementation has two value
functions in the critic module rather than one.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;predictor_loss&lt;/code&gt; is the intrinsic reward.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/RND_PPO_tf_graph_img/key.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;problems-encountered&quot;&gt;Problems encountered:&lt;/h2&gt;

&lt;p&gt;The actor’s network occasionally returns ‘'’nan’’’ for action. This happens randomly, most likely caused by exploding gradients.
Not initializing or randomly initializing actor’s weights results in nan when outputting action.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a name=&quot;charts&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;program-output&quot;&gt;Program output:&lt;/h2&gt;

&lt;p&gt;hit_counter 291 0.97&lt;/p&gt;

&lt;p&gt;Number of steps per episode:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/RND_PPO_tf_graph_img/output/steps.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Reward per episode:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/RND_PPO_tf_graph_img/output/reward.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Moving average reward per episode:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/RND_PPO_tf_graph_img/output/mv_avg.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;— 385.09387278556824 seconds —&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;references&quot;&gt;References:&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1810.12894.pdf&quot;&gt;Exploration by Random Network Distillation&lt;/a&gt;
(Yuri Burda, Harrison Edwards, Amos Storkey, Oleg Klimov, 2018)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">This post documents my implementation of the Random Network Distillation (RND) with Proximal Policy Optimization (PPO) algorithm. (continuous version)</summary></entry><entry><title type="html">DPPO distributed tensorflow</title><link href="https://chuacheowhuan.github.io/DPPO_dist_tf/" rel="alternate" type="text/html" title="DPPO distributed tensorflow" /><published>2019-06-25T00:00:00+08:00</published><updated>2019-06-25T00:00:00+08:00</updated><id>https://chuacheowhuan.github.io/DPPO_dist_tf</id><content type="html" xml:base="https://chuacheowhuan.github.io/DPPO_dist_tf/">&lt;p&gt;This post documents my implementation of the Distributed Proximal Policy
Optimization (Distributed PPO or DPPO) algorithm.
(&lt;strong&gt;Distributed&lt;/strong&gt; continuous version)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Distributed Proximal Policy Optimization (Distributed PPO or DPPO) continuous
version implementation with distributed Tensorflow and Python’s multiprocessing
package. This implementation uses normalized running rewards with GAE. The code
is tested with Gym’s continuous action space environment, Pendulum-v0 on Colab.&lt;/p&gt;

&lt;p&gt;Code on my &lt;a href=&quot;https://github.com/ChuaCheowHuan/reinforcement_learning/blob/master/DPPO/DPPO_cont_GAE_dist_GPU.ipynb&quot;&gt;Github&lt;/a&gt;:&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;notations&quot;&gt;Notations:&lt;/h2&gt;

&lt;p&gt;current policy =
&lt;script type=&quot;math/tex&quot;&gt;{\pi}_{\theta}
(a_{t}
  {\mid} s_{t})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;old policy =
&lt;script type=&quot;math/tex&quot;&gt;{\pi}_{\theta_{old}}
(a_{t}
  {\mid} s_{t})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;epsilon =
&lt;script type=&quot;math/tex&quot;&gt;{\epsilon}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Advantage function = A&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;equations&quot;&gt;Equations:&lt;/h2&gt;

&lt;p&gt;Truncated version of generalized advantage estimation (GAE) =&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;A_{t}&lt;/script&gt;
=
&lt;script type=&quot;math/tex&quot;&gt;{\delta}_{t}
+
({\gamma}
{\lambda})
{\delta}_{t}
+
...
+
({\gamma}
{\lambda})
^{T-t+1}
{\delta}_{T-1}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where
&lt;script type=&quot;math/tex&quot;&gt;{\delta}_{t}&lt;/script&gt; =
&lt;script type=&quot;math/tex&quot;&gt;{r}_{t} +
{\gamma}
V(s_{t+1}) -
V(s_{t})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;when &lt;script type=&quot;math/tex&quot;&gt;{\lambda}&lt;/script&gt; = 1,&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;A_{t}&lt;/script&gt; =
&lt;script type=&quot;math/tex&quot;&gt;-V(s_{t}) +
r_{t} +
{\gamma}r_{t+1} +
... +
{\gamma}^{T-t+1}
r_{T-1} +
{\gamma}^{T-t}
V(s_{T})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Probability ratio =&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;R_{t}({\theta})&lt;/script&gt; = &lt;script type=&quot;math/tex&quot;&gt;{\dfrac{ {\pi}_{\theta} (a_{t} {\mid} s_{t}) } { {\pi}_{\theta_{old}} (a_{t} {\mid} s_{t}) } }&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Clipped Surrogate Objective function =&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;L^{CLIP}
({\theta})&lt;/script&gt;
=
&lt;script type=&quot;math/tex&quot;&gt;\mathop{\mathbb{E_{t}}}
\lbrack
min(
  R_{t}({\theta})
  A_{t}
  ,
  clip
  (
    R_{t}({\theta}),
    1+{\epsilon},
    1-{\epsilon}
    )
    A_{t}
  )
\rbrack&lt;/script&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;key-implementation-details&quot;&gt;Key implementation details:&lt;/h2&gt;

&lt;p&gt;The following class is adapted from OpenAI’s baseline:
This class is used for the normalization of rewards in this program before GAE
computation.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class RunningStats(object):
    def __init__(self, epsilon=1e-4, shape=()):
        self.mean = np.zeros(shape, 'float64')
        self.var = np.ones(shape, 'float64')
        self.std = np.ones(shape, 'float64')
        self.count = epsilon

    def update(self, x):
        batch_mean = np.mean(x, axis=0)
        batch_var = np.var(x, axis=0)
        batch_count = x.shape[0]
        self.update_from_moments(batch_mean, batch_var, batch_count)

    def update_from_moments(self, batch_mean, batch_var, batch_count):
        delta = batch_mean - self.mean
        new_mean = self.mean + delta * batch_count / (self.count + batch_count)
        m_a = self.var * self.count
        m_b = batch_var * batch_count
        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)
        new_var = M2 / (self.count + batch_count)

        self.mean = new_mean
        self.var = new_var
        self.std = np.maximum(np.sqrt(self.var), 1e-6)
        self.count = batch_count + self.count
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This function in the &lt;code class=&quot;highlighter-rouge&quot;&gt;PPO&lt;/code&gt; class is adapted from OpenAI’s Baseline,
returns TD lamda return &amp;amp; advantage&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    def add_vtarg_and_adv(self, R, done, V, v_s_, gamma, lam):
        # Compute target value using TD(lambda) estimator, and advantage with GAE(lambda)
        # last element is only used for last vtarg, but we already zeroed it if last new = 1
        done = np.append(done, 0)
        V_plus = np.append(V, v_s_)
        T = len(R)
        adv = gaelam = np.empty(T, 'float32')
        lastgaelam = 0
        for t in reversed(range(T)):
            nonterminal = 1-done[t+1]        
            delta = R[t] + gamma * V_plus[t+1] * nonterminal - V_plus[t]
            gaelam[t] = lastgaelam = delta + gamma * lam * nonterminal * lastgaelam   
        #print(&quot;adv=&quot;, adv.shape)
        #print(&quot;V=&quot;, V.shape)
        #print(&quot;V_plus=&quot;, V_plus.shape)
        tdlamret = np.vstack(adv) + V
        #print(&quot;tdlamret=&quot;, tdlamret.shape)
        return tdlamret, adv # tdlamret is critic_target or Qs      
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following code segment from the &lt;code class=&quot;highlighter-rouge&quot;&gt;PPO&lt;/code&gt; class defines the Clipped Surrogate
Objective function:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with tf.variable_scope('surrogate'):
                    ratio = self.pi.prob(self.act) / self.oldpi.prob(self.act)
                    surr = ratio * self.adv
                    self.aloss = -tf.reduce_mean(tf.minimum(surr, tf.clip_by_value(ratio, 1.-epsilon, 1.+epsilon)*self.adv))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following code segment from the &lt;code class=&quot;highlighter-rouge&quot;&gt;work()&lt;/code&gt; function in the worker class
normalized the running rewards for each worker:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.running_stats_r.update(np.array(buffer_r))
                    buffer_r = np.clip( (np.array(buffer_r) - self.running_stats_r.mean) / self.running_stats_r.std, -stats_CLIP, stats_CLIP )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following code segment from the &lt;code class=&quot;highlighter-rouge&quot;&gt;work()&lt;/code&gt; function in the worker class computes
 the TD lamda return &amp;amp; advantage:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tdlamret, adv = self.ppo.add_vtarg_and_adv(np.vstack(buffer_r), np.vstack(buffer_done), np.vstack(buffer_V), v_s_, GAMMA, lamda)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following update function in the &lt;code class=&quot;highlighter-rouge&quot;&gt;PPO&lt;/code&gt; class does the training &amp;amp; the
updating of global &amp;amp; local parameters (Note the at the beginning of training,
  probability ratio = 1):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def update(self, s, a, r, adv):    
    self.sess.run(self.update_oldpi_op)

    for _ in range(A_EPOCH): # train actor
        self.sess.run(self.atrain_op, {self.state: s, self.act: a, self.adv: adv})
        # update actor
        self.sess.run([self.push_actor_pi_params,
                       self.pull_actor_pi_params],
                      {self.state: s, self.act: a, self.adv: adv})
    for _ in range(C_EPOCH): # train critic
        # update critic
        self.sess.run(self.ctrain_op, {self.state: s, self.discounted_r: r})
        self.sess.run([self.push_critic_params,
                       self.pull_critic_params],
                      {self.state: s, self.discounted_r: r})   
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;p&gt;The distributed Tensorflow &amp;amp; multiprocessing code sections are very similar to
the ones describe in the following posts:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://chuacheowhuan.github.io/A3C_dist_tf/&quot;&gt;A3C distributed tensorflow&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://chuacheowhuan.github.io/dist_tf/&quot;&gt;Distributed Tensorflow&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;references&quot;&gt;References:&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1707.06347.pdf&quot;&gt;Proximal Policy Optimization Algorithms&lt;/a&gt;
(Schulman, Wolski, Dhariwal, Radford, Klimov, 2017)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1707.02286.pdf&quot;&gt;Emergence of Locomotion Behaviours in Rich Environments&lt;/a&gt;
(Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, et al., 2017)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">This post documents my implementation of the Distributed Proximal Policy Optimization (Distributed PPO or DPPO) algorithm. (Distributed continuous version)</summary></entry><entry><title type="html">A3C distributed tensorflow</title><link href="https://chuacheowhuan.github.io/A3C_dist_tf/" rel="alternate" type="text/html" title="A3C distributed tensorflow" /><published>2019-06-25T00:00:00+08:00</published><updated>2019-06-25T00:00:00+08:00</updated><id>https://chuacheowhuan.github.io/A3C_dist_tf</id><content type="html" xml:base="https://chuacheowhuan.github.io/A3C_dist_tf/">&lt;p&gt;This post documents my implementation of the A3C
(Asynchronous Advantage Actor Critic) algorithm
(&lt;strong&gt;Distributed&lt;/strong&gt; discrete version).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A3C (Asynchronous Advantage Actor Critic) implementation with &lt;strong&gt;distributed
Tensorflow&lt;/strong&gt; &amp;amp; &lt;strong&gt;Python multiprocessing package&lt;/strong&gt;. This is a &lt;strong&gt;discrete&lt;/strong&gt;
version with N-step targets (use maximum terms possible). The code is tested
with Gym’s discrete action space environment, CartPole-v0 on Colab.&lt;/p&gt;

&lt;p&gt;Code on my &lt;a href=&quot;https://github.com/ChuaCheowHuan/reinforcement_learning/blob/master/A3C/A3C_disc_max_dist.ipynb&quot;&gt;Github&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The majority of the code is very similar to the &lt;a href=&quot;https://chuacheowhuan.github.io/A3C_disc_thread_nStep/&quot;&gt;discrete&lt;/a&gt; version with the
exceptions highlighted in the implementation details section:&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;key-implementation-details&quot;&gt;Key implementation details:&lt;/h2&gt;

&lt;p&gt;Updating the global episode counter &amp;amp; adding the episodic return to a
tf.FIFOqueue at the end of the work() function.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SESS.run(GLOBAL_EP.assign_add(1.0))
qe = GLOBAL_RUNNING_R.enqueue(ep_r)
SESS.run(qe)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The distributed Tensorflow part is very similar to a simple example described in
this &lt;a href=&quot;https://chuacheowhuan.github.io/dist_tf/&quot;&gt;post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Pin the global variables under the parameter server in both the parameter_server() &amp;amp; worker(worker_n) function:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with tf.device(&quot;/job:ps/task:0&quot;):
    GLOBAL_AC = ACNet(net_scope, sess, globalAC=None) # only need its params
    GLOBAL_EP = tf.Variable(0.0, name='GLOBAL_EP') # num of global episodes   
    # a queue of ep_r
    GLOBAL_RUNNING_R = tf.FIFOQueue(max_global_episodes, tf.float32, shared_name=&quot;GLOBAL_RUNNING_R&quot;)        
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the parameter_server() function, check the size of the tf.FIFOqueue every 1 sec.
If it’s full, dequeue the items in a list. the list will be used for display.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;while True:
    time.sleep(1.0)
    #print(&quot;ps 1 GLOBAL_EP: &quot;, sess.run(GLOBAL_EP))
    #print(&quot;ps 1 GLOBAL_RUNNING_R.size(): &quot;, sess.run(GLOBAL_RUNNING_R.size()))  
    if sess.run(GLOBAL_RUNNING_R.size()) &amp;gt;= max_global_episodes: # GLOBAL_EP starts from 0, hence +1 to max_global_episodes          
        time.sleep(5.0)
        #print(&quot;ps 2 GLOBAL_RUNNING_R.size(): &quot;, sess.run(GLOBAL_RUNNING_R.size()))  
        GLOBAL_RUNNING_R_list = []
        for j in range(sess.run(GLOBAL_RUNNING_R.size())):
            ep_r = sess.run(GLOBAL_RUNNING_R.dequeue())
            GLOBAL_RUNNING_R_list.append(ep_r) # for display
        break
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">This post documents my implementation of the A3C (Asynchronous Advantage Actor Critic) algorithm (Distributed discrete version).</summary></entry><entry><title type="html">A3C multi-threaded continuous version with N step targets</title><link href="https://chuacheowhuan.github.io/A3C_cont_thread_nStep/" rel="alternate" type="text/html" title="A3C multi-threaded continuous version with N step targets" /><published>2019-06-14T00:00:00+08:00</published><updated>2019-06-14T00:00:00+08:00</updated><id>https://chuacheowhuan.github.io/A3C_cont_thread_nStep</id><content type="html" xml:base="https://chuacheowhuan.github.io/A3C_cont_thread_nStep/">&lt;p&gt;This post documents my implementation of the A3C
(Asynchronous Advantage Actor Critic) algorithm.
(multi-threaded &lt;strong&gt;continuous&lt;/strong&gt; version)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;An A3C (Asynchronous Advantage Actor Critic) implementation with
Tensorflow. This is a multi-threaded &lt;strong&gt;continuous&lt;/strong&gt; version. The code is tested with
Gym’s continuous action space environment, Pendulum-v0 on Colab.&lt;/p&gt;

&lt;p&gt;Code on my &lt;a href=&quot;https://github.com/ChuaCheowHuan/reinforcement_learning/blob/master/A3C/A3C_cont_max.ipynb&quot;&gt;Github&lt;/a&gt;: (use maximum terms possible)&lt;/p&gt;

&lt;p&gt;The majority of the code is very similar to the &lt;a href=&quot;https://chuacheowhuan.github.io/A3C_disc_thread_nStep/&quot;&gt;discrete&lt;/a&gt; version with the
exceptions highlighted in the implementation details section:&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;key-implementation-details&quot;&gt;Key implementation details:&lt;/h2&gt;

&lt;p&gt;Action selection:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with tf.name_scope('select_action'):
    #mean = mean * action_bound[1]                   
    mean = mean * ( action_bound[1] - action_bound[0] ) / 2
    sigma += 1e-4
    normal_dist = tf.distributions.Normal(mean, sigma)                     
    self.choose_a = tf.clip_by_value(tf.squeeze(normal_dist.sample(1), axis=[0, 1]), action_bound[0], action_bound[1])                  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Loss function of the actor network:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with tf.name_scope('actor_loss'):
    log_prob = normal_dist.log_prob(self.a)
    #actor_component = log_prob * tf.stop_gradient(TD_err)
    actor_component = log_prob * tf.stop_gradient(self.baselined_returns)
    entropy = -tf.reduce_mean(normal_dist.entropy()) # Compute the differential entropy of the multivariate normal.                   
    self.actor_loss = -tf.reduce_mean( ENTROPY_BETA * entropy + actor_component)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following code segment creates a LSTM layer:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def _lstm(self, Inputs, cell_size):
        # [time_step, feature] =&amp;gt; [time_step, batch, feature]
        s = tf.expand_dims(Inputs, axis=1, name='time_major')  
        lstm_cell = tf.nn.rnn_cell.LSTMCell(cell_size)
        self.init_state = lstm_cell.zero_state(batch_size=1, dtype=tf.float32)
        outputs, self.final_state = tf.nn.dynamic_rnn(cell=lstm_cell, inputs=s, initial_state=self.init_state, time_major=True)
        # joined state representation          
        lstm_out = tf.reshape(outputs, [-1, cell_size], name='flatten_rnn_outputs')  
        return lstm_out
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following function in the ACNet class creates the actor and critic’s neural
networks(note that the critic’s network contains a LSTM layer):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def _create_net(self, scope):
    w_init = tf.glorot_uniform_initializer()
    #w_init = tf.random_normal_initializer(0., .1)
    with tf.variable_scope('actor'):                        
        hidden = tf.layers.dense(self.s, actor_hidden, tf.nn.relu6, kernel_initializer=w_init, name='hidden')            
        #lstm_out = self._lstm(hidden, cell_size)
        # tanh range = [-1,1]
        mean = tf.layers.dense(hidden, num_actions, tf.nn.tanh, kernel_initializer=w_init, name='mean')
        # softplus range = {0,inf}
        sigma = tf.layers.dense(hidden, num_actions, tf.nn.softplus, kernel_initializer=w_init, name='sigma')
    with tf.variable_scope('critic'):
        hidden = tf.layers.dense(self.s, critic_hidden, tf.nn.relu6, kernel_initializer=w_init, name='hidden')
        lstm_out = self._lstm(hidden, cell_size)
        V = tf.layers.dense(lstm_out, 1, kernel_initializer=w_init, name='V')  
    actor_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')
    critic_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')
    return mean, sigma, V, actor_params, critic_params
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">This post documents my implementation of the A3C (Asynchronous Advantage Actor Critic) algorithm. (multi-threaded continuous version)</summary></entry><entry><title type="html">A3C multi-threaded discrete version with N step targets</title><link href="https://chuacheowhuan.github.io/A3C_disc_thread_nStep/" rel="alternate" type="text/html" title="A3C multi-threaded discrete version with N step targets" /><published>2019-06-13T00:00:00+08:00</published><updated>2019-06-13T00:00:00+08:00</updated><id>https://chuacheowhuan.github.io/A3C_disc_thread_nStep</id><content type="html" xml:base="https://chuacheowhuan.github.io/A3C_disc_thread_nStep/">&lt;p&gt;This post documents my implementation of the A3C
(Asynchronous Advantage Actor Critic) algorithm (discrete).
(multi-threaded &lt;strong&gt;discrete&lt;/strong&gt; version)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A3C (Asynchronous Advantage Actor Critic) implementation with Tensorflow.
This is a multi-threaded &lt;strong&gt;discrete&lt;/strong&gt; version. The code is tested with Gym’s
discrete action space environment, CartPole-v0 on Colab.&lt;/p&gt;

&lt;p&gt;Code on my &lt;a href=&quot;https://github.com/ChuaCheowHuan/reinforcement_learning/blob/master/A3C/A3C_disc_miss.ipynb&quot;&gt;Github&lt;/a&gt;: (missing terms are treated as 0)&lt;/p&gt;

&lt;p&gt;Code on my &lt;a href=&quot;https://github.com/ChuaCheowHuan/reinforcement_learning/blob/master/A3C/A3C_disc_max.ipynb&quot;&gt;Github&lt;/a&gt;: (use maximum terms possible)&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;notations&quot;&gt;Notations:&lt;/h2&gt;

&lt;p&gt;Actor network = &lt;script type=&quot;math/tex&quot;&gt;{\pi}_{\theta}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Actor network parameter = &lt;script type=&quot;math/tex&quot;&gt;{\theta}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Critic network = &lt;script type=&quot;math/tex&quot;&gt;V_{\phi}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Critic network parameter = &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Advantage function = A&lt;/p&gt;

&lt;p&gt;Number of trajectories = m&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;equations&quot;&gt;Equations:&lt;/h2&gt;

&lt;p&gt;Actor component: log&lt;script type=&quot;math/tex&quot;&gt;{\pi}_{\theta}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;(a_{t} {\mid} s_{t})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Critic component = Advantage function = A = &lt;script type=&quot;math/tex&quot;&gt;Q(s_{t}, a_{t})&lt;/script&gt; - &lt;script type=&quot;math/tex&quot;&gt;V_{\phi}(s_{t})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Q values with N-step truncated estimate :&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Q^{\pi}(s_{t}, a_{t})&lt;/script&gt; = E(&lt;script type=&quot;math/tex&quot;&gt;r_{t}&lt;/script&gt; + &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;r_{t+1}&lt;/script&gt; + &lt;script type=&quot;math/tex&quot;&gt;\gamma^{2}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;r_{t+2}&lt;/script&gt; + … + &lt;script type=&quot;math/tex&quot;&gt;\gamma^{n}&lt;/script&gt; V(&lt;script type=&quot;math/tex&quot;&gt;s_{t+n}&lt;/script&gt;))&lt;/p&gt;

&lt;p&gt;Check this &lt;a href=&quot;https://chuacheowhuan.github.io/n_step_targets/&quot;&gt;post&lt;/a&gt; for more information on N-step truncated estimate.&lt;/p&gt;

&lt;p&gt;Policy gradient estimator&lt;/p&gt;

&lt;p&gt;= &lt;script type=&quot;math/tex&quot;&gt;\nabla_\theta J(\theta)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;= &lt;script type=&quot;math/tex&quot;&gt;{\dfrac{1}{m}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;{\sum\limits_{i=1}^{m}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;{\sum\limits_{t=0}^{T}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;\nabla_\theta&lt;/script&gt; log&lt;script type=&quot;math/tex&quot;&gt;{\pi}_{\theta}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;(a_{t} {\mid} s_{t})&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q(s_{t}, a_{t})&lt;/script&gt; - &lt;script type=&quot;math/tex&quot;&gt;V_{\phi}(s_{t})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;= &lt;script type=&quot;math/tex&quot;&gt;{\dfrac{1}{m}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;{\sum\limits_{i=1}^{m}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;{\sum\limits_{t=0}^{T}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;\nabla_\theta&lt;/script&gt; log&lt;script type=&quot;math/tex&quot;&gt;{\pi}_{\theta}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;(a_{t} {\mid} s_{t})&lt;/script&gt; A&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;key-implementation-details&quot;&gt;Key implementation details:&lt;/h2&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;ACNet&lt;/code&gt; class defines the models (Tensorflow graphs) and contains both
the actor and the critic networks. The &lt;code class=&quot;highlighter-rouge&quot;&gt;Worker&lt;/code&gt; class contains the work
function that does the main bulk of the computation. A copy of &lt;code class=&quot;highlighter-rouge&quot;&gt;ACNet&lt;/code&gt; is
declared globally &amp;amp; it’s parameters are shared by the threaded workers. Each
worker also have it’s own local copy of &lt;code class=&quot;highlighter-rouge&quot;&gt;ACNet&lt;/code&gt;. Workers are instantiated &amp;amp;
threaded in the main program.&lt;/p&gt;

&lt;h3 id=&quot;acnet-class&quot;&gt;ACNet class:&lt;/h3&gt;

&lt;p&gt;Loss function for the actor network for the discrete environment:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with tf.name_scope('actor_loss'):
    log_prob = tf.reduce_sum(tf.log(self.action_prob + 1e-5) * tf.one_hot(self.a, num_actions, dtype=tf.float32), axis=1, keep_dims=True)
    actor_component = log_prob * tf.stop_gradient(self.baselined_returns)
    # entropy for exploration
    entropy = -tf.reduce_sum(self.action_prob * tf.log(self.action_prob + 1e-5), axis=1, keep_dims=True)  # encourage exploration
    self.actor_loss = tf.reduce_mean( -(ENTROPY_BETA * entropy + actor_component) )                                        
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Loss function for the critic network for the discrete environment:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TD_err = tf.subtract(self.critic_target, self.V, name='TD_err')
      .
      .
      .
with tf.name_scope('critic_loss'):
    self.critic_loss = tf.reduce_mean(tf.square(TD_err))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following function in the ACNet class creates the actor and critic’s neural
networks:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def _create_net(self, scope):
    w_init = tf.glorot_uniform_initializer()
    with tf.variable_scope('actor'):
        hidden = tf.layers.dense(self.s, actor_hidden, tf.nn.relu6, kernel_initializer=w_init, name='hidden')
        action_prob = tf.layers.dense(hidden, num_actions, tf.nn.softmax, kernel_initializer=w_init, name='action_prob')        
    with tf.variable_scope('critic'):
        hidden = tf.layers.dense(self.s, critic_hidden, tf.nn.relu6, kernel_initializer=w_init, name='hidden')
        V = tf.layers.dense(hidden, 1, kernel_initializer=w_init, name='V')         
    actor_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')
    critic_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')       
    return action_prob, V, actor_params, critic_params
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;worker-class&quot;&gt;Worker class:&lt;/h3&gt;

&lt;p&gt;Discounted rewards are used as critic’s targets:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;critic_target = self.discount_rewards(buffer_r, GAMMA, V_s)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;N-step returns are used in the computation of the Advantage function
(baselined_returns):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Advantage function
baselined_returns = n_step_targets - baseline
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2 versions of N-step targets could be used:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;missing terms are treated as 0.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;use maximum terms possible.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Check this &lt;a href=&quot;https://chuacheowhuan.github.io/n_step_targets/&quot;&gt;post&lt;/a&gt; for more
information on N-step targets.&lt;/p&gt;

&lt;p&gt;The following code segment accumulates gradients &amp;amp; apply them to the local
critic network:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.AC.accumu_grad_critic(feed_dict) # accumulating gradients for local critic  
self.AC.apply_accumu_grad_critic(feed_dict)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following code segment computes the advantage function(baselined_returns):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;baseline = SESS.run(self.AC.V, {self.AC.s: buffer_s}) # Value function
epr = np.vstack(buffer_r).astype(np.float32)
n_step_targets = self.compute_n_step_targets_missing(epr, baseline, GAMMA, N_step) # Q values
# Advantage function
baselined_returns = n_step_targets - baseline
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following code segment accumulates gradients for the local actor network:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.AC.accumu_grad_actor(feed_dict) # accumulating gradients for local actor  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following code segment push the parameters from the local networks to the
global networks and then pulls the updated global parameters to the local
networks:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# update
self.AC.push_global_actor(feed_dict)                
self.AC.push_global_critic(feed_dict)
    .
    .
    .
self.AC.pull_global()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following code segment initialize storage for accumulated local gradients.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.AC.init_grad_storage_actor() # initialize storage for accumulated gradients.
self.AC.init_grad_storage_critic()            
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Check this &lt;a href=&quot;https://chuacheowhuan.github.io/tf_accumulate_grad/&quot;&gt;post&lt;/a&gt; for more
information on how to accumulate gradients in Tensorflow.&lt;/p&gt;

&lt;h2 id=&quot;main-program&quot;&gt;Main program:&lt;/h2&gt;

&lt;p&gt;The following code segment creates the workers:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;workers = []
for i in range(num_workers): # Create worker
    i_name = 'W_%i' % i # worker name
    workers.append(Worker(i_name, GLOBAL_AC))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following code segment threads the workers:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;worker_threads = []
for worker in workers:
    job = lambda: worker.work()
    t = threading.Thread(target=job)
    t.start()
    worker_threads.append(t)
COORD.join(worker_threads)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References:&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1602.01783.pdf&quot;&gt;Asynchronous Methods for Deep Reinforcement Learning
(Mnih, Badia, Mirza, Graves, Harley, Lillicrap, et al., 2016)&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">This post documents my implementation of the A3C (Asynchronous Advantage Actor Critic) algorithm (discrete). (multi-threaded discrete version)</summary></entry></feed>