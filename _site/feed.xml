<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="https://chuacheowhuan.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://chuacheowhuan.github.io/" rel="alternate" type="text/html" /><updated>2020-06-17T15:23:31+08:00</updated><id>https://chuacheowhuan.github.io/feed.xml</id><title type="html">Every little gist</title><subtitle></subtitle><author><name>Chua Cheow Huan</name></author><entry><title type="html">Finding the `ray_results` folder in colab</title><link href="https://chuacheowhuan.github.io/ray_results_folder_in_colab/" rel="alternate" type="text/html" title="Finding the `ray_results` folder in colab" /><published>2020-06-17T00:00:00+08:00</published><updated>2020-06-17T00:00:00+08:00</updated><id>https://chuacheowhuan.github.io/ray_results_folder_in_colab</id><content type="html" xml:base="https://chuacheowhuan.github.io/ray_results_folder_in_colab/">&lt;p&gt;The location of &lt;code class=&quot;highlighter-rouge&quot;&gt;ray_results&lt;/code&gt; folder in colab when using RLlib &amp;amp;/or tune.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;If you’re NOT using &lt;code class=&quot;highlighter-rouge&quot;&gt;tune.run()&lt;/code&gt; but using only RLlib’s python API &amp;amp; if
you set the &lt;code class=&quot;highlighter-rouge&quot;&gt;local_dir&lt;/code&gt; in RLlib’s &lt;code class=&quot;highlighter-rouge&quot;&gt;config&lt;/code&gt; to a non-default directory,
(The default is &lt;code class=&quot;highlighter-rouge&quot;&gt;~/ray_results&lt;/code&gt;.) you will only find the &lt;code class=&quot;highlighter-rouge&quot;&gt;results.json&lt;/code&gt; file
(&amp;amp; your checkpoint folders if you also specify them) in your specified
directory. The other files such as the tensorboard event files, the params.pkl
&amp;amp; params.json will still be saved in the default directory.&lt;/p&gt;

&lt;p&gt;However, if you use &lt;code class=&quot;highlighter-rouge&quot;&gt;tune.run()&lt;/code&gt; &amp;amp; set the &lt;code class=&quot;highlighter-rouge&quot;&gt;local_dir&lt;/code&gt; argument to your
specified directory, all the files will be saved there.&lt;/p&gt;

&lt;p&gt;If you’re using colab, the way to access &lt;code class=&quot;highlighter-rouge&quot;&gt;~/ray_results&lt;/code&gt; is to click on a small
folder icon on the left of the panel, it will open up a side panel, simply go to
&lt;code class=&quot;highlighter-rouge&quot;&gt;root/ray_results&lt;/code&gt;. All those files will be saved in there.&lt;/p&gt;

&lt;p&gt;Note: It seems like setting the (1)&lt;code class=&quot;highlighter-rouge&quot;&gt;local_dir&lt;/code&gt; in RLlib’s &lt;code class=&quot;highlighter-rouge&quot;&gt;config&lt;/code&gt; will not
automatically set the (2)&lt;code class=&quot;highlighter-rouge&quot;&gt;local_dir&lt;/code&gt; in the &lt;code class=&quot;highlighter-rouge&quot;&gt;Experiment&lt;/code&gt; class in &lt;code class=&quot;highlighter-rouge&quot;&gt;experiment.py&lt;/code&gt;
if you’re not using &lt;code class=&quot;highlighter-rouge&quot;&gt;tune.run()&lt;/code&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">The location of ray_results folder in colab when using RLlib &amp;amp;/or tune.</summary></entry><entry><title type="html">PBT for MARL</title><link href="https://chuacheowhuan.github.io/PBT_MARL_watered_down/" rel="alternate" type="text/html" title="PBT for MARL" /><published>2020-06-12T00:00:00+08:00</published><updated>2020-06-12T00:00:00+08:00</updated><id>https://chuacheowhuan.github.io/PBT_MARL_watered_down</id><content type="html" xml:base="https://chuacheowhuan.github.io/PBT_MARL_watered_down/">&lt;p&gt;My attempt to implement a water down version of PBT (Population based training) for MARL (Multi-agent reinforcement learning).&lt;/p&gt;

&lt;p&gt;Code on my &lt;a href=&quot;https://github.com/ChuaCheowHuan/PBT_MARL_watered_down&quot;&gt;Github&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;pbt_marl_watered_down&quot;&gt;PBT_MARL_watered_down&lt;/h1&gt;

&lt;h1 id=&quot;whats-in-this-repo&quot;&gt;What’s in this repo?&lt;/h1&gt;

&lt;p&gt;My attempt to implement a water down version of PBT (Population based training) for MARL (Multi-agent reinforcement learning) inspired by Algorithm 1 (PBT-MARL) on page 3 of this &lt;a href=&quot;https://arxiv.org/pdf/1902.07151.pdf&quot;&gt;paper&lt;/a&gt;[1].&lt;/p&gt;

&lt;h1 id=&quot;main-differences-from-the-paper&quot;&gt;MAIN differences from the paper:&lt;/h1&gt;

&lt;p&gt;(1) A simple 1 VS 1 &lt;a href=&quot;https://github.com/ray-project/ray/blob/57544b1ff9f97d4da9f64d25c8ea5a3d8d247ffc/rllib/examples/env/rock_paper_scissors.py&quot;&gt;RockPaperScissorsEnv&lt;/a&gt; environment (adapted &amp;amp; modified from a toy example from ray) is used instead of the 2 VS 2 &lt;a href=&quot;https://git.io/dm_soccer&quot;&gt;dm_soccer&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;(2) PPO is used instead of SVG0.&lt;/p&gt;

&lt;p&gt;(3) No reward shaping.&lt;/p&gt;

&lt;p&gt;(4) The evolution eligibility documented in B2 on page 16 in the &lt;a href=&quot;https://arxiv.org/pdf/1902.07151.pdf&quot;&gt;paper&lt;/a&gt;[1] is not implemented.&lt;/p&gt;

&lt;p&gt;(5) Probably many more…&lt;/p&gt;

&lt;h1 id=&quot;what-works&quot;&gt;What works?&lt;/h1&gt;

&lt;p&gt;(1) Policies weights can be inherited between different agents in the population.&lt;/p&gt;

&lt;p&gt;(2) Learning rate &amp;amp; gamma are the only 2 hyperparameters involved for now. Both can be inherited/mutated. Learning rate can be resampled/perturbed while gamma can only be resampled.&lt;/p&gt;

&lt;h1 id=&quot;simple-walkthru&quot;&gt;Simple walkthru:&lt;/h1&gt;

&lt;p&gt;Before each training iteration, the driver (in this context, the main process, this is also where the RLlib trainer resides) randomly selects a pair of agents (agt_i, agt_j, where i != j) from a population of agents. This i, j pair will take up the role of player_A &amp;amp; player_B respectively.&lt;/p&gt;

&lt;p&gt;The IDs of i,j will be transmitted down to the worker processes. Each worker has 1 or more environments (&lt;a href=&quot;https://rllib.readthedocs.io/en/latest/rllib-env.html#vectorized&quot;&gt;vectorized&lt;/a&gt;) &amp;amp; does it’s own rollout. When an episode is sampled (that’s when a match ends), the &lt;code class=&quot;highlighter-rouge&quot;&gt;on_episode_end&lt;/code&gt; callback will be called. That’s when the ratings of a match are computed &amp;amp; updated to a global storage.&lt;/p&gt;

&lt;p&gt;When enough samples are collected, training starts. Training is done using &lt;a href=&quot;https://docs.ray.io/en/master/rllib-algorithms.html#decentralized-distributed-proximal-policy-optimization-dd-ppo&quot;&gt;RLlib’s DDPPO&lt;/a&gt; (a variant of PPO). In DDPPO, learning does not happened in the trainer. Each worker does it’s own learning. However, the trainer is still involved in the weight sync.&lt;/p&gt;

&lt;p&gt;When a training iteration completes, &lt;code class=&quot;highlighter-rouge&quot;&gt;on_train_results&lt;/code&gt; callback will be called. That’s where inheritance &amp;amp; mutation happens (if conditions are fulfilled).&lt;/p&gt;

&lt;p&gt;All of the above happens during 1 single main training loop of the driver. Rinse &amp;amp; repeat.&lt;/p&gt;

&lt;p&gt;Note: Global coordination between different processes is done using &lt;a href=&quot;https://docs.ray.io/en/master/advanced.html#detached-actors&quot;&gt;detached actors&lt;/a&gt; from ray.&lt;/p&gt;

&lt;h1 id=&quot;example-of-whats-stored-in-the-global-storage&quot;&gt;Example of what’s stored in the global storage:&lt;/h1&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;&quot;&quot;
{'agt_0':
    {'hyperparameters':
        {'lr': [0.0027558, 0.0022046, ...]},
         'gamma': [0.9516804908336309, 0.9516804908336309, ...]
     'opponent': ['NA', 'agt_5', 'agt_5', ...],
     'score': [0, -4.0, -2.0, ...],
     'rating': [0.0, 0.05, 0.05, ...],
     'step': [0]},
 'agt_1': ...
    .
    .
    .
 'agt_n': ...    
}
&quot;&quot;&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;how-to-run-the-contents-in-this-repo&quot;&gt;How to run the contents in this repo?&lt;/h1&gt;

&lt;p&gt;The easiest way is to run the &lt;code class=&quot;highlighter-rouge&quot;&gt;PBT_MARL_watered_down.ipynb&lt;/code&gt; Jupyter notebook in Colab.&lt;/p&gt;

&lt;h1 id=&quot;dependencies&quot;&gt;Dependencies:&lt;/h1&gt;

&lt;p&gt;This is developed &amp;amp; tested on Colab &amp;amp; the following are the only packages that I explicitly &lt;code class=&quot;highlighter-rouge&quot;&gt;pip install&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;ray[rllib]==0.8.5&lt;/p&gt;

&lt;p&gt;tensorflow==2.2.0&lt;/p&gt;

&lt;h1 id=&quot;disclaimer&quot;&gt;Disclaimer:&lt;/h1&gt;

&lt;p&gt;(1) I’m not affiliated with any of the authors of the &lt;a href=&quot;https://arxiv.org/pdf/1902.07151.pdf&quot;&gt;paper&lt;/a&gt;[1].&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References:&lt;/h1&gt;

&lt;p&gt;[1] &lt;a href=&quot;https://arxiv.org/pdf/1902.07151.pdf&quot;&gt;EMERGENT COORDINATION THROUGH COMPETITION (Liu et al., 2019)&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">My attempt to implement a water down version of PBT (Population based training) for MARL (Multi-agent reinforcement learning).</summary></entry><entry><title type="html">RLlib trainer common config</title><link href="https://chuacheowhuan.github.io/RLlib_trainer_config/" rel="alternate" type="text/html" title="RLlib trainer common config" /><published>2020-03-23T00:00:00+08:00</published><updated>2020-03-23T00:00:00+08:00</updated><id>https://chuacheowhuan.github.io/RLlib_trainer_config</id><content type="html" xml:base="https://chuacheowhuan.github.io/RLlib_trainer_config/">Ray (0.8.2) RLlib trainer common config from:

[source](https://github.com/ray-project/ray/blob/master/rllib/agents/trainer.py)

---

```
# yapf: disable
# __sphinx_doc_begin__
COMMON_CONFIG = {
    # === Settings for Rollout Worker processes ===
    # Number of rollout worker actors to create for parallel sampling. Setting
    # this to 0 will force rollouts to be done in the trainer actor.
    &quot;num_workers&quot;: 2,
    # Number of environments to evaluate vectorwise per worker. This enables
    # model inference batching, which can improve performance for inference
    # bottlenecked workloads.
    &quot;num_envs_per_worker&quot;: 1,
    # Divide episodes into fragments of this many steps each during rollouts.
    # Sample batches of this size are collected from rollout workers and
    # combined into a larger batch of `train_batch_size` for learning.
    #
    # For example, given rollout_fragment_length=100 and train_batch_size=1000:
    #   1. RLlib collects 10 fragments of 100 steps each from rollout workers.
    #   2. These fragments are concatenated and we perform an epoch of SGD.
    #
    # When using multiple envs per worker, the fragment size is multiplied by
    # `num_envs_per_worker`. This is since we are collecting steps from
    # multiple envs in parallel. For example, if num_envs_per_worker=5, then
    # rollout workers will return experiences in chunks of 5*100 = 500 steps.
    #
    # The dataflow here can vary per algorithm. For example, PPO further
    # divides the train batch into minibatches for multi-epoch SGD.
    &quot;rollout_fragment_length&quot;: 200,
    # Deprecated; renamed to `rollout_fragment_length` in 0.8.4.
    &quot;sample_batch_size&quot;: DEPRECATED_VALUE,
    # Whether to rollout &quot;complete_episodes&quot; or &quot;truncate_episodes&quot; to
    # `rollout_fragment_length` length unrolls. Episode truncation guarantees
    # evenly sized batches, but increases variance as the reward-to-go will
    # need to be estimated at truncation boundaries.
    &quot;batch_mode&quot;: &quot;truncate_episodes&quot;,

    # === Settings for the Trainer process ===
    # Number of GPUs to allocate to the trainer process. Note that not all
    # algorithms can take advantage of trainer GPUs. This can be fractional
    # (e.g., 0.3 GPUs).
    &quot;num_gpus&quot;: 0,
    # Training batch size, if applicable. Should be &gt;= rollout_fragment_length.
    # Samples batches will be concatenated together to a batch of this size,
    # which is then passed to SGD.
    &quot;train_batch_size&quot;: 200,
    # Arguments to pass to the policy model. See models/catalog.py for a full
    # list of the available model options.
    &quot;model&quot;: MODEL_DEFAULTS,
    # Arguments to pass to the policy optimizer. These vary by optimizer.
    &quot;optimizer&quot;: {},

    # === Environment Settings ===
    # Discount factor of the MDP.
    &quot;gamma&quot;: 0.99,
    # Number of steps after which the episode is forced to terminate. Defaults
    # to `env.spec.max_episode_steps` (if present) for Gym envs.
    &quot;horizon&quot;: None,
    # Calculate rewards but don't reset the environment when the horizon is
    # hit. This allows value estimation and RNN state to span across logical
    # episodes denoted by horizon. This only has an effect if horizon != inf.
    &quot;soft_horizon&quot;: False,
    # Don't set 'done' at the end of the episode. Note that you still need to
    # set this if soft_horizon=True, unless your env is actually running
    # forever without returning done=True.
    &quot;no_done_at_end&quot;: False,
    # Arguments to pass to the env creator.
    &quot;env_config&quot;: {},
    # Environment name can also be passed via config.
    &quot;env&quot;: None,
    # Unsquash actions to the upper and lower bounds of env's action space
    &quot;normalize_actions&quot;: False,
    # Whether to clip rewards prior to experience postprocessing. Setting to
    # None means clip for Atari only.
    &quot;clip_rewards&quot;: None,
    # Whether to np.clip() actions to the action space low/high range spec.
    &quot;clip_actions&quot;: True,
    # Whether to use rllib or deepmind preprocessors by default
    &quot;preprocessor_pref&quot;: &quot;deepmind&quot;,
    # The default learning rate.
    &quot;lr&quot;: 0.0001,

    # === Debug Settings ===
    # Whether to write episode stats and videos to the agent log dir. This is
    # typically located in ~/ray_results.
    &quot;monitor&quot;: False,
    # Set the ray.rllib.* log level for the agent process and its workers.
    # Should be one of DEBUG, INFO, WARN, or ERROR. The DEBUG level will also
    # periodically print out summaries of relevant internal dataflow (this is
    # also printed out once at startup at the INFO level). When using the
    # `rllib train` command, you can also use the `-v` and `-vv` flags as
    # shorthand for INFO and DEBUG.
    &quot;log_level&quot;: &quot;WARN&quot;,
    # Callbacks that will be run during various phases of training. These all
    # take a single &quot;info&quot; dict as an argument. For episode callbacks, custom
    # metrics can be attached to the episode by updating the episode object's
    # custom metrics dict (see examples/custom_metrics_and_callbacks.py). You
    # may also mutate the passed in batch data in your callback.
    &quot;callbacks&quot;: {
        &quot;on_episode_start&quot;: None,     # arg: {&quot;env&quot;: .., &quot;episode&quot;: ...}
        &quot;on_episode_step&quot;: None,      # arg: {&quot;env&quot;: .., &quot;episode&quot;: ...}
        &quot;on_episode_end&quot;: None,       # arg: {&quot;env&quot;: .., &quot;episode&quot;: ...}
        &quot;on_sample_end&quot;: None,        # arg: {&quot;samples&quot;: .., &quot;worker&quot;: ...}
        &quot;on_train_result&quot;: None,      # arg: {&quot;trainer&quot;: ..., &quot;result&quot;: ...}
        &quot;on_postprocess_traj&quot;: None,  # arg: {
                                      #   &quot;agent_id&quot;: ..., &quot;episode&quot;: ...,
                                      #   &quot;pre_batch&quot;: (before processing),
                                      #   &quot;post_batch&quot;: (after processing),
                                      #   &quot;all_pre_batches&quot;: (other agent ids),
                                      # }
    },
    # Whether to attempt to continue training if a worker crashes. The number
    # of currently healthy workers is reported as the &quot;num_healthy_workers&quot;
    # metric.
    &quot;ignore_worker_failures&quot;: False,
    # Log system resource metrics to results. This requires `psutil` to be
    # installed for sys stats, and `gputil` for GPU metrics.
    &quot;log_sys_usage&quot;: True,

    # === Framework Settings ===
    # Use PyTorch (instead of tf). If using `rllib train`, this can also be
    # enabled with the `--torch` flag.
    # NOTE: Some agents may not support `torch` yet and throw an error.
    &quot;use_pytorch&quot;: False,

    # Enable TF eager execution (TF policies only). If using `rllib train`,
    # this can also be enabled with the `--eager` flag.
    &quot;eager&quot;: False,
    # Enable tracing in eager mode. This greatly improves performance, but
    # makes it slightly harder to debug since Python code won't be evaluated
    # after the initial eager pass.
    &quot;eager_tracing&quot;: False,
    # Disable eager execution on workers (but allow it on the driver). This
    # only has an effect if eager is enabled.
    &quot;no_eager_on_workers&quot;: False,

    # === Exploration Settings ===
    # Default exploration behavior, iff `explore`=None is passed into
    # compute_action(s).
    # Set to False for no exploration behavior (e.g., for evaluation).
    &quot;explore&quot;: True,
    # Provide a dict specifying the Exploration object's config.
    &quot;exploration_config&quot;: {
        # The Exploration class to use. In the simplest case, this is the name
        # (str) of any class present in the `rllib.utils.exploration` package.
        # You can also provide the python class directly or the full location
        # of your class (e.g. &quot;ray.rllib.utils.exploration.epsilon_greedy.
        # EpsilonGreedy&quot;).
        &quot;type&quot;: &quot;StochasticSampling&quot;,
        # Add constructor kwargs here (if any).
    },
    # === Evaluation Settings ===
    # Evaluate with every `evaluation_interval` training iterations.
    # The evaluation stats will be reported under the &quot;evaluation&quot; metric key.
    # Note that evaluation is currently not parallelized, and that for Ape-X
    # metrics are already only reported for the lowest epsilon workers.
    &quot;evaluation_interval&quot;: None,
    # Number of episodes to run per evaluation period. If using multiple
    # evaluation workers, we will run at least this many episodes total.
    &quot;evaluation_num_episodes&quot;: 10,
    # Internal flag that is set to True for evaluation workers.
    &quot;in_evaluation&quot;: False,
    # Typical usage is to pass extra args to evaluation env creator
    # and to disable exploration by computing deterministic actions.
    # IMPORTANT NOTE: Policy gradient algorithms are able to find the optimal
    # policy, even if this is a stochastic one. Setting &quot;explore=False&quot; here
    # will result in the evaluation workers not using this optimal policy!
    &quot;evaluation_config&quot;: {
        # Example: overriding env_config, exploration, etc:
        # &quot;env_config&quot;: {...},
        # &quot;explore&quot;: False
    },
    # Number of parallel workers to use for evaluation. Note that this is set
    # to zero by default, which means evaluation will be run in the trainer
    # process. If you increase this, it will increase the Ray resource usage
    # of the trainer since evaluation workers are created separately from
    # rollout workers.
    &quot;evaluation_num_workers&quot;: 0,
    # Customize the evaluation method. This must be a function of signature
    # (trainer: Trainer, eval_workers: WorkerSet) -&gt; metrics: dict. See the
    # Trainer._evaluate() method to see the default implementation. The
    # trainer guarantees all eval workers have the latest policy state before
    # this function is called.
    &quot;custom_eval_function&quot;: None,
    # EXPERIMENTAL: use the execution plan based API impl of the algo. Can also
    # be enabled by setting RLLIB_EXEC_API=1.
    &quot;use_exec_api&quot;: False,

    # === Advanced Rollout Settings ===
    # Use a background thread for sampling (slightly off-policy, usually not
    # advisable to turn on unless your env specifically requires it).
    &quot;sample_async&quot;: False,
    # Element-wise observation filter, either &quot;NoFilter&quot; or &quot;MeanStdFilter&quot;.
    &quot;observation_filter&quot;: &quot;NoFilter&quot;,
    # Whether to synchronize the statistics of remote filters.
    &quot;synchronize_filters&quot;: True,
    # Configures TF for single-process operation by default.
    &quot;tf_session_args&quot;: {
        # note: overriden by `local_tf_session_args`
        &quot;intra_op_parallelism_threads&quot;: 2,
        &quot;inter_op_parallelism_threads&quot;: 2,
        &quot;gpu_options&quot;: {
            &quot;allow_growth&quot;: True,
        },
        &quot;log_device_placement&quot;: False,
        &quot;device_count&quot;: {
            &quot;CPU&quot;: 1
        },
        &quot;allow_soft_placement&quot;: True,  # required by PPO multi-gpu
    },
    # Override the following tf session args on the local worker
    &quot;local_tf_session_args&quot;: {
        # Allow a higher level of parallelism by default, but not unlimited
        # since that can cause crashes with many concurrent drivers.
        &quot;intra_op_parallelism_threads&quot;: 8,
        &quot;inter_op_parallelism_threads&quot;: 8,
    },
    # Whether to LZ4 compress individual observations
    &quot;compress_observations&quot;: False,
    # Wait for metric batches for at most this many seconds. Those that
    # have not returned in time will be collected in the next train iteration.
    &quot;collect_metrics_timeout&quot;: 180,
    # Smooth metrics over this many episodes.
    &quot;metrics_smoothing_episodes&quot;: 100,
    # If using num_envs_per_worker &gt; 1, whether to create those new envs in
    # remote processes instead of in the same worker. This adds overheads, but
    # can make sense if your envs can take much time to step / reset
    # (e.g., for StarCraft). Use this cautiously; overheads are significant.
    &quot;remote_worker_envs&quot;: False,
    # Timeout that remote workers are waiting when polling environments.
    # 0 (continue when at least one env is ready) is a reasonable default,
    # but optimal value could be obtained by measuring your environment
    # step / reset and model inference perf.
    &quot;remote_env_batch_wait_ms&quot;: 0,
    # Minimum time per train iteration (frequency of metrics reporting).
    &quot;min_iter_time_s&quot;: 0,
    # Minimum env steps to optimize for per train call. This value does
    # not affect learning, only the length of train iterations.
    &quot;timesteps_per_iteration&quot;: 0,  # TODO(ekl) deprecate this
    # This argument, in conjunction with worker_index, sets the random seed of
    # each worker, so that identically configured trials will have identical
    # results. This makes experiments reproducible.
    &quot;seed&quot;: None,

    # === Advanced Resource Settings ===
    # Number of CPUs to allocate per worker.
    &quot;num_cpus_per_worker&quot;: 1,
    # Number of GPUs to allocate per worker. This can be fractional. This is
    # usually needed only if your env itself requires a GPU (i.e., it is a
    # GPU-intensive video game), or model inference is unusually expensive.
    &quot;num_gpus_per_worker&quot;: 0,
    # Any custom Ray resources to allocate per worker.
    &quot;custom_resources_per_worker&quot;: {},
    # Number of CPUs to allocate for the trainer. Note: this only takes effect
    # when running in Tune. Otherwise, the trainer runs in the main program.
    &quot;num_cpus_for_driver&quot;: 1,
    # You can set these memory quotas to tell Ray to reserve memory for your
    # training run. This guarantees predictable execution, but the tradeoff is
    # if your workload exceeeds the memory quota it will fail.
    # Heap memory to reserve for the trainer process (0 for unlimited). This
    # can be large if your are using large train batches, replay buffers, etc.
    &quot;memory&quot;: 0,
    # Object store memory to reserve for the trainer process. Being large
    # enough to fit a few copies of the model weights should be sufficient.
    # This is enabled by default since models are typically quite small.
    &quot;object_store_memory&quot;: 0,
    # Heap memory to reserve for each worker. Should generally be small unless
    # your environment is very heavyweight.
    &quot;memory_per_worker&quot;: 0,
    # Object store memory to reserve for each worker. This only needs to be
    # large enough to fit a few sample batches at a time. This is enabled
    # by default since it almost never needs to be larger than ~200MB.
    &quot;object_store_memory_per_worker&quot;: 0,

    # === Offline Datasets ===
    # Specify how to generate experiences:
    #  - &quot;sampler&quot;: generate experiences via online simulation (default)
    #  - a local directory or file glob expression (e.g., &quot;/tmp/*.json&quot;)
    #  - a list of individual file paths/URIs (e.g., [&quot;/tmp/1.json&quot;,
    #    &quot;s3://bucket/2.json&quot;])
    #  - a dict with string keys and sampling probabilities as values (e.g.,
    #    {&quot;sampler&quot;: 0.4, &quot;/tmp/*.json&quot;: 0.4, &quot;s3://bucket/expert.json&quot;: 0.2}).
    #  - a function that returns a rllib.offline.InputReader
    &quot;input&quot;: &quot;sampler&quot;,
    # Specify how to evaluate the current policy. This only has an effect when
    # reading offline experiences. Available options:
    #  - &quot;wis&quot;: the weighted step-wise importance sampling estimator.
    #  - &quot;is&quot;: the step-wise importance sampling estimator.
    #  - &quot;simulation&quot;: run the environment in the background, but use
    #    this data for evaluation only and not for learning.
    &quot;input_evaluation&quot;: [&quot;is&quot;, &quot;wis&quot;],
    # Whether to run postprocess_trajectory() on the trajectory fragments from
    # offline inputs. Note that postprocessing will be done using the *current*
    # policy, not the *behavior* policy, which is typically undesirable for
    # on-policy algorithms.
    &quot;postprocess_inputs&quot;: False,
    # If positive, input batches will be shuffled via a sliding window buffer
    # of this number of batches. Use this if the input data is not in random
    # enough order. Input is delayed until the shuffle buffer is filled.
    &quot;shuffle_buffer_size&quot;: 0,
    # Specify where experiences should be saved:
    #  - None: don't save any experiences
    #  - &quot;logdir&quot; to save to the agent log dir
    #  - a path/URI to save to a custom output directory (e.g., &quot;s3://bucket/&quot;)
    #  - a function that returns a rllib.offline.OutputWriter
    &quot;output&quot;: None,
    # What sample batch columns to LZ4 compress in the output data.
    &quot;output_compress_columns&quot;: [&quot;obs&quot;, &quot;new_obs&quot;],
    # Max output file size before rolling over to a new file.
    &quot;output_max_file_size&quot;: 64 * 1024 * 1024,

    # === Settings for Multi-Agent Environments ===
    &quot;multiagent&quot;: {
        # Map from policy ids to tuples of (policy_cls, obs_space,
        # act_space, config). See rollout_worker.py for more info.
        &quot;policies&quot;: {},
        # Function mapping agent ids to policy ids.
        &quot;policy_mapping_fn&quot;: None,
        # Optional whitelist of policies to train, or None for all policies.
        &quot;policies_to_train&quot;: None,
    },
}
# __sphinx_doc_end__
# yapf: enable
```

---

&lt;br&gt;</content><author><name>Huan</name></author><summary type="html">Ray (0.8.2) RLlib trainer common config from:</summary></entry><entry><title type="html">Output dimension from convolution layer</title><link href="https://chuacheowhuan.github.io/conv_output/" rel="alternate" type="text/html" title="Output dimension from convolution layer" /><published>2020-03-09T00:00:00+08:00</published><updated>2020-03-09T00:00:00+08:00</updated><id>https://chuacheowhuan.github.io/conv_output</id><content type="html" xml:base="https://chuacheowhuan.github.io/conv_output/">How to calculate dimension of output from a convolution layer?

---

# No padding (aka valid padding):

input  = n x n = 6 x 6

kernel = f x f = 3 x 3

output = m x m = 4 x 4

How do we get output = 4 x 4 ?

Ans: Use the formula: (n - f + 1) x (n - f + 1)

---

# With padding of size 1:

p = 1

input  = n x n = 6 x 6

kernel = f x f = 3 x 3

output = m x m = 6 x 6

How do we get output = 6 x 6 ?

Ans: Use the formula: (n + 2p - f + 1) x (n + 2p - f + 1)

---

# Meaning of valid padding &amp; same padding:

1) No padding is also known as valid padding.

2) Same padding means pad input so that the resulting output dimension after
convolution will be the same as input.

---

# Size of padding needed to achieve same padding:

Size of padding needed to achieve same padding depends on the kernel size, f.

Using p = (f - 1) / 2 will produce output dimension = input dimension.

---
&lt;br&gt;</content><author><name>Huan</name></author><summary type="html">How to calculate dimension of output from a convolution layer?</summary></entry><entry><title type="html">Changing G drive directory in Colab</title><link href="https://chuacheowhuan.github.io/colab/" rel="alternate" type="text/html" title="Changing G drive directory in Colab" /><published>2020-02-28T00:00:00+08:00</published><updated>2020-02-28T00:00:00+08:00</updated><id>https://chuacheowhuan.github.io/colab</id><content type="html" xml:base="https://chuacheowhuan.github.io/colab/">Changing Google drive directory in Colab.

---

```
from google.colab import drive
drive.mount('/content/gdrive')

%cd &quot;/content/gdrive/My Drive/Colab Notebooks/courses/deep_learning/&quot;

!pwd
```

---

&lt;br&gt;</content><author><name>Huan</name></author><summary type="html">Changing Google drive directory in Colab.</summary></entry><entry><title type="html">Linear regression (Bayesian)</title><link href="https://chuacheowhuan.github.io/linear_regression_bayesian/" rel="alternate" type="text/html" title="Linear regression (Bayesian)" /><published>2020-01-29T00:00:00+08:00</published><updated>2020-01-29T00:00:00+08:00</updated><id>https://chuacheowhuan.github.io/linear_regression_bayesian</id><content type="html" xml:base="https://chuacheowhuan.github.io/linear_regression_bayesian/">Notes on the probability for linear regression (Bayesian)

---

Bayes' theorem:

$$P(a, b) = P(a|b) P(b) = P(b|a) P(a) -&gt; P(a|b) = P(b|a) P(a) / P(b)$$

So,

$$P(y, w|x) = P(x|y, w) P(y, w) / P(x) ---&gt; 1$$

The 1st part of the nominator from 1 is:

$$P(x|y, w) ---&gt; 2$$

From joint probability:

$$P(a, b, c) = P(a|b, c) P(b, c) = P(b|a, c) P (a, c)$$

$$i.e. P(a|b, c) = P(b|a, c) P(a, c) / P(b, c) ---&gt; 3$$

Apply 3 to 2:

$$P(x|y, w) = P(y|x, w) P(x, w) / P(y, w) ---&gt; 4$$

Plug 4 back to 1:

$$P(y, w|x) = [ P(y|x, w) P(x, w) P(y, w) ] / [P(y, w) P( x)]$$

$$P(y, w|x) = [P(y|x, w) P(x, w)] / P(x)$$

$$P(y, w|x) = [P(y|x, w) P(w|x) P(x)] / P(x)$$

$$P(y, w|x) = P(y|x, w) P(w|x) ---&gt; 5$$

If w and x are independent:

$$P(y, w|x) = P(y|x, w) P(w) ---&gt; 6$$

Also from 5, if we switch w with y, we can obtain:

$$P(w, y|x) = P(w|x, y) P(y|x)$$

$$P(y, w|x) = P(w|x, y) P(y|x)$$

$$P(w|x, y) = P(y, w|x) / P(y|x) ---&gt; 7$$

We try to maximize 7 with respect to w. Only the nominator depends on w, so we
can ignore the denominator P(y|x) and we get:

Maximize with respect to w in the following equations 8, 9, 10:

$$P(w|x, y) = P(y, w|x) ---&gt; 8$$

Therefore, from 6 &amp; 8 we get:

$$P(w|x, y) = P(y, w|x) = P(y|x, w) P(w) ---&gt; 9$$

$$P(w|x, y) = P(y|x, w) P(w) ---&gt; 10$$

---

&lt;br&gt;</content><author><name>Huan</name></author><summary type="html">Notes on the probability for linear regression (Bayesian)</summary></entry><entry><title type="html">RNN backprop thru time(BPTT part 2) $$\frac{\delta h_{t}} {\delta h_{t-1}}$$</title><link href="https://chuacheowhuan.github.io/RNN_BPTT_2/" rel="alternate" type="text/html" title="RNN backprop thru time(BPTT part 2) $$\frac{\delta h_{t}} {\delta h_{t-1}}$$" /><published>2020-01-13T00:00:00+08:00</published><updated>2020-01-13T00:00:00+08:00</updated><id>https://chuacheowhuan.github.io/RNN_BPTT_2</id><content type="html" xml:base="https://chuacheowhuan.github.io/RNN_BPTT_2/">Notes on the math for RNN back propagation through time(BPTT), part 2. The 1st
derivative of $$h_t$$ with respect to $$h_{t-1}$$.

---
Given a series:
$$X = \{x_1, x_2...x_n\}$$

Given a set of functions that takes in $$X$$:

$$Y = F(X)$$

We have a vector of functions:

$$Y =
\begin{pmatrix}
f_1(X) \\
f_2(X) \\
. \\
. \\
. \\
f_n(X) \\
\end{pmatrix}$$

---

A Jacobian is a matrix of 1st derivatives of the functions:

$$\begin{pmatrix}
\frac{\delta y_1} {\delta x_1} &amp; \frac{\delta y_1} {\delta x_2} &amp; ... &amp; \frac{\delta y_1} {\delta x_n} \\
. &amp; . &amp; ... &amp; .\\
. &amp; . &amp; ... &amp; .\\
. &amp; . &amp; ... &amp; .\\
\frac{\delta y_n} {\delta x_1} &amp; \frac{\delta y_n} {\delta x_2} &amp; ... &amp; \frac{\delta y_n} {\delta x_n}
\end{pmatrix}$$

For the calculations below, we are only interested in the diagonal derivatives
which are terms with the same subscript in the nominator &amp; denominator (i.e: derivatives with respect to the same time step).

$$\frac{\delta h_t} {\delta E_t} = diag(f'(E_t))$$

---

Deriving $$\frac{\delta h_t} {\delta h_{t-1}}$$:

$$Vx_t + Wh_{t-1} + b_{h} = E_t$$

$$h_{t} = f_{h} (Vx_t + Wh_{t-1} + b_{h}) = f_{h}(E_t)$$

$$\frac{\delta h_t} {\delta E_t} = diag(f'(E_t))$$

$$\frac{\delta E_t} {\delta h_{t-1}} = W$$

$$\frac{\delta h_t} {\delta h_{t-1}} =
\frac{\delta h_t} {\delta E_t}
\frac{\delta E_t} {\delta h_{t-1}} =
diag(f'(E_t)) W$$

---

&lt;br&gt;</content><author><name>Huan</name></author><summary type="html">Notes on the math for RNN back propagation through time(BPTT), part 2. The 1st derivative of with respect to .</summary></entry><entry><title type="html">RNN backprop thru time(BPTT)</title><link href="https://chuacheowhuan.github.io/RNN_BPTT/" rel="alternate" type="text/html" title="RNN backprop thru time(BPTT)" /><published>2020-01-12T00:00:00+08:00</published><updated>2020-01-12T00:00:00+08:00</updated><id>https://chuacheowhuan.github.io/RNN_BPTT</id><content type="html" xml:base="https://chuacheowhuan.github.io/RNN_BPTT/">Notes on the math for RNN back propagation through time(BPTT).

---

$$\hat{y}$$  is the prediction.

$$U$$ is the weight matrix after the hidden layer.

$$\hat{y} = f_y (U h_t + b_y)$$

---

$$h_{t}$$ is the hidden layer at time t.

$$f_{h}$$ is the non linear function in the hidden layer.

$$V$$ is the weight matrix before the hidden layer.

$$W_{h_{t-1}}$$ is the weight matrix in the hidden layer at the previous time
step.

$$b_{h}$$ is the bias at the hidden layer.

$$h_{t} = f_{h} (Vx_t + Wh_{t-1} + b_{h})$$

---

No need to BPTT for this:

$$\frac{\delta L_{t}} {\delta U} =
\sum_{i=0}^{T} \frac{\delta L_i} {\delta U} =
\frac{\delta L_t} {\delta \hat{y}_t}
\frac{\delta \hat{y}_t} {\delta U}$$

---

**BPTT**

The loss at time t with respect to the weights in the hidden layer.

The terms in square brackets $$[]$$ are written in such a way that the leftmost
term is the most recent term while the rightmost is the oldest term.

When k = t, the product sequence (or factor) on the left of
$$\frac{\delta h_{t}} {\delta W}$$, equals 1.

$$\frac{\delta L_{t}} {\delta W} =
\frac{\delta L_{t}} {\delta \hat{y}_{t}}
\frac{\delta \hat{y}_{t}} {\delta L_{t}}
[
(1)
\frac{\delta h_{t}} {\delta W} +
(
\frac{\delta h_{t}} {\delta h_{t-1}}
\frac{\delta h_{t-1}} {\delta W} ) +
(
\frac{\delta h_{t}} {\delta h_{t-1}}
\frac{\delta h_{t-1}} {\delta h_{t-2}}
\frac{\delta h_{t-2}} {\delta W}
) + ...
]$$

$$=
\frac{\delta L_t} {\delta \hat{y}_t}
\frac{\delta \hat{y}_t} {\delta h_t}
\sum_{k=0}^{t}
[
\frac{\delta h_{t}} {\delta h_{t-1}}  
\frac{\delta h_{t-1}} {\delta h_{t-2}} ...
\frac{\delta h_{k+2}} {\delta h_{k+1}}
\frac{\delta h_{k+1}} {\delta h_k}
\frac{\delta h_k} {\delta W}
]$$

$$=
\frac{\delta L_t} {\delta \hat{y}_t}
\frac{\delta \hat{y}_t} {\delta h_t}
[ \sum_{k=0}^{t}
( \prod_{i=k+1}^{t}
\frac{\delta h_{i}} {\delta h_{i-1}} )
\frac{\delta h_k} {\delta W} ]$$

---

$$\frac{\delta L_{t}} {\delta V}$$

BPTT also needs to be done for calculating the loss with respect to weight matrix V.
The dependence between hidden units and weight matrix V is  not only in one place.
Hidden units from all the previous time steps also depend on V so we need to go
backwards in time to calculate this gradient.

---

&lt;br&gt;</content><author><name>Huan</name></author><summary type="html">Notes on the math for RNN back propagation through time(BPTT).</summary></entry><entry><title type="html">Filter rows with same column values in Pandas dataframe</title><link href="https://chuacheowhuan.github.io/pandas_df_rm_row_with_same_col_val/" rel="alternate" type="text/html" title="Filter rows with same column values in Pandas dataframe" /><published>2019-12-25T00:00:00+08:00</published><updated>2019-12-25T00:00:00+08:00</updated><id>https://chuacheowhuan.github.io/pandas_df_rm_row_with_same_col_val</id><content type="html" xml:base="https://chuacheowhuan.github.io/pandas_df_rm_row_with_same_col_val/">Filter rows with same column values in a Pandas dataframe.

---

Assume the following Pandas dataframe where Q1 &amp; Q2 are charges. A charge can
either be +1 or -1.

```
df = pd.DataFrame([[1,1],[-1,-1],[1,-1],[-1,1]], columns=['Q1', 'Q2'])

   Q1  Q2
0   1   1
1  -1  -1
2   1  -1
3  -1   1
```

The following code produces a dataframe where each row only contains
opposite charges, i.e if Q1 is 1, Q2 is -1 &amp; vice versa.

```
df = df[ (df.Q1 == 1) &amp; (df.Q2 == -1) | (df.Q1 == -1) &amp; (df.Q2 == 1) ]

   Q1  Q2
2   1  -1
3  -1   1
```

---

&lt;br&gt;</content><author><name>Huan</name></author><summary type="html">Filter rows with same column values in a Pandas dataframe.</summary></entry><entry><title type="html">Custom Sagemaker reinforcement learning container</title><link href="https://chuacheowhuan.github.io/custom_sagemaker_RL_container/" rel="alternate" type="text/html" title="Custom Sagemaker reinforcement learning container" /><published>2019-09-15T00:00:00+08:00</published><updated>2019-09-15T00:00:00+08:00</updated><id>https://chuacheowhuan.github.io/custom_sagemaker_RL_container</id><content type="html" xml:base="https://chuacheowhuan.github.io/custom_sagemaker_RL_container/">Building &amp; testing custom Sagemaker RL container.

Instead of using the official SageMaker supported version of Ray RLlib
(version 0.5.3 &amp; 0.6.5), I want to use version 0.7.3. In order to do so, I have
to build &amp; test my custom Sagemaker RL container.

---

**The Dockerfile:**

Add the Dockerfile below to ```sagemaker-rl-container/ray/docker/0.7.3/```:

```
ARG processor
#FROM 520713654638.dkr.ecr.us-west-2.amazonaws.com/sagemaker-tensorflow-scriptmode:1.14.0-$processor-py3
FROM 520713654638.dkr.ecr.us-west-2.amazonaws.com/sagemaker-tensorflow-scriptmode:1.12.0-$processor-py3

RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \
        build-essential \
        jq \
        libav-tools \
        libjpeg-dev \
        libxrender1 \
        python3.6-dev \
        python3-opengl \
        wget \
        xvfb &amp;&amp; \
    apt-get clean &amp;&amp; \
    rm -rf /var/lib/apt/lists/*

RUN pip install --no-cache-dir \
    Cython==0.29.7 \
    gym==0.14.0 \
    lz4==2.1.10 \
    opencv-python-headless==4.1.0.25 \
    PyOpenGL==3.1.0 \
    pyyaml==5.1.1 \
    redis&gt;=3.2.2 \
    ray==0.7.3 \
    ray[rllib]==0.7.3 \
    scipy==1.3.0 \
    requests

# https://click.palletsprojects.com/en/7.x/python3/
ENV LC_ALL=C.UTF-8
ENV LANG=C.UTF-8

# Copy workaround script for incorrect hostname
COPY lib/changehostname.c /

COPY lib/start.sh /usr/local/bin/start.sh
RUN chmod +x /usr/local/bin/start.sh

# Starts framework
ENTRYPOINT [&quot;bash&quot;, &quot;-m&quot;, &quot;start.sh&quot;]
```

---

**Remove unneeded test files:**

Backup the ```test``` folder as ```test_bkup```
in ```sagemaker-rl-container/```.

Remove the following files not used in testing
in ```sagemaker-rl-container/test/integration/local/```:

```
test_coach.py
test_vw_cb_explore.py
test_vw_cbify.py
test_vw_serving.py
```

---

**Add/replace codes in test files to get role:**

In the ```sagemaker-rl-container/test/conftest.py```file, add/replace the
following:

```
from sagemaker import get_execution_role
```

```
#parser.addoption('--role', default='SageMakerContainerBuildIntegrationTests')
parser.addoption('--role', default=get_execution_role()),

```

In the following files:

```
sagemaker-rl-container/test/integration/local/test_gym.py
sagemaker-rl-container/test/integration/local/test_ray.py
```

Add/replace the following:

```
from sagemaker import get_execution_role
```

```
#role='SageMakerRole',
role = get_execution_role(),
```

---

**Build the image:**

In SageMaker, start a Jupyter notebook instance &amp; open a terminal.

Login into SageMaker ECR account:

```
$ (aws ecr get-login --no-include-email --region &lt;region&gt; --registry-ids &lt;AWS_ACC_ID&gt;)
$ (aws ecr get-login --no-include-email --region us-west-2 --registry-ids 520713654638)
```

Copy &amp; paste the output from the above command into the terminal &amp; press Enter.

---

Pull the base Tensorflow image from the aws ecr:

```
$ docker pull 520713654638.dkr.ecr.us-west-2.amazonaws.com/sagemaker-tensorflow-scriptmode:1.12.0-cpu-py3
```

Build the Ray image using the ```Dockerfile.tf``` from above:

```
$ docker build -t custom-smk-rl-ctn:tf-1.12.0-ray-0.7.3-cpu-py3 -f ray/docker/0.7.3/Dockerfile.tf --build-arg processor=cpu .
```

---

**Local testing:**

Install dependencies for testing:

```
$ cd sagemaker-rl-container
$ pip install .
```

Run the command below for local testing:

```
clear &amp;&amp; \
docker images &amp;&amp; \
pytest test/integration/local --framework tensorflow \
                              --toolkit ray \
                              --toolkit-version 0.7.3 \
                              --docker-base-name custom-smk-rl-ctn \
                              --tag tf-1.12.0-ray-0.7.3-cpu-py3 \
                              --processor cpu | tee local_test_output.txt
```

The output from the test will be saved in local_test_output.txt.

---

**Pushing to registry on AWS ECR:**

```
$ (aws ecr get-login --no-include-email --region &lt;region&gt; --registry-ids &lt;AWS_ACC_ID&gt;)
$ (aws ecr get-login --no-include-email --region us-west-2  --registry-ids 123456789012)
# Copy &amp; paste output to terminal &amp; press enter.

$ aws ecr create-repository --repository-name &lt;repo_name&gt;
$ aws ecr create-repository --repository-name custom-smk-rl-ctn

$ docker tag &lt;image_ID&gt; &lt;AWS_ACC_ID&gt;.dkr.ecr.us-west-2.amazonaws.com/&lt;repo_name&gt;:&lt;tag&gt;
$ docker tag ba542f0b9706 &lt;123456789012&gt;.dkr.ecr.us-west-2.amazonaws.com/custom-smk-rl-ctn:tf-1.12.0-cpu-py3
$ docker tag ba542f0b9706 &lt;123456789012&gt;.dkr.ecr.us-west-2.amazonaws.com/custom-smk-rl-ctn:tf-1.12.0-ray-0.7.3-cpu-py3

$ docker push &lt;AWS_ACC_ID&gt;.dkr.ecr.us-west-2.amazonaws.com/&lt;repo_name&gt;:&lt;tag&gt;
$ docker push &lt;123456789012&gt;.dkr.ecr.us-west-2.amazonaws.com/custom-smk-rl-ctn:tf-1.12.0-cpu-py3
$ docker push &lt;123456789012&gt;.dkr.ecr.us-west-2.amazonaws.com/custom-smk-rl-ctn:tf-1.12.0-ray-0.7.3-cpu-py3


$ aws ecr describe-repositories

$ aws ecr list-images --repository-name &lt;repo_name&gt;
$ aws ecr list-images --repository-name custom-smk-rl-ctn
```

---

**Testing with AWS SageMaker ML instance:**

Run the command below for testing with SageMaker:

```
clear &amp;&amp; \
docker images &amp;&amp; \
pytest test/integration/sagemaker --aws-id 123456789012 \
                                  --instance-type ml.m4.xlarge \
                                  --framework tensorflow \
                                  --toolkit ray \
                                  --toolkit-version 0.7.3 \
                                  --docker-base-name custom-smk-rl-ctn \
                                  --tag tf-1.12.0-ray-0.7.3-cpu-py3 | tee SageMaker_test_output.txt
```

The output from the test will be saved in SageMaker_test_output.txt.

---

**Pushing to registry on Docker hub:**

```
$ docker login

$ docker tag &lt;image_ID&gt; &lt;DockerHubUserName&gt;/&lt;repo_name&gt;:&lt;tag&gt;
$ docker tag ba542f0b9706 &lt;DockerHubUserName&gt;/custom-smk-rl-ctn:tf-1.12.0-cpu-py3
$ docker tag ba542f0b9706 &lt;DockerHubUserName&gt;/custom-smk-rl-ctn:tf-1.12.0-ray-0.7.3-cpu-py3

$ docker push &lt;DockerHubUserName&gt;/&lt;repo_name&gt;:&lt;tag&gt;
$ docker push &lt;DockerHubUserName&gt;/custom-smk-rl-ctn:tf-1.12.0-cpu-py3
$ docker push &lt;DockerHubUserName&gt;/custom-smk-rl-ctn:tf-1.12.0-ray-0.7.3-cpu-py3
```

---

**Training with custom SageMaker RL container:**



---

**Useful Docker commands:**

```
$ docker ps -a

$ docker images

$ docker rm &lt;container&gt;

$ docker rmi &lt;image&gt;
```

---

**Useful AWS commands:**

```
$ aws ecr delete-repository --force --repository-name &lt;repo_name&gt;
```

---

**References:**

[https://github.com/aws/sagemaker-rl-container](https://github.com/aws/sagemaker-rl-container)

---

&lt;br&gt;</content><author><name>Huan</name></author><summary type="html">Building &amp;amp; testing custom Sagemaker RL container.</summary></entry></feed>