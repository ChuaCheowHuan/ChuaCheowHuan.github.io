<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-05-30T17:34:34+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Chua Cheow Huan</title><subtitle></subtitle><author><name>Chua Cheow Huan</name></author><entry><title type="html">N-step target</title><link href="http://localhost:4000/n_step_target/" rel="alternate" type="text/html" title="N-step target" /><published>2019-05-30T00:00:00+08:00</published><updated>2019-05-30T00:00:00+08:00</updated><id>http://localhost:4000/n_step_target</id><content type="html" xml:base="http://localhost:4000/n_step_target/">&lt;p&gt;N-step target&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ChuaCheowHuan/reinforcement_learning/tree/master/&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">N-step target</summary></entry><entry><title type="html">Python’s multiprocessing package</title><link href="http://localhost:4000/np_array_manipulation/" rel="alternate" type="text/html" title="Python's multiprocessing package" /><published>2019-05-30T00:00:00+08:00</published><updated>2019-05-30T00:00:00+08:00</updated><id>http://localhost:4000/np_array_manipulation</id><content type="html" xml:base="http://localhost:4000/np_array_manipulation/">&lt;p&gt;Python’s multiprocessing package for parallel data generation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ChuaCheowHuan/reinforcement_learning/tree/master/&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In the context of reinforcement learning algorithms such as A3C or DPPO, data generation is done in parallel. This simple example program demonstrates how to use the Python’s multiprocessing package to achieve parallel data generation.&lt;/p&gt;

&lt;p&gt;The main program has a chief that spawns multiple worker processes. Each worker spawns a single work process. The work process generates random integer data [1,3].&lt;/p&gt;

&lt;p&gt;Each worker has it’s own local queue. When data is generated, it is stored in it’s local queue. When the local queue’s size is greater than 5, the data is retrieved &amp;amp; 0.1 is added to the data, this result is stored in the Chief’s global queue. When the Chief’s global queue’s size is greater than 3, the result is retrieved &amp;amp; printed on screen.&lt;/p&gt;

&lt;p&gt;The Worker class:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class Worker(object):
  def __init__(self, worker_id, g_queue):
    self.g_queue = g_queue
    self.worker_id = worker_id
    self.queue = Queue() # local worker queue
    self.work_process = Process(target=self.work, args=())
    self.work_process.start()
    info(worker_id, self.work_process, &quot;Worker&quot;)

  def work(self):

    info(self.worker_id, self.work_process, &quot;work&quot;)

    while True:
      data = np.random.randint(1,4)
      self.queue.put(data)

      # process data in queue
      if self.queue.qsize() &amp;gt; 5:
        data = self.queue.get()
        result = data + 0.1
        self.g_queue.put(result) # send result to global queue

      time.sleep(1) # work every x sec interval

    return self.w_id  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Chief class:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class Chief(object):
  def __init__(self, num_workers):
    self.g_queue = Queue() # global queue    
    self.num_workers = num_workers

  def dispatch_workers(self):   
    worker_processes = [Process(target=Worker(w_id, self.g_queue), args=()) for w_id in range(num_workers)]
    return worker_processes

  def result(self):
    if self.g_queue.qsize() &amp;gt; 3:
      result = self.g_queue.get()
      print(&quot;result&quot;, result)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The main program:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if __name__ == '__main__':  
  print('main parent process id:', os.getppid())
  print('main process id:', os.getpid())

  num_workers = 2
  chief = Chief(num_workers)
  workers_processes = chief.dispatch_workers()

  i = 0
  while True:    
    time.sleep(2) # chk g_queue every x sec interval to get result
    chief.result()
    print(&quot;i=&quot;, i)

    if i&amp;gt;9:
      break
    i+=1    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A helper display function:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def info(worker_id, process, function_name):
    print(&quot;worker_id=&quot;, worker_id,
          'module name:', __name__,
          'function name:', function_name,
          'parent process:', os.getppid(),
          'current process id:', os.getpid(),
          'spawn process id:', process.pid)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Huan</name></author><summary type="html">Python’s multiprocessing package for parallel data generation.</summary></entry><entry><title type="html">Numpy array manipulation</title><link href="http://localhost:4000/py_mpp/" rel="alternate" type="text/html" title="Numpy array manipulation" /><published>2019-05-29T00:00:00+08:00</published><updated>2019-05-29T00:00:00+08:00</updated><id>http://localhost:4000/py_mpp</id><content type="html" xml:base="http://localhost:4000/py_mpp/">&lt;p&gt;Simple numpy array manipulation examples&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ChuaCheowHuan/reinforcement_learning/tree/master/&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This Jupyter notebook contains simple examples on how to manipulate numpy arrays. The code block below shows the codes &amp;amp; it’s corresponding display output.&lt;/p&gt;

&lt;p&gt;Setting up a numpy array:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;buffer=[0,1]
print('buffer=', buffer)
$buffer= [0, 1]

new=2
print('new=', new)
$new= 2

buffer = np.array(buffer + [new]) # append a new item &amp;amp; create a numpy array
print('np.array(buffer + [new])=', buffer)
$np.array(buffer + [new])= [0 1 2]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Slicing examples:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# numpy array slicing syntax
# buffer[start:stop:step]

print('buffer[1:]=', buffer[1:]) # starting from index 1
$buffer[1:]= [1 2]

print('buffer[-1:]=', buffer[-1:]) # getting item in last index
$buffer[-1:]= [2]

print('buffer[:1]=', buffer[:1]) # stop at index 1 (exclusive), keep only 1st item
$buffer[:1]= [0]

print('buffer[:-1]=', buffer[:-1]) # stop at last index (exclusive), discard item in last index
$buffer[:-1]= [0 1]

print('buffer[::-1]=', buffer[::-1]) # start from last index (reversal)
$buffer[::-1]= [2 1 0]

print('buffer[1::-1]=', buffer[1::-1]) # reverse starting from index 1
$buffer[1::-1]= [1 0]

# Starting from index 1 will return [1 2], reversing will return [2,1]
print('buffer[1:][::-1]=', buffer[1:][::-1])
$buffer[1:][::-1]= [2 1]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;np.newaxis is an alias for None:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# np.newaxis = None

print('buffer[:, np.newaxis]=', buffer[:, np.newaxis])
$buffer[:, np.newaxis]= [[0][1][2]]

print('buffer[:, None]=', buffer[:, None])
$buffer[:, None]= [[0][1][2]]

print('buffer[np.newaxis, :]=', buffer[np.newaxis, :])
$buffer[np.newaxis, :]= [[0 1 2]]

print('buffer[None, :]=', buffer[None, :])
$buffer[None, :]= [[0 1 2]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Stacking:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;a = [1,2,3]
b = [4,5,6]
c = [7,8,9]

r = np.hstack((a,b,c)) # horizontal stacking
print(&quot;r=&quot;, r)
$r= [1 2 3 4 5 6 7 8 9]

QUEUE = queue.Queue()
QUEUE.put(a)
QUEUE.put(b)
QUEUE.put(c)

r = [QUEUE.get() for _ in range(QUEUE.qsize())]
print(r)
$[[1, 2, 3], [4, 5, 6], [7, 8, 9]]

r = np.vstack(r) # vertical stacking
print(r)
$[[1 2 3]
  [4 5 6]
  [7 8 9]]

print(r[:, ::-1]) # col reversal
$[[3 2 1]
  [6 5 4]
  [9 8 7]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Huan</name></author><summary type="html">Simple numpy array manipulation examples</summary></entry><entry><title type="html">Dueling DDQN with PER</title><link href="http://localhost:4000/Duel_DDQN_with_PER/" rel="alternate" type="text/html" title="Dueling DDQN with PER" /><published>2019-03-15T00:00:00+08:00</published><updated>2019-03-15T00:00:00+08:00</updated><id>http://localhost:4000/Duel_DDQN_with_PER</id><content type="html" xml:base="http://localhost:4000/Duel_DDQN_with_PER/">&lt;p&gt;A &lt;strong&gt;Dueling Double Deep Q Network with Priority Experience Replay (Duel DDQN with PER)&lt;/strong&gt; implementation in tensorflow.&lt;/p&gt;

&lt;p&gt;Environment from OpenAI’s gym: CartPole-v0&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ChuaCheowHuan/reinforcement_learning/tree/master/DQN_variants/duel_DDQN_PER&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Notations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Model network = &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; &lt;br /&gt;
Model parameter = &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; &lt;br /&gt;
Model network Q value = &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; (s, a) &lt;br /&gt;
&lt;br /&gt;
Target network = &lt;script type=&quot;math/tex&quot;&gt;Q_{\phi}&lt;/script&gt; &lt;br /&gt;
Target parameter = &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; &lt;br /&gt;
Target network Q value = &lt;script type=&quot;math/tex&quot;&gt;Q_{\phi}&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;s^{'}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;a^{'}&lt;/script&gt;) &lt;br /&gt;
&lt;br /&gt;
A small constant to ensure that no sample has 0 probability to be selected = e&lt;/p&gt;

&lt;p&gt;Hyper parameter  = &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Decides how to sample, range from 0 to 1, where 0 corresponds to fully
uniformly random sample selection &amp;amp; 1 corresponding to selecting samples based
on highest priority.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Hyper parameter  = &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Starts close to 0, gradually annealed  to 1, slowly giving more importance to weights during training.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Minibatch size = k &lt;br /&gt;
Replay memory size = N&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Equations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;TD target = r (s, a) &lt;script type=&quot;math/tex&quot;&gt;+&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\phi}&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;s^{'}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;argmax_{a^{'}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; (s&lt;script type=&quot;math/tex&quot;&gt;^{'}&lt;/script&gt;, a&lt;script type=&quot;math/tex&quot;&gt;^{'}&lt;/script&gt;)) &lt;br /&gt;
&lt;br /&gt;
TD  error = &lt;script type=&quot;math/tex&quot;&gt;{\delta}&lt;/script&gt; &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\hspace{26pt}&lt;/script&gt;
= (TD target) &lt;script type=&quot;math/tex&quot;&gt;-&lt;/script&gt; (Model network Q value) &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\hspace{26pt}&lt;/script&gt;
= [r (s, a) &lt;script type=&quot;math/tex&quot;&gt;+&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\phi}&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;s^{'}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;argmax_{a^{'}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; (s&lt;script type=&quot;math/tex&quot;&gt;^{'}&lt;/script&gt;, a&lt;script type=&quot;math/tex&quot;&gt;^{'}&lt;/script&gt;))] &lt;script type=&quot;math/tex&quot;&gt;-&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; (s, a) &lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;priority_{i}&lt;/script&gt; = &lt;script type=&quot;math/tex&quot;&gt;p_{i}&lt;/script&gt; &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\hspace{32pt}&lt;/script&gt;
= &lt;script type=&quot;math/tex&quot;&gt;{|\delta_{i}|}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;+&lt;/script&gt; e &lt;br /&gt;
&lt;br /&gt;
probability(i) = P(i) &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\hspace{41pt}&lt;/script&gt;
= &lt;script type=&quot;math/tex&quot;&gt;\frac{p_{i}^{\alpha}}  {\sum_{k}p_{k}^{\alpha}}&lt;/script&gt; &lt;br /&gt;
&lt;br /&gt;
weights = &lt;script type=&quot;math/tex&quot;&gt;w_{i}&lt;/script&gt; = (N &lt;script type=&quot;math/tex&quot;&gt;\cdot&lt;/script&gt; P(i)) &lt;script type=&quot;math/tex&quot;&gt;^{-\beta}&lt;/script&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Implementation details&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Sum tree:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Assume an example of a sum tree with 7 nodes (with 4 leaves which corresponds to the replay memory size):&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;At initialization:&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1-quXFm1UnNnaThHxhaMoYl5RTAJnJUVI&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;When item 1 is added:&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1Jk-RO9Yqeq2DQKO1CKD9e_KQTxWgtMOu&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;When item 2 is added:&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1fTopGfDSeQj3uEKZPlo_2KSTWaBHrFfK&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;When item 3 is added:&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1d37aBtukIExVU7k84XjUPPphiFJlKXBZ&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;When item 4 is added:&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1V7B3vODsz2ELpW5--oQPh1vxmPMLYxOz&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;When item 5 is added:&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1KBPd61jU4nNug7b475gbKLe5sBJhC_l-&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Figure below shows the corresponding code &amp;amp; array contents. The tree represents the entire sum tree while data represents the leaves.&lt;/p&gt;
  &lt;blockquote&gt;
    &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1kk60DiIQOEkR03wakk2Qwyj2xcK7ac3k&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;In the implementation, only one sumTree object is needed to store the collected experiences, this sumTree object resides in the Replay_memory class. The sumTree object has number of leaves = replay memory size = capacity.
The data array in sumTree object stores an Exp object, which is a sample of experience.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;
The following code decides how to sample:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1KlrlE3KANwmO56vtUdgAI6djfWcDfgWf&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Refer to appendix B.2.1, under the section, “Proportional prioritization”, from the original (Schaul et al., 2016) &lt;a href=&quot;https://arxiv.org/pdf/1511.05952.pdf&quot;&gt;paper&lt;/a&gt; for sampling details.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1511.05952.pdf&quot;&gt;Prioritized experience replay (Schaul et al., 2016)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">A Dueling Double Deep Q Network with Priority Experience Replay (Duel DDQN with PER) implementation in tensorflow.</summary></entry><entry><title type="html">Dueling DDQN</title><link href="http://localhost:4000/Duel_DDQN/" rel="alternate" type="text/html" title="Dueling DDQN" /><published>2019-03-10T00:00:00+08:00</published><updated>2019-03-10T00:00:00+08:00</updated><id>http://localhost:4000/Duel_DDQN</id><content type="html" xml:base="http://localhost:4000/Duel_DDQN/">&lt;p&gt;A &lt;strong&gt;Dueling Double Deep Q Network (Dueling DDQN)&lt;/strong&gt; implementation in tensorflow with random experience replay.&lt;/p&gt;

&lt;p&gt;Environment from OpenAI’s gym: CartPole-v0&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ChuaCheowHuan/reinforcement_learning/tree/master/DQN_variants/duel_DDQN&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Notations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Network = &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; &lt;br /&gt;
Parameter = &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; &lt;br /&gt;
Network Q value = &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; (s, a) &lt;br /&gt;
&lt;br /&gt;
Value function = V(s) &lt;br /&gt;
Advantage function = A(s, a) &lt;br /&gt;
&lt;br /&gt;
Parameter from the Advantage function layer = &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; &lt;br /&gt;
Parameter from the Value function layer = &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Equations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(eqn 9) from the original paper &lt;a href=&quot;https://arxiv.org/pdf/1511.06581.pdf&quot;&gt;(Wang et al., 2015)&lt;/a&gt;: &lt;br /&gt;
Q(s, a; &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;) =
V(s; &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;)
&lt;script type=&quot;math/tex&quot;&gt;+&lt;/script&gt; &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\hspace{50pt}&lt;/script&gt;
[ A(s, a; &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;)
&lt;script type=&quot;math/tex&quot;&gt;-&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\frac{1}{|A|}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;\sum_{a'}&lt;/script&gt; A(s, &lt;script type=&quot;math/tex&quot;&gt;a^{'}&lt;/script&gt;; &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;) ]&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Implementation details&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;V represents the value function layer, A represents the Advantage function layer:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1f901lKe-Fa_Y4ITX8NFNeMO7IX_O2fB9&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1511.06581.pdf&quot;&gt;Dueling Network Architectures for Deep Reinforcement Learning
(Wang et al., 2015)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">A Dueling Double Deep Q Network (Dueling DDQN) implementation in tensorflow with random experience replay.</summary></entry><entry><title type="html">DDQN</title><link href="http://localhost:4000/DDQN/" rel="alternate" type="text/html" title="DDQN" /><published>2019-03-07T00:00:00+08:00</published><updated>2019-03-07T00:00:00+08:00</updated><id>http://localhost:4000/DDQN</id><content type="html" xml:base="http://localhost:4000/DDQN/">&lt;p&gt;A &lt;strong&gt;Double Deep Q Network (DDQN)&lt;/strong&gt; implementation in tensorflow with random experience replay.&lt;/p&gt;

&lt;p&gt;Environment from OpenAI’s gym: CartPole-v0&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ChuaCheowHuan/reinforcement_learning/tree/master/DQN_variants/DDQN&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Notations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Model network = &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; &lt;br /&gt;
Model parameter = &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; &lt;br /&gt;
Model network Q value = &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; (s, a) &lt;br /&gt;
&lt;br /&gt;
Target network = &lt;script type=&quot;math/tex&quot;&gt;Q_{\phi}&lt;/script&gt; &lt;br /&gt;
Target parameter = &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; &lt;br /&gt;
Target network Q value = &lt;script type=&quot;math/tex&quot;&gt;Q_{\phi}&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;s^{'}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;a^{'}&lt;/script&gt;)&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Equations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;TD target = r (s, a) &lt;script type=&quot;math/tex&quot;&gt;+&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\phi}&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;s^{'}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;argmax_{a^{'}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; (s&lt;script type=&quot;math/tex&quot;&gt;^{'}&lt;/script&gt;, a&lt;script type=&quot;math/tex&quot;&gt;^{'}&lt;/script&gt;)) &lt;br /&gt;
&lt;br /&gt;
TD  error = (TD target) &lt;script type=&quot;math/tex&quot;&gt;-&lt;/script&gt; (Model network Q value) &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\hspace{26pt}&lt;/script&gt;
= [r (s, a) &lt;script type=&quot;math/tex&quot;&gt;+&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\phi}&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;s^{'}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;argmax_{a^{'}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; (s&lt;script type=&quot;math/tex&quot;&gt;^{'}&lt;/script&gt;, a&lt;script type=&quot;math/tex&quot;&gt;^{'}&lt;/script&gt;))] &lt;script type=&quot;math/tex&quot;&gt;-&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; (s, a)&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Implementation details&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Create a placeholder to feed Q values from model network:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1CcZVw82JRQRWYmTFFN9PvLKjd4b5BOAF&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Select Q values from model network using &lt;script type=&quot;math/tex&quot;&gt;s^{'}&lt;/script&gt; as features &amp;amp; feed them to the training session:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=15uOc3uOz83V76X5s3PmgzzVWYJkkwR0Z&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Select minibatch actions with largest Q values from model network, create indices &amp;amp; select corresponding minibatch actions from target network:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1YelpKjS68nPBWtg8oeLiZV4mpzkmTPT_&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1509.06461.pdf&quot;&gt;Deep Reinforcement Learning with Double Q-learning
(Hasselt, Guez &amp;amp; Silver, 2016)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">A Double Deep Q Network (DDQN) implementation in tensorflow with random experience replay.</summary></entry><entry><title type="html">DQN</title><link href="http://localhost:4000/DQN/" rel="alternate" type="text/html" title="DQN" /><published>2019-03-01T00:00:00+08:00</published><updated>2019-03-01T00:00:00+08:00</updated><id>http://localhost:4000/DQN</id><content type="html" xml:base="http://localhost:4000/DQN/">&lt;p&gt;A &lt;strong&gt;Deep Q Network&lt;/strong&gt; implementation in tensorflow with target network &amp;amp; random experience replay.&lt;/p&gt;

&lt;p&gt;Environment from OpenAI’s gym: CartPole-v0&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ChuaCheowHuan/reinforcement_learning/tree/master/DQN_variants/DQN&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Notations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Model network = &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; &lt;br /&gt;
Model parameter = &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; &lt;br /&gt;
Model network Q value = &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; (s, a) &lt;br /&gt;
&lt;br /&gt;
Target network = &lt;script type=&quot;math/tex&quot;&gt;Q_{\phi}&lt;/script&gt; &lt;br /&gt;
Target parameter = &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; &lt;br /&gt;
Target network Q value = &lt;script type=&quot;math/tex&quot;&gt;Q_{\phi}&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;s^{'}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;a^{'}&lt;/script&gt;)&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Equations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;TD target = r (s, a) &lt;script type=&quot;math/tex&quot;&gt;+&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;max_{a}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\phi}&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;s^{'}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;a^{'}&lt;/script&gt;) &lt;br /&gt;
&lt;br /&gt;
TD  error = (TD target) &lt;script type=&quot;math/tex&quot;&gt;-&lt;/script&gt; (Model network Q value) &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\hspace{26pt}&lt;/script&gt;
= [r (s, a) &lt;script type=&quot;math/tex&quot;&gt;+&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;max_{a^{'}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\phi}&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;s^{'}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;a^{'}&lt;/script&gt;)] &lt;script type=&quot;math/tex&quot;&gt;-&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; (s, a)&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Implementation details&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Update target parameter &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; with model parameter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; :&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Copy &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; with &lt;em&gt;either&lt;/em&gt; soft or hard parameter update.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Hard parameter update:&lt;/p&gt;
  &lt;blockquote&gt;
    &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=18CK3rHYEfDxVtxe1gnVn2Z10Dosrmrww&quot; alt=&quot;alt text&quot; /&gt;
&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1lNBR6BxZZfk_uGkDSOumUm9qntiJ5QhH&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Soft parameter update:&lt;/p&gt;
  &lt;blockquote&gt;
    &lt;p&gt;polyak &lt;script type=&quot;math/tex&quot;&gt;\cdot&lt;/script&gt;  &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; + (1 &lt;script type=&quot;math/tex&quot;&gt;-&lt;/script&gt; polyak)  &lt;script type=&quot;math/tex&quot;&gt;\cdot&lt;/script&gt;  &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1OfxkRAMve0liZ3BlkS4pCoJ6CPPEjwQG&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;Stop TD target from contributing to gradient computation:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1sw1WtddZn4t48QJhz_LMTthIPhOc4jtl&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf&quot;&gt;Human-level control through deep reinforcement learning
(Mnih et al., 2015)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">A Deep Q Network implementation in tensorflow with target network &amp;amp; random experience replay.</summary></entry></feed>