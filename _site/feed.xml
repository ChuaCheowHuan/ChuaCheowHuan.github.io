<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-06-17T02:02:55+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Chua Cheow Huan</title><subtitle></subtitle><author><name>Chua Cheow Huan</name></author><entry><title type="html">A3C multi-threaded continuous version with N step targets</title><link href="http://localhost:4000/A3C_cont_thread_nStep/" rel="alternate" type="text/html" title="A3C multi-threaded continuous version with N step targets" /><published>2019-06-14T00:00:00+08:00</published><updated>2019-06-14T00:00:00+08:00</updated><id>http://localhost:4000/A3C_cont_thread_nStep</id><content type="html" xml:base="http://localhost:4000/A3C_cont_thread_nStep/">&lt;p&gt;pending update…&lt;/p&gt;

&lt;p&gt;This post demonstrates how to implement the A3C (Asynchronous Advantage Actor Critic) algorithm with Tensorflow. This is a multi-threaded continuous version.&lt;/p&gt;

&lt;p&gt;Environment from OpenAI’s gym: Pendulum-v0 (Discrete)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://&quot;&gt;Full code&lt;/a&gt;: A3C (discrete) multi-threaded version with N-step targets(missing terms are treated as 0)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://&quot;&gt;Full code&lt;/a&gt;: A3C (discrete) multi-threaded version with N-step
targets(use maximum terms possible)&lt;/p&gt;

&lt;p&gt;The majority of the code is very similar to the discrete version with the exceptions highlighted in the following sections:&lt;/p&gt;

&lt;p&gt;Action selection:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with tf.name_scope('select_action'):
    #mean = mean * action_bound[1]                   
    mean = mean * ( action_bound[1] - action_bound[0] ) / 2
    sigma += 1e-4
    normal_dist = tf.distributions.Normal(mean, sigma)                     
    self.choose_a = tf.clip_by_value(tf.squeeze(normal_dist.sample(1), axis=[0, 1]), action_bound[0], action_bound[1])                  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Loss function of the actor network:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with tf.name_scope('actor_loss'):
    log_prob = normal_dist.log_prob(self.a)
    #actor_component = log_prob * tf.stop_gradient(TD_err)
    actor_component = log_prob * tf.stop_gradient(self.baselined_returns)
    entropy = -tf.reduce_mean(normal_dist.entropy()) # Compute the differential entropy of the multivariate normal.                   
    self.actor_loss = -tf.reduce_mean( ENTROPY_BETA * entropy + actor_component)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following code segment creates a LSTM layer:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def _lstm(self, Inputs, cell_size):
        # [time_step, feature] =&amp;gt; [time_step, batch, feature]
        s = tf.expand_dims(Inputs, axis=1, name='time_major')  
        lstm_cell = tf.nn.rnn_cell.LSTMCell(cell_size)
        self.init_state = lstm_cell.zero_state(batch_size=1, dtype=tf.float32)
        outputs, self.final_state = tf.nn.dynamic_rnn(cell=lstm_cell, inputs=s, initial_state=self.init_state, time_major=True)
        # joined state representation          
        lstm_out = tf.reshape(outputs, [-1, cell_size], name='flatten_rnn_outputs')  
        return lstm_out
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following function in the ACNet class creates the actor and critic’s neural networks(note that the critic’s network contains a LSTM layer):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def _create_net(self, scope):
    w_init = tf.glorot_uniform_initializer()
    #w_init = tf.random_normal_initializer(0., .1)
    with tf.variable_scope('actor'):                        
        hidden = tf.layers.dense(self.s, actor_hidden, tf.nn.relu6, kernel_initializer=w_init, name='hidden')            
        #lstm_out = self._lstm(hidden, cell_size)
        # tanh range = [-1,1]
        mean = tf.layers.dense(hidden, num_actions, tf.nn.tanh, kernel_initializer=w_init, name='mean')
        # softplus range = {0,inf}
        sigma = tf.layers.dense(hidden, num_actions, tf.nn.softplus, kernel_initializer=w_init, name='sigma')
    with tf.variable_scope('critic'):
        hidden = tf.layers.dense(self.s, critic_hidden, tf.nn.relu6, kernel_initializer=w_init, name='hidden')
        lstm_out = self._lstm(hidden, cell_size)
        V = tf.layers.dense(lstm_out, 1, kernel_initializer=w_init, name='V')  
    actor_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')
    critic_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')
    return mean, sigma, V, actor_params, critic_params
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Huan</name></author><summary type="html">pending update…</summary></entry><entry><title type="html">A3C multi-threaded discrete version with N step targets</title><link href="http://localhost:4000/A3C_disc_thread_nStep/" rel="alternate" type="text/html" title="A3C multi-threaded discrete version with N step targets" /><published>2019-06-13T00:00:00+08:00</published><updated>2019-06-13T00:00:00+08:00</updated><id>http://localhost:4000/A3C_disc_thread_nStep</id><content type="html" xml:base="http://localhost:4000/A3C_disc_thread_nStep/">&lt;p&gt;pending update…&lt;/p&gt;

&lt;p&gt;This post demonstrates how to implement the A3C (Asynchronous Advantage Actor Critic) algorithm with Tensorflow. This is a multi-threaded discrete version.&lt;/p&gt;

&lt;p&gt;Environment from OpenAI’s gym: CartPole-v0 (Discrete)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://&quot;&gt;Full code&lt;/a&gt;: A3C (discrete) multi-threaded version with N-step targets(missing terms are treated as 0)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://&quot;&gt;Full code&lt;/a&gt;: A3C (discrete) multi-threaded version with N-step
targets(use maximum terms possible)&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Notations:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Actor network = &lt;script type=&quot;math/tex&quot;&gt;{\pi}_{\theta}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Actor network parameter = &lt;script type=&quot;math/tex&quot;&gt;{\theta}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Critic network = &lt;script type=&quot;math/tex&quot;&gt;V_{\phi}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Critic network parameter = &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Advantage function = A&lt;/p&gt;

&lt;p&gt;Number of trajectories = m&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Equations:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Actor component: log&lt;script type=&quot;math/tex&quot;&gt;{\pi}_{\theta}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;(a_{t} {\mid} s_{t})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Critic component = Advantage function = A = &lt;script type=&quot;math/tex&quot;&gt;Q(s_{t}, a_{t})&lt;/script&gt; - &lt;script type=&quot;math/tex&quot;&gt;V_{\phi}(s_{t})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Q values with N-step truncated estimate :&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Q^{\pi}(s_{t}, a_{t})&lt;/script&gt; = E(&lt;script type=&quot;math/tex&quot;&gt;r_{t}&lt;/script&gt; + &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;r_{t+1}&lt;/script&gt; + &lt;script type=&quot;math/tex&quot;&gt;\gamma^{2}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;r_{t+2}&lt;/script&gt; + … + &lt;script type=&quot;math/tex&quot;&gt;\gamma^{n}&lt;/script&gt; V(&lt;script type=&quot;math/tex&quot;&gt;s_{t+n}&lt;/script&gt;))&lt;/p&gt;

&lt;p&gt;Check this &lt;a href=&quot;https://chuacheowhuan.github.io/n_step_targets/&quot;&gt;post&lt;/a&gt; for more information on N-step truncated estimate.&lt;/p&gt;

&lt;p&gt;Policy gradient estimator&lt;/p&gt;

&lt;p&gt;= &lt;script type=&quot;math/tex&quot;&gt;\nabla_\theta J(\theta)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;= &lt;script type=&quot;math/tex&quot;&gt;{\dfrac{1}{m}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;{\sum\limits_{i=1}^{m}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;{\sum\limits_{t=0}^{T}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;\nabla_\theta&lt;/script&gt; log&lt;script type=&quot;math/tex&quot;&gt;{\pi}_{\theta}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;(a_{t} {\mid} s_{t})&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q(s_{t}, a_{t})&lt;/script&gt; - &lt;script type=&quot;math/tex&quot;&gt;V_{\phi}(s_{t})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;= &lt;script type=&quot;math/tex&quot;&gt;{\dfrac{1}{m}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;{\sum\limits_{i=1}^{m}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;{\sum\limits_{t=0}^{T}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;\nabla_\theta&lt;/script&gt; log&lt;script type=&quot;math/tex&quot;&gt;{\pi}_{\theta}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;(a_{t} {\mid} s_{t})&lt;/script&gt; A&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Implementation details:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The ACNet class defines the models (Tensorflow graphs) and contains both the actor and the critic networks.&lt;/p&gt;

&lt;p&gt;The Worker class contains the work function that does the main bulk of the computation.&lt;/p&gt;

&lt;p&gt;A copy of ACNet is declared globally &amp;amp; the parameters are shared by the threaded workers. Each worker also have it’s own local copy of ACNet.&lt;/p&gt;

&lt;p&gt;Workers are instantiated &amp;amp; threaded in the main program.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ACNet class:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Loss function for the actor network for the discrete environment:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with tf.name_scope('actor_loss'):
    log_prob = tf.reduce_sum(tf.log(self.action_prob + 1e-5) * tf.one_hot(self.a, num_actions, dtype=tf.float32), axis=1, keep_dims=True)
    actor_component = log_prob * tf.stop_gradient(self.baselined_returns)
    # entropy for exploration
    entropy = -tf.reduce_sum(self.action_prob * tf.log(self.action_prob + 1e-5), axis=1, keep_dims=True)  # encourage exploration
    self.actor_loss = tf.reduce_mean( -(ENTROPY_BETA * entropy + actor_component) )                                        
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Loss function for the critic network for the discrete environment:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;TD_err = tf.subtract(self.critic_target, self.V, name='TD_err')
      .
      .
      .
with tf.name_scope('critic_loss'):
    self.critic_loss = tf.reduce_mean(tf.square(TD_err))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following function in the ACNet class creates the actor and critic’s neural networks:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def _create_net(self, scope):
    w_init = tf.glorot_uniform_initializer()
    with tf.variable_scope('actor'):
        hidden = tf.layers.dense(self.s, actor_hidden, tf.nn.relu6, kernel_initializer=w_init, name='hidden')
        action_prob = tf.layers.dense(hidden, num_actions, tf.nn.softmax, kernel_initializer=w_init, name='action_prob')        
    with tf.variable_scope('critic'):
        hidden = tf.layers.dense(self.s, critic_hidden, tf.nn.relu6, kernel_initializer=w_init, name='hidden')
        V = tf.layers.dense(hidden, 1, kernel_initializer=w_init, name='V')         
    actor_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')
    critic_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')       
    return action_prob, V, actor_params, critic_params
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Worker class:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Discounted rewards are used as critic’s targets:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;critic_target = self.discount_rewards(buffer_r, GAMMA, V_s)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;N-step targets are used in the computation of the Advantage function(baselined_returns):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Advantage function
baselined_returns = n_step_targets - baseline
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2 versions of N-step targets could be used:&lt;/p&gt;

&lt;p&gt;Version 1) missing terms are treated as 0.&lt;/p&gt;

&lt;p&gt;Version 2) use maximum terms possible.&lt;/p&gt;

&lt;p&gt;Check this &lt;a href=&quot;https://chuacheowhuan.github.io/n_step_targets/&quot;&gt;post&lt;/a&gt; for more information on N-step targets.&lt;/p&gt;

&lt;p&gt;The following code segment accumulates gradients &amp;amp; apply them to the local critic network:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.AC.accumu_grad_critic(feed_dict) # accumulating gradients for local critic  
self.AC.apply_accumu_grad_critic(feed_dict)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following code segment computes the advantage function(baselined_returns):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;baseline = SESS.run(self.AC.V, {self.AC.s: buffer_s}) # Value function
epr = np.vstack(buffer_r).astype(np.float32)
n_step_targets = self.compute_n_step_targets_missing(epr, baseline, GAMMA, N_step) # Q values
# Advantage function
baselined_returns = n_step_targets - baseline
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following code segment accumulates gradients for the local actor network:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.AC.accumu_grad_actor(feed_dict) # accumulating gradients for local actor  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following code segment push the parameters from the local networks to the global networks and then pulls the updated global parameters to the local networks:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# update
self.AC.push_global_actor(feed_dict)                
self.AC.push_global_critic(feed_dict)
    .
    .
    .
self.AC.pull_global()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following code segment initialize storage for accumulated local gradients.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.AC.init_grad_storage_actor() # initialize storage for accumulated gradients.
self.AC.init_grad_storage_critic()            
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Check this &lt;a href=&quot;https://chuacheowhuan.github.io/tf_accumulate_grad/&quot;&gt;post&lt;/a&gt; for more information on how to accumulate gradients in Tensorflow.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Main program:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The following code segment creates the workers:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;workers = []
for i in range(num_workers): # Create worker
    i_name = 'W_%i' % i # worker name
    workers.append(Worker(i_name, GLOBAL_AC))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following code segment threads the workers:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;worker_threads = []
for worker in workers:
    job = lambda: worker.work()
    t = threading.Thread(target=job)
    t.start()
    worker_threads.append(t)
COORD.join(worker_threads)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1602.01783.pdf&quot;&gt;Asynchronous Methods for Deep Reinforcement Learning
(Mnih, Badia, Mirza, Graves, Harley, Lillicrap, et al., 2016)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">pending update…</summary></entry><entry><title type="html">Accumulate gradients with Tensorflow</title><link href="http://localhost:4000/tf_accumulate_grad/" rel="alternate" type="text/html" title="Accumulate gradients with Tensorflow" /><published>2019-06-11T00:00:00+08:00</published><updated>2019-06-11T00:00:00+08:00</updated><id>http://localhost:4000/tf_accumulate_grad</id><content type="html" xml:base="http://localhost:4000/tf_accumulate_grad/">&lt;p&gt;This post demonstrates how to accumulate gradients with Tensorflow.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ChuaCheowHuan/misc_code_examples/blob/master/tf/tf_accumulate_grad.ipynb&quot;&gt;Full code&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import tensorflow as tf

def accumu_grad(self, OPT, loss, scope):
    # retrieve trainable variables in scope of graph
    #tvs = tf.trainable_variables(scope=scope + '/actor')
    tvs = tf.trainable_variables(scope=scope)

    # ceate a list of variables with the same shape as the trainable
    accumu = [tf.Variable(tf.zeros_like(tv.initialized_value()), trainable=False) for tv in tvs]

    zero_op = [tv.assign(tf.zeros_like(tv)) for tv in accumu] # initialized with 0s

    gvs = OPT.compute_gradients(loss, tvs) # obtain list of gradients &amp;amp; variables
    #gvs = [(tf.where( tf.is_nan(grad), tf.zeros_like(grad), grad ), var) for grad, var in gvs]

    # adds to each element from the list you initialized earlier with zeros its gradient
    # accumu and gvs are in same shape, index 0 is grads, index 1 is vars
    accumu_op = [accumu[i].assign_add(gv[0]) for i, gv in enumerate(gvs)]

    apply_op = OPT.apply_gradients([(accumu[i], gv[1]) for i, gv in enumerate(gvs)]) # apply grads

    return zero_op, accumu_op, apply_op, accumu                
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Huan</name></author><summary type="html">This post demonstrates how to accumulate gradients with Tensorflow.</summary></entry><entry><title type="html">Distributed Tensorflow</title><link href="http://localhost:4000/dist_tf/" rel="alternate" type="text/html" title="Distributed Tensorflow" /><published>2019-06-09T00:00:00+08:00</published><updated>2019-06-09T00:00:00+08:00</updated><id>http://localhost:4000/dist_tf</id><content type="html" xml:base="http://localhost:4000/dist_tf/">&lt;p&gt;Distributed Tensorflow with Python multiprocessing package.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ChuaCheowHuan/misc_code_examples/blob/master/tf/dist_tf/dist_tf.ipynb&quot;&gt;Full code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This post demonstrates how to use distributed Tensorflow with Python’s
multiprocessing package. A tf.FIFOQueue is used as a storage across processes.&lt;/p&gt;

&lt;p&gt;Cluster definition:&lt;/p&gt;

&lt;p&gt;2 workers, 1 parameter server.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cluster = tf.train.ClusterSpec({
    &quot;worker&quot;: [&quot;localhost:2223&quot;,
               &quot;localhost:2224&quot;
              ],
    &quot;ps&quot;: [&quot;localhost:2225&quot;]
})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Parameter server function:&lt;/p&gt;

&lt;p&gt;A tf.Variable (var) &amp;amp; a tf.FIFOQueue (q) is declared with the parameter server.
They are both sharable across processes.&lt;/p&gt;

&lt;p&gt;For tf.FIFOQueue to be sharable, it has to be declared with the same device
(in this case, the ps device) in both the parameter_server function and
the worker function. A shared_name has to be given as well.&lt;/p&gt;

&lt;p&gt;The tf.Variable (var) is also declared under the ps device. The value of var is displayed in the first for loop.&lt;/p&gt;

&lt;p&gt;At the end of the function, the values stored in q will be displayed in the last for loop.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def parameter_server():
    with tf.device(&quot;/job:ps/task:0&quot;):
        var = tf.Variable(0.0, name='var')        
        q = tf.FIFOQueue(10, tf.float32, shared_name=&quot;shared_queue&quot;)

    server = tf.train.Server(cluster,
                             job_name=&quot;ps&quot;,
                             task_index=0)
    sess = tf.Session(target=server.target)

    print(&quot;Parameter server: waiting for cluster connection...&quot;)
    sess.run(tf.report_uninitialized_variables())
    print(&quot;Parameter server: cluster ready!&quot;)

    print(&quot;Parameter server: initializing variables...&quot;)
    sess.run(tf.global_variables_initializer())
    print(&quot;Parameter server: variables initialized&quot;)

    for i in range(10):
        print(&quot;Parameter server: var has value %.1f&quot; % sess.run(var))
        sleep(1.0)
        if sess.run(var) == 10.0:
          break

    sleep(3.0)
    print(&quot;ps q.size(): &quot;, sess.run(q.size()))  

    for j in range(sess.run(q.size())):
        print(&quot;ps: r&quot;, sess.run(q.dequeue()))

    #print(&quot;Parameter server: blocking...&quot;)
    #server.join() # currently blocks forever    
    print(&quot;Parameter server: ended...&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Worker function:&lt;/p&gt;

&lt;p&gt;tf.FIFOQueue (q) is declared with the ps device. A same shared_name is also
used.&lt;/p&gt;

&lt;p&gt;The tf.Variable (var) is declared under the worker device. It does not have to
be declared under the ps device.&lt;/p&gt;

&lt;p&gt;The for loop increments the value of var and the values are stored in q.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def worker(worker_n):
    with tf.device(&quot;/job:ps/task:0&quot;):
        q = tf.FIFOQueue(10, tf.float32, shared_name=&quot;shared_queue&quot;)     
    with tf.device(tf.train.replica_device_setter(
                        worker_device='/job:worker/task:' + str(worker_n),
                        cluster=cluster)):
        var = tf.Variable(0.0, name='var')

    server = tf.train.Server(cluster,
                             job_name=&quot;worker&quot;,
                             task_index=worker_n)
    sess = tf.Session(target=server.target)

    print(&quot;Worker %d: waiting for cluster connection...&quot; % worker_n)
    sess.run(tf.report_uninitialized_variables())
    print(&quot;Worker %d: cluster ready!&quot; % worker_n)

    while sess.run(tf.report_uninitialized_variables()):
        print(&quot;Worker %d: waiting for variable initialization...&quot; % worker_n)
        sleep(1.0)
    print(&quot;Worker %d: variables initialized&quot; % worker_n)

    for i in range(5):
        print(&quot;Worker %d: incrementing var&quot; % worker_n, sess.run(var))
        sess.run(var.assign_add(1.0))
        qe = q.enqueue(sess.run(var))
        sess.run(qe)
        sleep(1.0)

    print(&quot;Worker %d: ended...&quot; % worker_n)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Main program:&lt;/p&gt;

&lt;p&gt;Create the processes, run them and finally terminate them in a for loop.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ps_proc = Process(target=parameter_server, daemon=True)
w1_proc = Process(target=worker, args=(0, ), daemon=True)
w2_proc = Process(target=worker, args=(1, ), daemon=True)

ps_proc.start()
w1_proc.start()
w2_proc.start()

ps_proc.join() # only ps need to call join()

for proc in [w1_proc, w2_proc, ps_proc]:
    proc.terminate() # only way to kill server is to kill it's process

print('All done.')            
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Output:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Parameter server: waiting for cluster connection...
Worker 0: waiting for cluster connection...
Worker 1: waiting for cluster connection...
Worker 1: cluster ready!
Worker 1: waiting for variable initialization...
Parameter server: cluster ready!
Parameter server: initializing variables...
Parameter server: variables initialized
Parameter server: var has value 0.0
Worker 0: cluster ready!
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:65: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size &amp;gt; 0` to check that an array is not empty.
Worker 0: variables initialized
Worker 0: incrementing var 0.0
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:65: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size &amp;gt; 0` to check that an array is not empty.
Worker 1: variables initialized
Worker 1: incrementing var 1.0
Parameter server: var has value 2.0
Worker 0: incrementing var 2.0
Worker 1: incrementing var 3.0
Parameter server: var has value 4.0
Worker 0: incrementing var 4.0
Worker 1: incrementing var 5.0
Parameter server: var has value 6.0
Worker 0: incrementing var 6.0
Worker 1: incrementing var 7.0
Parameter server: var has value 8.0
Worker 0: incrementing var 8.0
Worker 1: incrementing var 9.0
Worker 0: ended...
Worker 1: ended...
ps q.size():  10
ps: r 1.0
ps: r 2.0
ps: r 3.0
ps: r 4.0
ps: r 5.0
ps: r 6.0
ps: r 7.0
ps: r 8.0
ps: r 9.0
ps: r 10.0
Parameter server: ended...
All done.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Huan</name></author><summary type="html">Distributed Tensorflow with Python multiprocessing package.</summary></entry><entry><title type="html">N-step targets</title><link href="http://localhost:4000/n_step_targets/" rel="alternate" type="text/html" title="N-step targets" /><published>2019-05-31T00:00:00+08:00</published><updated>2019-05-31T00:00:00+08:00</updated><id>http://localhost:4000/n_step_targets</id><content type="html" xml:base="http://localhost:4000/n_step_targets/">&lt;p&gt;N-step Q-values estimation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ChuaCheowHuan/reinforcement_learning/blob/master/policy_gradient_based/n_step_targets.ipynb&quot;&gt;Full code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The following two functions computes truncated Q-values estimates:&lt;/p&gt;

&lt;p&gt;1) n_step_targets_missing&lt;/p&gt;

&lt;p&gt;treats missing terms as 0.&lt;/p&gt;

&lt;p&gt;2) n_step_targets_max&lt;/p&gt;

&lt;p&gt;use maximum terms possible.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;1-step truncated estimate :&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Q^{\pi}(s_{t}, a_{t})&lt;/script&gt; = E(&lt;script type=&quot;math/tex&quot;&gt;r_{t}&lt;/script&gt; + &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; V(&lt;script type=&quot;math/tex&quot;&gt;s_{t+1}&lt;/script&gt;))&lt;/p&gt;

&lt;p&gt;2-step truncated estimate :&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Q^{\pi}(s_{t}, a_{t})&lt;/script&gt; = E(&lt;script type=&quot;math/tex&quot;&gt;r_{t}&lt;/script&gt; + &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;r_{t+1}&lt;/script&gt; +  &lt;script type=&quot;math/tex&quot;&gt;\gamma^{2}&lt;/script&gt; V(&lt;script type=&quot;math/tex&quot;&gt;s_{t+2}&lt;/script&gt;))&lt;/p&gt;

&lt;p&gt;3-step truncated estimate :&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Q^{\pi}(s_{t}, a_{t})&lt;/script&gt; = E(&lt;script type=&quot;math/tex&quot;&gt;r_{t}&lt;/script&gt; + &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;r_{t+1}&lt;/script&gt; + &lt;script type=&quot;math/tex&quot;&gt;\gamma^{2}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;r_{t+2}&lt;/script&gt; + &lt;script type=&quot;math/tex&quot;&gt;\gamma^{3}&lt;/script&gt; V(&lt;script type=&quot;math/tex&quot;&gt;s_{t+3}&lt;/script&gt;))&lt;/p&gt;

&lt;p&gt;N-step truncated estimate :&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Q^{\pi}(s_{t}, a_{t})&lt;/script&gt; = E(&lt;script type=&quot;math/tex&quot;&gt;r_{t}&lt;/script&gt; + &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;r_{t+1}&lt;/script&gt; + &lt;script type=&quot;math/tex&quot;&gt;\gamma^{2}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;r_{t+2}&lt;/script&gt; + … + &lt;script type=&quot;math/tex&quot;&gt;\gamma^{n}&lt;/script&gt; V(&lt;script type=&quot;math/tex&quot;&gt;s_{t+n}&lt;/script&gt;))&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Assuming we have the following variables setup:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;N=2 # N steps
gamma=2
t=5
v_s_ = 10 # value of next state

epr=np.arange(t).reshape(t,1)
print(&quot;epr=&quot;, epr)

baselines=np.arange(t).reshape(t,1)
print(&quot;baselines=&quot;, baselines)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Display output of episodic rewards(epr) &amp;amp; baselines:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;epr= [[0]
 [1]
 [2]
 [3]
 [4]]

baselines= [[0]
 [1]
 [2]
 [3]
 [4]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This function computes the n-step targets, treats missing terms as zero:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# if number of steps unavailable, missing terms treated as 0.
def n_step_targets_missing(epr, baselines, gamma, N):
  N = N+1
  targets = np.zeros_like(epr)    
  if N &amp;gt; epr.size:
    N = epr.size
  for t in range(epr.size):   
    print(&quot;t=&quot;, t)
    for n in range(N):
      print(&quot;n=&quot;, n)
      if t+n == epr.size:            
        print('missing terms treated as 0, break') # last term for those with insufficient steps.
        break # missing terms treated as 0
      if n == N-1: # last term
        targets[t] += (gamma**n) * baselines[t+n] # last term for those with sufficient steps
        print('last term for those with sufficient steps, end inner n loop')
      else:
        targets[t] += (gamma**n) * epr[t+n] # non last terms
  return targets
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run the function n_step_targets_missing:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print('n_step_targets_missing:')
T = n_step_targets_missing(epr, baselines, gamma, N)
print(T)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Display the output:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;n_step_targets_missing:
t= 0
n= 0
n= 1
n= 2
last term for those with sufficient steps, end inner n loop
t= 1
n= 0
n= 1
n= 2
last term for those with sufficient steps, end inner n loop
t= 2
n= 0
n= 1
n= 2
last term for those with sufficient steps, end inner n loop
t= 3
n= 0
n= 1
n= 2
missing terms treated as 0, break
t= 4
n= 0
n= 1
missing terms treated as 0, break
[[10]
 [17]
 [24]
 [11]
 [ 4]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;For the output above, note that when t+n = 5 which is greater than the last index 4, missing terms are treated as 0.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This function computes the n-step targets, it will use maximum number of terms possible:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# if number of steps unavailable, use max steps available.
# uses v_s_ as input
def n_step_targets_max(epr, baselines, v_s_, gamma, N):
  N = N+1
  targets = np.zeros_like(epr)    
  if N &amp;gt; epr.size:
    N = epr.size
  for t in range(epr.size):  
    print(&quot;t=&quot;, t)
    for n in range(N):
      print(&quot;n=&quot;, n)
      if t+n == epr.size:            
        targets[t] += (gamma**n) * v_s_ # last term for those with insufficient steps.
        print('last term for those with INSUFFICIENT steps, break')
        break
      if n == N-1:
        targets[t] += (gamma**n) * baselines[t+n] # last term for those with sufficient steps
        print('last term for those with sufficient steps, end inner n loop')
      else:
        targets[t] += (gamma**n) * epr[t+n] # non last terms
  return targets
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run the function n_step_targets_max:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print('n_step_targets_max:')
T = n_step_targets_max(epr, baselines, v_s_, gamma, N)
print(T)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Display the output:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;n_step_targets_max:
t= 0
n= 0
n= 1
n= 2
last term for those with sufficient steps, end inner n loop
t= 1
n= 0
n= 1
n= 2
last term for those with sufficient steps, end inner n loop
t= 2
n= 0
n= 1
n= 2
last term for those with sufficient steps, end inner n loop
t= 3
n= 0
n= 1
n= 2
last term for those with INSUFFICIENT steps, break
t= 4
n= 0
n= 1
last term for those with INSUFFICIENT steps, break
[[10]
 [17]
 [24]
 [51]
 [24]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;For the output above, note that when t+n = 5 which is greater than the last index 4, maximum terms are used where possible. ( Last term for those with INSUFFICIENT steps is given by (gamma**n) * v_s_ = &lt;script type=&quot;math/tex&quot;&gt;\gamma^{5}&lt;/script&gt; V(&lt;script type=&quot;math/tex&quot;&gt;s_{5}&lt;/script&gt;)), where v_s_ = V(&lt;script type=&quot;math/tex&quot;&gt;s_{5}&lt;/script&gt;)&lt;/p&gt;

&lt;p&gt;t=2, normal 2 steps estimation:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Q^{\pi}(s_{t}, a_{t})&lt;/script&gt; = E(&lt;script type=&quot;math/tex&quot;&gt;r_{2}&lt;/script&gt; + &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;r_{3}&lt;/script&gt; +  &lt;script type=&quot;math/tex&quot;&gt;\gamma^{4}&lt;/script&gt; V(&lt;script type=&quot;math/tex&quot;&gt;s_{4}&lt;/script&gt;))&lt;/p&gt;

&lt;p&gt;t=3, 2 steps estimation with insufficient step, using v_s_ in the last term:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Q^{\pi}(s_{t}, a_{t})&lt;/script&gt; = E(&lt;script type=&quot;math/tex&quot;&gt;r_{3}&lt;/script&gt; + &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;r_{4}&lt;/script&gt; +  &lt;script type=&quot;math/tex&quot;&gt;\gamma^{5}&lt;/script&gt; V(&lt;script type=&quot;math/tex&quot;&gt;s_{5}&lt;/script&gt;))&lt;/p&gt;

&lt;p&gt;t=4, insufficient step for 2 steps estimation, resorting to 1 step estimation:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Q^{\pi}(s_{t}, a_{t})&lt;/script&gt; = E(&lt;script type=&quot;math/tex&quot;&gt;r_{4}&lt;/script&gt; + &lt;script type=&quot;math/tex&quot;&gt;\gamma^{5}&lt;/script&gt; V(&lt;script type=&quot;math/tex&quot;&gt;s_{5}&lt;/script&gt;))&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">N-step Q-values estimation.</summary></entry><entry><title type="html">Python’s multiprocessing package</title><link href="http://localhost:4000/py_mpp/" rel="alternate" type="text/html" title="Python's multiprocessing package" /><published>2019-05-30T00:00:00+08:00</published><updated>2019-05-30T00:00:00+08:00</updated><id>http://localhost:4000/py_mpp</id><content type="html" xml:base="http://localhost:4000/py_mpp/">&lt;p&gt;Python’s multiprocessing package for parallel data generation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ChuaCheowHuan/misc_code_examples/blob/master/py/py_mpp.ipynb&quot;&gt;Full code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This simple example program demonstrates how to use the Python’s multiprocessing package to achieve parallel data generation.&lt;/p&gt;

&lt;p&gt;The main program has a chief that spawns multiple worker processes. Each worker spawns a single work process. The work process generates random integer data [1,3].&lt;/p&gt;

&lt;p&gt;Each worker has it’s own local queue. When data is generated, it is stored in it’s local queue. When the local queue’s size is greater than 5, the data is retrieved &amp;amp; 0.1 is added to the data, this result is stored in the Chief’s global queue. When the Chief’s global queue’s size is greater than 3, the result is retrieved &amp;amp; printed on screen.&lt;/p&gt;

&lt;p&gt;The Worker class:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class Worker(object):
  def __init__(self, worker_id, g_queue):
    self.g_queue = g_queue
    self.worker_id = worker_id
    self.queue = Queue() # local worker queue
    self.work_process = Process(target=self.work, args=())
    self.work_process.start()
    info(worker_id, self.work_process, &quot;Worker&quot;)

  def work(self):

    info(self.worker_id, self.work_process, &quot;work&quot;)

    while True:
      data = np.random.randint(1,4)
      self.queue.put(data)

      # process data in queue
      if self.queue.qsize() &amp;gt; 5:
        data = self.queue.get()
        result = data + 0.1
        self.g_queue.put(result) # send result to global queue

      time.sleep(1) # work every x sec interval

    return self.w_id  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Chief class:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class Chief(object):
  def __init__(self, num_workers):
    self.g_queue = Queue() # global queue    
    self.num_workers = num_workers

  def dispatch_workers(self):   
    worker_processes = [Process(target=Worker(w_id, self.g_queue), args=()) for w_id in range(num_workers)]
    return worker_processes

  def result(self):
    if self.g_queue.qsize() &amp;gt; 3:
      result = self.g_queue.get()
      print(&quot;result&quot;, result)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The main program:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;if __name__ == '__main__':  
  print('main parent process id:', os.getppid())
  print('main process id:', os.getpid())

  num_workers = 2
  chief = Chief(num_workers)
  workers_processes = chief.dispatch_workers()

  i = 0
  while True:    
    time.sleep(2) # chk g_queue every x sec interval to get result
    chief.result()
    print(&quot;i=&quot;, i)

    if i&amp;gt;9:
      break
    i+=1    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A helper display function:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def info(worker_id, process, function_name):
    print(&quot;worker_id=&quot;, worker_id,
          'module name:', __name__,
          'function name:', function_name,
          'parent process:', os.getppid(),
          'current process id:', os.getpid(),
          'spawn process id:', process.pid)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Huan</name></author><summary type="html">Python’s multiprocessing package for parallel data generation.</summary></entry><entry><title type="html">Numpy array manipulation</title><link href="http://localhost:4000/np_array_manipulation/" rel="alternate" type="text/html" title="Numpy array manipulation" /><published>2019-05-29T00:00:00+08:00</published><updated>2019-05-29T00:00:00+08:00</updated><id>http://localhost:4000/np_array_manipulation</id><content type="html" xml:base="http://localhost:4000/np_array_manipulation/">&lt;p&gt;Simple numpy array manipulation examples.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ChuaCheowHuan/misc_code_examples/blob/master/py/np_array_manipulation.ipynb&quot;&gt;Full code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This Jupyter notebook contains simple examples on how to manipulate numpy arrays. The code blocks below shows the codes &amp;amp; it’s corresponding display output.&lt;/p&gt;

&lt;p&gt;Setting up a numpy array:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;buffer=[0,1]
print('buffer=', buffer)
$buffer= [0, 1]

new=2
print('new=', new)
$new= 2

buffer = np.array(buffer + [new]) # append a new item &amp;amp; create a numpy array
print('np.array(buffer + [new])=', buffer)
$np.array(buffer + [new])= [0 1 2]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Slicing examples:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# numpy array slicing syntax
# buffer[start:stop:step]

print('buffer[1:]=', buffer[1:]) # starting from index 1
$buffer[1:]= [1 2]

print('buffer[-1:]=', buffer[-1:]) # getting item in last index
$buffer[-1:]= [2]

print('buffer[:1]=', buffer[:1]) # stop at index 1 (exclusive), keep only 1st item
$buffer[:1]= [0]

print('buffer[:-1]=', buffer[:-1]) # stop at last index (exclusive), discard item in last index
$buffer[:-1]= [0 1]

print('buffer[::-1]=', buffer[::-1]) # start from last index (reversal)
$buffer[::-1]= [2 1 0]

print('buffer[1::-1]=', buffer[1::-1]) # reverse starting from index 1
$buffer[1::-1]= [1 0]

# Starting from index 1 will return [1 2], reversing will return [2,1]
print('buffer[1:][::-1]=', buffer[1:][::-1])
$buffer[1:][::-1]= [2 1]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;np.newaxis is an alias for None:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# np.newaxis = None

print('buffer[:, np.newaxis]=', buffer[:, np.newaxis])
$buffer[:, np.newaxis]= [[0][1][2]]

print('buffer[:, None]=', buffer[:, None])
$buffer[:, None]= [[0][1][2]]

print('buffer[np.newaxis, :]=', buffer[np.newaxis, :])
$buffer[np.newaxis, :]= [[0 1 2]]

print('buffer[None, :]=', buffer[None, :])
$buffer[None, :]= [[0 1 2]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Stacking:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;a = [1,2,3]
b = [4,5,6]
c = [7,8,9]

r = np.hstack((a,b,c)) # horizontal stacking
print(&quot;r=&quot;, r)
$r= [1 2 3 4 5 6 7 8 9]

QUEUE = queue.Queue()
QUEUE.put(a)
QUEUE.put(b)
QUEUE.put(c)

r = [QUEUE.get() for _ in range(QUEUE.qsize())]
print(r)
$[[1, 2, 3], [4, 5, 6], [7, 8, 9]]

r = np.vstack(r) # vertical stacking
print(r)
$[[1 2 3]
  [4 5 6]
  [7 8 9]]

print(r[:, ::-1]) # col reversal
$[[3 2 1]
  [6 5 4]
  [9 8 7]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Huan</name></author><summary type="html">Simple numpy array manipulation examples.</summary></entry><entry><title type="html">Dueling DDQN with PER</title><link href="http://localhost:4000/Duel_DDQN_with_PER/" rel="alternate" type="text/html" title="Dueling DDQN with PER" /><published>2019-03-15T00:00:00+08:00</published><updated>2019-03-15T00:00:00+08:00</updated><id>http://localhost:4000/Duel_DDQN_with_PER</id><content type="html" xml:base="http://localhost:4000/Duel_DDQN_with_PER/">&lt;p&gt;A &lt;strong&gt;Dueling Double Deep Q Network with Priority Experience Replay (Duel DDQN with PER)&lt;/strong&gt; implementation in tensorflow.&lt;/p&gt;

&lt;p&gt;Environment from OpenAI’s gym: CartPole-v0&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ChuaCheowHuan/reinforcement_learning/tree/master/DQN_variants/duel_DDQN_PER&quot;&gt;Full code&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Notations:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Model network = &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; &lt;br /&gt;
Model parameter = &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; &lt;br /&gt;
Model network Q value = &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; (s, a) &lt;br /&gt;
&lt;br /&gt;
Target network = &lt;script type=&quot;math/tex&quot;&gt;Q_{\phi}&lt;/script&gt; &lt;br /&gt;
Target parameter = &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; &lt;br /&gt;
Target network Q value = &lt;script type=&quot;math/tex&quot;&gt;Q_{\phi}&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;s^{'}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;a^{'}&lt;/script&gt;) &lt;br /&gt;
&lt;br /&gt;
A small constant to ensure that no sample has 0 probability to be selected = e&lt;/p&gt;

&lt;p&gt;Hyper parameter  = &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Decides how to sample, range from 0 to 1, where 0 corresponds to fully
uniformly random sample selection &amp;amp; 1 corresponding to selecting samples based
on highest priority.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Hyper parameter  = &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Starts close to 0, gradually annealed  to 1, slowly giving more importance to weights during training.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Minibatch size = k &lt;br /&gt;
Replay memory size = N&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Equations:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;TD target = r (s, a) &lt;script type=&quot;math/tex&quot;&gt;+&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\phi}&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;s^{'}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;argmax_{a^{'}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; (s&lt;script type=&quot;math/tex&quot;&gt;^{'}&lt;/script&gt;, a&lt;script type=&quot;math/tex&quot;&gt;^{'}&lt;/script&gt;)) &lt;br /&gt;
&lt;br /&gt;
TD  error = &lt;script type=&quot;math/tex&quot;&gt;{\delta}&lt;/script&gt; &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\hspace{26pt}&lt;/script&gt;
= (TD target) &lt;script type=&quot;math/tex&quot;&gt;-&lt;/script&gt; (Model network Q value) &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\hspace{26pt}&lt;/script&gt;
= [r (s, a) &lt;script type=&quot;math/tex&quot;&gt;+&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\phi}&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;s^{'}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;argmax_{a^{'}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; (s&lt;script type=&quot;math/tex&quot;&gt;^{'}&lt;/script&gt;, a&lt;script type=&quot;math/tex&quot;&gt;^{'}&lt;/script&gt;))] &lt;script type=&quot;math/tex&quot;&gt;-&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; (s, a) &lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;priority_{i}&lt;/script&gt; = &lt;script type=&quot;math/tex&quot;&gt;p_{i}&lt;/script&gt; &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\hspace{32pt}&lt;/script&gt;
= &lt;script type=&quot;math/tex&quot;&gt;{|\delta_{i}|}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;+&lt;/script&gt; e &lt;br /&gt;
&lt;br /&gt;
probability(i) = P(i) &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\hspace{41pt}&lt;/script&gt;
= &lt;script type=&quot;math/tex&quot;&gt;\frac{p_{i}^{\alpha}}  {\sum_{k}p_{k}^{\alpha}}&lt;/script&gt; &lt;br /&gt;
&lt;br /&gt;
weights = &lt;script type=&quot;math/tex&quot;&gt;w_{i}&lt;/script&gt; = (N &lt;script type=&quot;math/tex&quot;&gt;\cdot&lt;/script&gt; P(i)) &lt;script type=&quot;math/tex&quot;&gt;^{-\beta}&lt;/script&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Implementation details:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Sum tree:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Assume an example of a sum tree with 7 nodes (with 4 leaves which corresponds to the replay memory size):&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;At initialization:&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1-quXFm1UnNnaThHxhaMoYl5RTAJnJUVI&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;When item 1 is added:&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1Jk-RO9Yqeq2DQKO1CKD9e_KQTxWgtMOu&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;When item 2 is added:&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1fTopGfDSeQj3uEKZPlo_2KSTWaBHrFfK&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;When item 3 is added:&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1d37aBtukIExVU7k84XjUPPphiFJlKXBZ&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;When item 4 is added:&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1V7B3vODsz2ELpW5--oQPh1vxmPMLYxOz&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;When item 5 is added:&lt;/p&gt;
    &lt;blockquote&gt;
      &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1KBPd61jU4nNug7b475gbKLe5sBJhC_l-&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Figure below shows the corresponding code &amp;amp; array contents. The tree represents the entire sum tree while data represents the leaves.&lt;/p&gt;
  &lt;blockquote&gt;
    &lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=view&amp;amp;id=1kk60DiIQOEkR03wakk2Qwyj2xcK7ac3k&quot; alt=&quot;alt text&quot; /&gt;&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;In the implementation, only one sumTree object is needed to store the collected experiences, this sumTree object resides in the Replay_memory class. The sumTree object has number of leaves = replay memory size = capacity.
The data array in sumTree object stores an Exp object, which is a sample of experience.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;
The following code decides how to sample:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def sample(self, k): # k = minibatch size
    batch = []

    # total_p() gives the total sum of priorities of the leaves in the sumTree
    # which is the value stored in the root node
    segment = self.tree.total_p() / k

    for i in range(k):
        a = segment * i # start of segment
        b = segment * (i + 1) # end of segment
        s = np.random.uniform(a, b) # rand value between a, b

        (idx, p, data) = self.tree.get(s)
        batch.append( (idx, p, data) )            

    return batch    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Refer to appendix B.2.1, under the section, “Proportional prioritization”, from the original (Schaul et al., 2016) &lt;a href=&quot;https://arxiv.org/pdf/1511.05952.pdf&quot;&gt;paper&lt;/a&gt; for sampling details.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1511.05952.pdf&quot;&gt;Prioritized experience replay (Schaul et al., 2016)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">A Dueling Double Deep Q Network with Priority Experience Replay (Duel DDQN with PER) implementation in tensorflow.</summary></entry><entry><title type="html">Dueling DDQN</title><link href="http://localhost:4000/Duel_DDQN/" rel="alternate" type="text/html" title="Dueling DDQN" /><published>2019-03-10T00:00:00+08:00</published><updated>2019-03-10T00:00:00+08:00</updated><id>http://localhost:4000/Duel_DDQN</id><content type="html" xml:base="http://localhost:4000/Duel_DDQN/">&lt;p&gt;A &lt;strong&gt;Dueling Double Deep Q Network (Dueling DDQN)&lt;/strong&gt; implementation in tensorflow with random experience replay.&lt;/p&gt;

&lt;p&gt;Environment from OpenAI’s gym: CartPole-v0&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ChuaCheowHuan/reinforcement_learning/tree/master/DQN_variants/duel_DDQN&quot;&gt;Full code&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Notations:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Network = &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; &lt;br /&gt;
Parameter = &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; &lt;br /&gt;
Network Q value = &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; (s, a) &lt;br /&gt;
&lt;br /&gt;
Value function = V(s) &lt;br /&gt;
Advantage function = A(s, a) &lt;br /&gt;
&lt;br /&gt;
Parameter from the Advantage function layer = &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; &lt;br /&gt;
Parameter from the Value function layer = &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Equations:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(eqn 9) from the original paper &lt;a href=&quot;https://arxiv.org/pdf/1511.06581.pdf&quot;&gt;(Wang et al., 2015)&lt;/a&gt;: &lt;br /&gt;
Q(s, a; &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;) =
V(s; &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;)
&lt;script type=&quot;math/tex&quot;&gt;+&lt;/script&gt; &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\hspace{50pt}&lt;/script&gt;
[ A(s, a; &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;)
&lt;script type=&quot;math/tex&quot;&gt;-&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\frac{1}{|A|}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;\sum_{a'}&lt;/script&gt; A(s, &lt;script type=&quot;math/tex&quot;&gt;a^{'}&lt;/script&gt;; &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;) ]&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Implementation details:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;V represents the value function layer, A represents the Advantage function layer:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# contruct neural network
def built_net(self, var_scope, w_init, b_init, features, num_hidden, num_output):              
    with tf.variable_scope(var_scope):          
      feature_layer = tf.contrib.layers.fully_connected(features, num_hidden,
                                                        activation_fn = tf.nn.relu,
                                                        weights_initializer = w_init,
                                                        biases_initializer = b_init)
      V = tf.contrib.layers.fully_connected(feature_layer, 1,
                                            activation_fn = None,
                                            weights_initializer = w_init,
                                            biases_initializer = b_init)
      A = tf.contrib.layers.fully_connected(feature_layer, num_output,
                                            activation_fn = None,
                                            weights_initializer = w_init,
                                            biases_initializer = b_init)   
      Q_val = V + (A - tf.reduce_mean(A, reduction_indices=1, keepdims=True)) # refer to eqn 9 from the original paper          
    return Q_val   
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1511.06581.pdf&quot;&gt;Dueling Network Architectures for Deep Reinforcement Learning
(Wang et al., 2015)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">A Dueling Double Deep Q Network (Dueling DDQN) implementation in tensorflow with random experience replay.</summary></entry><entry><title type="html">DDQN</title><link href="http://localhost:4000/DDQN/" rel="alternate" type="text/html" title="DDQN" /><published>2019-03-07T00:00:00+08:00</published><updated>2019-03-07T00:00:00+08:00</updated><id>http://localhost:4000/DDQN</id><content type="html" xml:base="http://localhost:4000/DDQN/">&lt;p&gt;A &lt;strong&gt;Double Deep Q Network (DDQN)&lt;/strong&gt; implementation in tensorflow with random experience replay.&lt;/p&gt;

&lt;p&gt;Environment from OpenAI’s gym: CartPole-v0&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ChuaCheowHuan/reinforcement_learning/tree/master/DQN_variants/DDQN&quot;&gt;Full code&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Notations:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Model network = &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; &lt;br /&gt;
Model parameter = &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; &lt;br /&gt;
Model network Q value = &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; (s, a) &lt;br /&gt;
&lt;br /&gt;
Target network = &lt;script type=&quot;math/tex&quot;&gt;Q_{\phi}&lt;/script&gt; &lt;br /&gt;
Target parameter = &lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt; &lt;br /&gt;
Target network Q value = &lt;script type=&quot;math/tex&quot;&gt;Q_{\phi}&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;s^{'}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;a^{'}&lt;/script&gt;)&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Equations:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;TD target = r (s, a) &lt;script type=&quot;math/tex&quot;&gt;+&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\phi}&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;s^{'}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;argmax_{a^{'}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; (s&lt;script type=&quot;math/tex&quot;&gt;^{'}&lt;/script&gt;, a&lt;script type=&quot;math/tex&quot;&gt;^{'}&lt;/script&gt;)) &lt;br /&gt;
&lt;br /&gt;
TD  error = (TD target) &lt;script type=&quot;math/tex&quot;&gt;-&lt;/script&gt; (Model network Q value) &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\hspace{26pt}&lt;/script&gt;
= [r (s, a) &lt;script type=&quot;math/tex&quot;&gt;+&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\phi}&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;s^{'}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;argmax_{a^{'}}&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; (s&lt;script type=&quot;math/tex&quot;&gt;^{'}&lt;/script&gt;, a&lt;script type=&quot;math/tex&quot;&gt;^{'}&lt;/script&gt;))] &lt;script type=&quot;math/tex&quot;&gt;-&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;Q_{\theta}&lt;/script&gt; (s, a)&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Implementation details:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Create a placeholder to feed Q values from model network:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;self.model_s_next_Q_val = tf.placeholder(tf.float32, [None,self.num_actions], name='model_s_next_Q_val')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Select Q values from model network using &lt;script type=&quot;math/tex&quot;&gt;s^{'}&lt;/script&gt; as features &amp;amp; feed them to the training session:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# select actions from model network
model_s_next_Q_val = self.sess.run(self.model_Q_val, feed_dict={self.s: s_next})

# training
_, loss = self.sess.run([self.optimizer, self.loss],
                        feed_dict = {self.s: s,
                                     self.a: a,
                                     self.r: r,
                                     self.s_next: s_next,
                                     self.done: done,
                                     self.model_s_next_Q_val: model_s_next_Q_val})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Select minibatch actions with largest Q values from model network, create indices &amp;amp; select corresponding minibatch actions from target network:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def td_target(self, s_next, r, done, model_s_next_Q_val, target_Q_val):
    # select action with largest Q value from model network
    model_max_a = tf.argmax(model_s_next_Q_val, axis=1, output_type=tf.dtypes.int32)

    arr = tf.range(tf.shape(model_max_a)[0], dtype=tf.int32) # create row indices
    indices = tf.stack([arr, model_max_a], axis=1) # create 2D indices        
    max_target_Q_val = tf.gather_nd(target_Q_val, indices) # select minibatch actions from target network
    max_target_Q_val = tf.reshape(max_target_Q_val, (self.minibatch_size,1))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1509.06461.pdf&quot;&gt;Deep Reinforcement Learning with Double Q-learning
(Hasselt, Guez &amp;amp; Silver, 2016)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Huan</name></author><summary type="html">A Double Deep Q Network (DDQN) implementation in tensorflow with random experience replay.</summary></entry></feed>