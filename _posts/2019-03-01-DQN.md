---
layout: posts
author: Huan
title: DQN



---


A **DQN** implementation in tensorflow with target network & random experience replay.

Environment from openai gym: CartPole-v0

[Code](https://github.com/ChuaCheowHuan/reinforcement_learning/tree/master/DQN_variants/DQN)

---
<br>
**Notations**

Model network = $$Q_{\theta}$$ \\
Model parameter = $$\theta$$ \\
Model network Q value = $$Q_{\theta}$$ (s, a) \\
<br>
Target network = $$Q_{\phi}$$ \\
Target parameter = $$\phi$$ \\
Target network Q value = $$Q_{\phi}$$ ($$s^{'}$$, $$a^{'}$$)

---
<br>
**Equations**

TD target = r (s, a) $$+$$ $$\gamma$$ $$max_{a}$$ $$Q_{\phi}$$ ($$s^{'}$$, $$a^{'}$$) \\
<br>
TD  error = (TD target) $$-$$ (Model network Q value) \\
$$\hspace{26pt}$$
= [r (s, a) $$+$$ $$\gamma$$ $$max_{a^{'}}$$ $$Q_{\phi}$$ ($$s^{'}$$, $$a^{'}$$)] $$-$$ $$Q_{\theta}$$ (s, a)

---
<br>
**Implementation details**

Update target parameter $$\phi$$ with model parameter $$\theta$$ :
>Copy $$\theta$$ to $$\phi$$ with *either* soft or hard parameter update.

>Hard parameter update:
>>![alt text](https://drive.google.com/uc?export=view&id=18CK3rHYEfDxVtxe1gnVn2Z10Dosrmrww)
![alt text](https://drive.google.com/uc?export=view&id=1lNBR6BxZZfk_uGkDSOumUm9qntiJ5QhH)

>Soft parameter update:
>>polyak $$\cdot$$  $$\theta$$ + (1 $$-$$ polyak)  $$\cdot$$  $$\phi$$

>>![alt text](https://drive.google.com/uc?export=view&id=1OfxkRAMve0liZ3BlkS4pCoJ6CPPEjwQG)

Stop TD target from contributing to gradient computation:
>![alt text](https://drive.google.com/uc?export=view&id=1sw1WtddZn4t48QJhz_LMTthIPhOc4jtl)

---
<br>
**References**

[Human-level control through deep reinforcement learning
(Mnih et al., 2015)](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)

<br>
