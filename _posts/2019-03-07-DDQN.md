---
layout: posts
author: Huan


---
Double DQN (DDQN)

A **Double DQN (DDQN)** implementation in tensorflow with random experience replay.

Environment from openai gym: CartPole-v0

---
<br>
**Notations**

Model network = $$Q_{\theta}$$ \\
Model parameter = $$\theta$$ \\
Model network Q value = $$Q_{\theta}$$ (s, a) \\
<br>
Target network = $$Q_{\phi}$$ \\
Target parameter = $$\phi$$ \\
Target network Q value = $$Q_{\phi}$$ ($$s^{'}$$, $$a^{'}$$)

---
<br>
**Equations**

TD target = r (s, a) $$+$$ $$\gamma$$ $$Q_{\phi}$$ ($$s^{'}$$, $$argmax_{a^{'}}$$ $$Q_{\theta}$$ (s$$^{'}$$, a$$^{'}$$)) \\
<br>
TD  error = (TD target) $$-$$ (Model network Q value) \\
$$\hspace{26pt}$$
= [r (s, a) $$+$$ $$\gamma$$ $$Q_{\phi}$$ ($$s^{'}$$, $$argmax_{a^{'}}$$ $$Q_{\theta}$$ (s$$^{'}$$, a$$^{'}$$))] $$-$$ $$Q_{\theta}$$ (s, a)

---
<br>
**Implementation details**

Create a placeholder to feed Q values from model network:
>![alt text](https://drive.google.com/uc?export=view&id=1CcZVw82JRQRWYmTFFN9PvLKjd4b5BOAF)

Select Q values from model network using $$s^{'}$$ as features & feed them to the training session:
>![alt text](https://drive.google.com/uc?export=view&id=15uOc3uOz83V76X5s3PmgzzVWYJkkwR0Z)

Select minibatch actions with largest Q values from model network, create indices & select corresponding minibatch actions from target network:
>![alt text](https://drive.google.com/uc?export=view&id=1YelpKjS68nPBWtg8oeLiZV4mpzkmTPT_)

---
<br>
**References**

[Deep Reinforcement Learning with Double Q-learning
(Hasselt, Guez & Silver, 2016)](https://arxiv.org/pdf/1509.06461.pdf)
