---
layout: posts
author: Huan
---
Dueling DDQN

A **Dueling Double DQN (Dueling DDQN)** implementation in tensorflow with random experience replay.

Environment from openai gym: CartPole-v0

---
<br>
**Notations**

Network = $$Q_{\theta}$$ \\
Parameter = $$\theta$$ \\
Network Q value = $$Q_{\theta}$$ (s, a) \\
<br>
Value function = V(s) \\
Advantage function = A(s, a) \\
<br>
Parameter from the Advantage function layer = $$\alpha$$ \\
Parameter from the Value function layer = $$\beta$$

---
<br>
**Equations**

(eqn 9) from the original paper [(Wang et al., 2015)](https://arxiv.org/pdf/1511.06581.pdf): \\
Q(s, a; $$\theta$$, $$\alpha$$, $$\beta$$) =
V(s; $$\theta$$, $$\beta$$)
$$+$$ \\
$$\hspace{50pt}$$
[ A(s, a; $$\theta$$, $$\alpha$$)
$$-$$
$$\frac{1}{|A|}$$ $$\sum_{a'}$$ A(s, $$a^{'}$$; $$\theta$$, $$\alpha$$) ]

---
<br>
**Implementation details**

V represents the value function layer, A represents the Advantage function layer:
>![alt text](https://drive.google.com/uc?export=view&id=1f901lKe-Fa_Y4ITX8NFNeMO7IX_O2fB9)

---
<br>
**References**

[Dueling Network Architectures for Deep Reinforcement Learning
(Wang et al., 2015)](https://arxiv.org/pdf/1511.06581.pdf)
