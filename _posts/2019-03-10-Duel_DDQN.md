---
layout: posts
author: Huan
title: Dueling DDQN
---

This post documents my implementation of the Dueling Double Deep Q Network
(Dueling DDQN) algorithm.

A **Dueling Double Deep Q Network (Dueling DDQN)** implementation in tensorflow
with random experience replay. The code is tested with Gym's discrete action
space environment, CartPole-v0 on Colab.

Code on my [Github](https://github.com/ChuaCheowHuan/reinforcement_learning/tree/master/DQN_variants/duel_DDQN)

---

## Notations:

Network = $$Q_{\theta}$$

Parameter = $$\theta$$

Network Q value =
$$Q_{\theta}$$
(s, a)

Value function = V(s)

Advantage function = A(s, a)

Parameter from the Advantage function layer = $$\alpha$$

Parameter from the Value function layer = $$\beta$$

---

## Equations:

(eqn 9) from the original paper [(Wang et al., 2015)](https://arxiv.org/pdf/1511.06581.pdf):

Q(s, a; $$\theta$$, $$\alpha$$, $$\beta$$) =
V(s; $$\theta$$, $$\beta$$)
$$+$$
[ A(s, a; $$\theta$$, $$\alpha$$)
$$-$$
$$\frac{1}{|A|}$$ $$\sum_{a'}$$
A(s, $$a^{'}$$;
$$\theta$$, $$\alpha$$) ]

---

## Key implementation details:

V represents the value function layer, A represents the Advantage function layer:
```
# contruct neural network
def built_net(self, var_scope, w_init, b_init, features, num_hidden, num_output):              
    with tf.variable_scope(var_scope):          
      feature_layer = tf.contrib.layers.fully_connected(features, num_hidden,
                                                        activation_fn = tf.nn.relu,
                                                        weights_initializer = w_init,
                                                        biases_initializer = b_init)
      V = tf.contrib.layers.fully_connected(feature_layer, 1,
                                            activation_fn = None,
                                            weights_initializer = w_init,
                                            biases_initializer = b_init)
      A = tf.contrib.layers.fully_connected(feature_layer, num_output,
                                            activation_fn = None,
                                            weights_initializer = w_init,
                                            biases_initializer = b_init)   
      Q_val = V + (A - tf.reduce_mean(A, reduction_indices=1, keepdims=True)) # refer to eqn 9 from the original paper          
    return Q_val   
```

---

## References:

[Dueling Network Architectures for Deep Reinforcement Learning
(Wang et al., 2015)](https://arxiv.org/pdf/1511.06581.pdf)

---

<br>
